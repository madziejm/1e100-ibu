{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1e100ibu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madziejm/1e100-ibu/blob/master/1e100ibu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary"
      ],
      "metadata": {
        "id": "ZMhH5l9MeZho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dependencies"
      ],
      "metadata": {
        "id": "CT48xPyXJHy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'dev = {dev}')"
      ],
      "metadata": {
        "id": "Sl9RJDTxJOTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b20fce-88a2-48a2-f684-beb3915fe384"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev = cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet icecream\n",
        "from icecream import ic"
      ],
      "metadata": {
        "id": "Yet9JpBt8kcx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset representation"
      ],
      "metadata": {
        "id": "JcpSzAJmFV56"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ8cumFHEEQd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beda4d36-985b-48fa-cac5-2b507244d872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 6.0 MB 6.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 628 kB 39.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 22.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 451 kB 31.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 43.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 943 kB/s \n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "\u001b[?25hCollecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install 'spacy<3.3.0,>=3.2.0' --quiet\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show spacy | egrep Version\n",
        "# we want SpaCy 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvQgSM-fB7YP",
        "outputId": "f530c196-9922-466a-c7ff-3efc8af63372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### review example"
      ],
      "metadata": {
        "id": "38ztJX3eRuqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#     \"\"\"\n",
        "#     beer/name: John Harvards Simcoe IPA\n",
        "#     beer/beerId: 63836\n",
        "#     beer/brewerId: 8481\n",
        "#     beer/ABV: 5.4\n",
        "#     beer/style: India Pale Ale &#40;IPA&#41;\n",
        "#     review/appearance: 4/5\n",
        "#     review/aroma: 6/10\n",
        "#     review/palate: 3/5\n",
        "#     review/taste: 6/10\n",
        "#     review/overall: 13/20\n",
        "#     review/time: 1157587200\n",
        "#     review/profileName: hopdog\n",
        "#     review/text: On tap at the Springfield, PA location. Poured a deep and cloudy orange (almost a copper) color with a small sized off white head. Aromas or oranges and all around citric. Tastes of oranges, light caramel and a very light grapefruit finish. I too would not believe the 80+ IBUs - I found this one to have a very light bitterness with a medium sweetness to it. Light lacing left on the glass.\n",
        "#     \"\"\""
      ],
      "metadata": {
        "id": "1rYfSug_5Vlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dataset representation"
      ],
      "metadata": {
        "id": "_yZeL2NQcSjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from torchtext._torchtext import (Vocab as VocabPybind) # make use of some hidden interface\n",
        "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import gc # garbage collector interface\n",
        "import io\n",
        "import re\n",
        "import spacy # nlp toolkit\n",
        "import torch\n",
        "\n",
        "class RateBeerReviews(torch.utils.data.Dataset):\n",
        "    def __init__(self, filepath='/content/SNAP-Ratebeer.txt', reviews_max=float('inf')):\n",
        "        self.aspects = ['appearance', 'aroma', 'palate', 'taste', 'overall']\n",
        "        self.aspect_count = len(self.aspects)\n",
        "        self.aspect_max = [5 + 1, 10 + 1, 5 + 1, 10 + 1, 20 + 1]\n",
        "        self._aspect_ratings = [ [] for _ in self.aspects ]\n",
        "        self._texts = []\n",
        "        self.unkn_tok = \"<unk>\" # unknown/out of vocabulary token\n",
        "        self._len = 0\n",
        "        self._fetch_data(filepath, reviews_max)\n",
        "        self._post_process(max_word_count=20000) # 20K words should be okay\n",
        "\n",
        "    def _fetch_data(self, filepath, reviews_max):\n",
        "        with io.open(filepath, encoding='utf-8') as f:\n",
        "            for line in tqdm(f, total=(40938282 if reviews_max == float('inf') else reviews_max * 14), desc='Reading data'):\n",
        "                if line == '\\n': # separator\n",
        "                    self._len += 1\n",
        "                    if reviews_max <= self._len:\n",
        "                        break\n",
        "                elif line.startswith('review/appearance: '):\n",
        "                    line = line[len('review/appearance: '):]\n",
        "                    self._aspect_ratings[0].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/aroma: '):\n",
        "                    line = line[len('review/aroma: '):]\n",
        "                    self._aspect_ratings[1].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/palate: '):\n",
        "                    line = line[len('review/palate: '):]\n",
        "                    self._aspect_ratings[2].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/taste: '):\n",
        "                    line = line[len('review/taste: '):]\n",
        "                    self._aspect_ratings[3].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/overall: '):\n",
        "                    line = line[len('review/overall: '):]\n",
        "                    self._aspect_ratings[4].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/text: '):\n",
        "                    line = line[len('review/text: '):]\n",
        "                    if line.startswith('UPDATED:'):\n",
        "                        line = line[len(\"UPDATED: APR 29, 2008\"):] # drop prefix\n",
        "                    line = re.sub('~', ' ', line.strip()) # remove whitespace incl. trailing newline and tildes that can be found in data for some reason\n",
        "                    if line:\n",
        "                        self._texts.append(line)\n",
        "                    else: # some reviews do not have associated text; unwind (remove) their ratings for each aspect\n",
        "                        for aspect_ratings in self._aspect_ratings:\n",
        "                            aspect_ratings.pop()\n",
        "                        self._len -= 1\n",
        "\n",
        "    def _post_process(self, min_word_freq=None, max_word_count=None):\n",
        "        assert (min_word_freq is not None) ^ bool(max_word_count is not None), \"provide one of min_word_freq and max_word_count\"\n",
        "        nlp = spacy.util.get_lang_class('en')()\n",
        "        nlp.add_pipe(\"sentencizer\", config={\"punct_chars\": ['.', '?', '!']})\n",
        "        nlp.Defaults.stop_words |= { '-', '+'}\n",
        "        nlp.Defaults.stop_words -= {'mostly', 'whole', 'indeed', 'quite', 'ever', 'nothing', 'perhaps', 'not', 'no', 'only', 'well', 'really', 'except'}\n",
        "        print(\"Spacy pipe (tokenization&sentence split)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = [tuple(list(tok.lower_ for tok in sent if not tok.is_stop and not tok.is_punct and not tok.is_space and len(tok) > 2) for sent in doc.sents) for doc in nlp.pipe(self._texts)]\n",
        "        print(\"Building vocab (word-id mapping)..\")\n",
        "        gc.collect(generation=0) # force garbage collection\n",
        "        gc.collect(generation=1) # force garbage collection\n",
        "        gc.collect(generation=2) # force garbage collection\n",
        "        sent_gen = (sent for text in self._texts for sent in text)\n",
        "        if min_word_freq:\n",
        "            self.vocab = build_vocab_from_iterator(sent_gen, specials=[self.unkn_tok], min_word_freq=5)\n",
        "        else:\n",
        "            words = Counter()\n",
        "            for tokens in sent_gen:\n",
        "                words.update(tokens)\n",
        "            words = [word for word, freq in words.most_common(max_word_count)] # list sorted by frequency yikees\n",
        "            self.vocab = Vocab(VocabPybind(words, None))\n",
        "        self.vocab.insert_token(self.unkn_tok, 0)\n",
        "        self.vocab.set_default_index(self.vocab[self.unkn_tok]) # set index for out-of-vocabulary words\n",
        "        print(\"Mapping words to ids..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = [tuple(self.vocab.lookup_indices(sent) for sent in text) for text in self._texts]\n",
        "        gc.collect() # force garbage collection\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        sentences = self._texts[i]\n",
        "        ratings = tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count))\n",
        "        return (sentences, ratings)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len"
      ],
      "metadata": {
        "id": "C1MCM0dWB93e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to read dataset from dataset file, set FETCH_RATEBEER to true in the cell below and RECREATE_PICKLE to True. If you left them untouched, it'lle be read from serialized `RateBeerReviews` class object instead of parsing text file."
      ],
      "metadata": {
        "id": "oy1qrIH1CAlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "export FETCH_RATEBEER=false\n",
        "if [ \"$FETCH_RATEBEER\" = true ] && [ -e $RATEBEER_FILE ]\n",
        "then # original dataset\n",
        "    export RATEBEER_FILE='/content/SNAP-Ratebeer.txt'\n",
        "    gdown --id '12tEEYQcHZtg5aWyfIiWWVIDAJNT-5d_T' # https://drive.google.com/file/d/12tEEYQcHZtg5aWyfIiWWVIDAJNT-5d_T/view?usp=sharing\n",
        "    echo \"Dataset head (trailing newline makes entry end): \"\n",
        "    head -n 16 $RATEBEER_FILE\n",
        "    iconv -f ISO-8859-1 -t UTF-8 $RATEBEER_FILE -o {RATEBEER_FILE}.new && mv {RATEBEER_FILE}.new $RATEBEER_FILE\n",
        "else # pickle\n",
        "    gdown --id '1VBDjyR4jpzAgzcDUGNQFguOfLC3rtOV_' # https://drive.google.com/file/d/1VBDjyR4jpzAgzcDUGNQFguOfLC3rtOV_/view?usp=sharing  # 20K words dataset\n",
        "    # gdown --id '1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j' # https://drive.google.com/file/d/1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j/view?usp=sharing # min frequency 5 words dataset\n",
        "fi"
      ],
      "metadata": {
        "id": "B_QTo-xBCelp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df049b4f-2509-4789-be05-8370cbc55cbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VBDjyR4jpzAgzcDUGNQFguOfLC3rtOV_\n",
            "To: /content/ratebeer-20K-vocab.pickle\n",
            "\r  0%|          | 0.00/319M [00:00<?, ?B/s]\r  1%|1         | 4.72M/319M [00:00<00:13, 24.1MB/s]\r  7%|7         | 22.5M/319M [00:00<00:09, 32.5MB/s]\r 11%|#         | 34.1M/319M [00:00<00:06, 41.0MB/s]\r 13%|#3        | 42.5M/319M [00:00<00:05, 46.3MB/s]\r 21%|##        | 65.5M/319M [00:00<00:04, 60.9MB/s]\r 24%|##4       | 77.6M/319M [00:00<00:03, 66.5MB/s]\r 32%|###1      | 101M/319M [00:00<00:02, 81.2MB/s] \r 36%|###5      | 114M/319M [00:01<00:02, 87.3MB/s]\r 41%|####1     | 132M/319M [00:01<00:01, 103MB/s] \r 46%|####5     | 146M/319M [00:01<00:01, 107MB/s]\r 51%|#####1    | 163M/319M [00:01<00:01, 119MB/s]\r 55%|#####5    | 177M/319M [00:01<00:01, 116MB/s]\r 61%|######    | 193M/319M [00:01<00:01, 124MB/s]\r 66%|######5   | 209M/319M [00:01<00:00, 132MB/s]\r 70%|#######   | 224M/319M [00:01<00:00, 119MB/s]\r 77%|#######6  | 244M/319M [00:01<00:00, 131MB/s]\r 81%|########1 | 258M/319M [00:02<00:00, 122MB/s]\r 86%|########6 | 274M/319M [00:02<00:00, 131MB/s]\r 90%|######### | 288M/319M [00:02<00:00, 127MB/s]\r 97%|#########6| 309M/319M [00:02<00:00, 143MB/s]\r100%|##########| 319M/319M [00:02<00:00, 129MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle # serialize lib\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/drive')\n",
        "\n",
        "DATASET_PICKLE='/content/ratebeer.pickle'\n",
        "DATASET_PICKLE='/content/ratebeer-20K-vocab.pickle'\n",
        "RECREATE_PICKLE = False\n",
        "\n",
        "if RECREATE_PICKLE:\n",
        "    with open(DATASET_PICKLE, 'wb') as f:\n",
        "        rb = RateBeerReviews()\n",
        "        print('Dumping..')\n",
        "        pickle.dump(rb, f)\n",
        "else:\n",
        "    with open(DATASET_PICKLE, 'rb') as f:\n",
        "        rb = pickle.load(f)"
      ],
      "metadata": {
        "id": "N5zmSYDBCg4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word ID-s count and 1K of least common words\n",
        "print(len(rb.vocab.get_itos()))\n",
        "print(rb.vocab.get_itos()[-1000:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ0p4rxp2UJE",
        "outputId": "f0d8fb49-7a4d-4b5b-b50a-4aa48c3afafa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20001\n",
            "['tractor', 'syd', 'chiefly', 'defiant', 'stripper', 'val', 'bierhaus', 'eyeball', 'sporatic', 'deviate', 'grapenuts', 'beervana', 'tarness', 'lenny', 'phenomenon', 'reek', 'usualy', 'bernard', 'barbecued', 'fruitbeer', 'enjoyit', 'rive', 'sas', 'flavorable', 'ecstatic', 'leopoldstoch', 'soulless', 'allagashs', 'mikrobryggeri', 'disgusted', 'heine', 'covey', 'respectively', 'scout', 'emitting', 'flemdawg', 'atom', 'brightens', 'pch', 'www.bierzwerg.de', 'oakwood', 'flagged', 'collor', 'rbnag-11', 'belter', 'ecstasy', 'deem', 'gust', 'creamyish', 'opq', 'fruti', 'accompagne', 'zunge', 'sinful', 'prunish', 'ingrediants', 'refill', 'rug', 'alc%', 'blase', 'paramount', 'oldrtybastrd', 'fininsh', 'sofort', 'sazz', 'echoing', 'prospect', 'contributor', 'crazily', 'rogueone', 'beer_hawk', 'presenza', 'overlain', 'hinteren', 'spur', 'suspicions', 'glendale', 'renowned', 'hover', '09/08/2008', 'flashback', 'degraded', 'vegetabley', 'sunburst', 'snug', 'beneficial', 'linkery', 'mtn', 'doublebock', 'bevy', 'itch', 'instances', 'filmed', 'pliney', 'equinox', 'habe', 'frying', 'froot', 'admired', 'torbida', 'lassan', 'blatantly', 'poolside', 'finishable', 'rename', 'understandably', 'caramel/', 'luggage', 'clusters', 'thingy', 'enemy', 'tilting', 'feelin', 'halogen', 'whichever', 'soulard', 'rlb', 'slot', 'byproducts', 'humboldt', 'scoff', 'f&amp;t', 'jaghana', 'companions', 'deeep', 'inaugural', 'unadventurous', 'cutler', 'five+', 'resemblence', 'arising', 'wimbledon', 'index.html', 'flemmish', 'talc', 'liefmans', 'coniston', 'absorb', 'denisons', '340ml', 'stroh', 'appease', 'paradox', 'fella', 'greedy', 'ome', 'ead', 'wre', 'nastiest', '20110129', 'laff', 'chocoffee', 'insists', 'bratislava', 'lungo', 'induce', 'quebecois', 'quadrupels', 'precede', 'crackly', '20/20', 'licious', 'strangly', 'conscious', 'centers', 'turva', 'tycker', 'smoooooth', 'buildup', 'mouthfeeel', 'lastin', 'declare', '1.89', 'delhaize', 'gyldenbrun', 'isnt', 'rdbrun', 'sommer', 'units', 'outs', 'ale--2007', 'galena', 'locked', 'historically', 'transform', 'bleed', 'burpy', 'quckly', 'hungover', 'cappuchino', 'facts', 'individuality', 'stability', 'everclear', 'witnessed', 'dpt', 'deau', 'fittingly', 'houblonnage', 'fortement', 'pcc', 'cui', 'legally', 'bde', 'massage', 'strident', 'bf08', 'treacley', 'uric', 'spewing', 'stripes', 'spa', '5,5', 'isles', 'google', 'graphite', 'contemplating', 'wesley', 'petersburg', 'humulus', 'recovered', 'erased', 'deuce', 'crawfish', 'creamily', 'boozed', 'blueberrys', 'provoking', 'turnover', 'propose', 'warranted', 'attain', '100ml', 'saf', 'mspindler', 'effusive', 'epics', 'spontaneously', 'toch', 'tuoksussa', '270511', 'frankenmuth', 'rg1', 'wendy', 'rbsg2003', 'frome', 'heiniken', 'potosi', 'erasmus', 'bedondaine', 'coffee-', 'toom', 'weakens', '3.5/5.0score', 'flyer', 'drinakble', 'bleah', 'charamel', 'surviving', 'challenges', 'alkoholos', 'caramalts', 'deffinately', 'tooo', 'differant', 'eloquent', 'asset', 'ohhh', 'retros', 'ambree', 'fantasic', 'altho', 'adjustment', 'umbrella', 'petty', 'unwashed', 'mouldering', 'arousing', 'czechs', 'backwash', 'goldy', 'betting', 'lloyds', 'fulham', 'plymouth', 'focussed', 'input', 'beard', 'thanks!pours', 'brles', 'underlined', 'chracter', 'scone', 'shores', 'patty', 'links', \"'em\", '20100803', 'vortex', 'robbed', 'bombardment', 'largo', '7/4/8/4/16', 'lagerbier', 'dripped', 'narines', 'woolly', 'grief', 'contend', 'solar', 'compaired', 'misguided', 'yardwork', 'cloak', 'flavoe', 'spices-', 'grapefuity', 'southport', 'styling', 'szpen', 'nowt', 'aesthetic', 'zombie', 'flanked', 'hater', 'deciso', 'badged', 'lowlander', 'beiaard', 'dishwatery', 'humide', 'guineas', 'zeitgeist', 'tug', 'dewy', 'amp', 'gotta', 'anvil', 'commemorate', 'devotion', 'application', 'anyhoo', 'grav', 'merky', 'fatheads', 'pepperish', 'sunes', 'feminine', 'pivovar', 'nth', 'sorbon', 'tappours', 'spitty', 'macdoogals', '2.29', 'spitfire', 'templar', 'tapny', 'tying', 'geur', 'b&amp;t', 'mcfarland', 'tregs', 'rodger', 'heidelberg', 'shiver', 'aussies', 'orangina', 'coexist', 'recurring', 'unangenehm', 'elmers', 'conserve', 'lager-', 'bietet', '20110416', 'vry', 'gratifying', 'unvaried', 'complexes', 'definently', 'prevalant', 'ceder', 'disclosure', 'wiffs', 'foaminess', 'yowza', 'segment', 'playdough', 'steriods', 'vitt', 'thirsting', 'obliged', 'glgg', 'persuasion', 'wagner', 'bristling', 'sixpence', 'elgin', 'briank.', 'repour', 'tweety', 'chocolate/', 'bulbous', 'all-', 'coworker', 'partying', 'mund', 'citirc', 'strasse', 'chatting', 'barclays', 'ownership', 'vera', 'pourri', 'mediumbroad', 'masculine', 'bonnes', 'aftetaste', 'ellum', 'killian', 'seperates', 'restrain', 'dogfishhead', 'kclingers', 'rangers', 'everthing', 'black-', 'rideau', 'gauze', 'winding', 'chiffon', 'sone', 'frumento', 'sera', 'magt', 'citrics', 'craziness', 'decient', 'dextrin', 'veel', 'fazit', 'loaves', 'stubbornly', 'outlandish', 'pondering', 'copperfields', 'pil', 'afore', 'measuring', 'trained', 'terry', 'midlothian', 'dissect', 'semi-', 'scones', 'kennedy', '1/5', '1pint', 'melody', 'boutin', 'perfuminess', '12z', 'narke', '1.8/5.0drinkability', 'ferocious', 'hammersmith', 'earhy', 'dorm', 'barrow', 'hazing', 'peaceful', 'periphery', 'babe', 'oettinger', 'widmers', 'mibromeo', 'sowie', 'cbf07', 'chien', 'cigarcitybrew', 'porn', 'gratitude', 'wlv', 'parallels', 'mly', 'nitroed', 'geringer', 'harmonize', 'erhalten', 'weicher', 'berzeugen', 'surfaced', 'overactive', 'fruitful', 'reccommend', 'amt', 'afther', 'beermerchants', 'treating', 'critic', 'durao', 'grapeskins', 'entrancing', 'brains', 'uncharacteristically', 'supporter', 'mealiness', 'boggling', 'questioning', 'lawnmowing', 'classica', 'turbulent', 'immortal', 'dai', 'glasset', 'rips', 'restaraunt', 'metall', 'scope', 'sulfites', 'moneys', 'unruly', 'scotish', 'phils', '25/09/2009', 'mattyb83', 'attend', 'unroasted', 'lgith', 'disolves', 'residence', 'sessioned', 'kingpinipa', 'goof', 'boards', 'disconnected', 'fulfilled', 'critter', 'soviet', 'aleish', 'decidely', 'chocolte', 'realization', 'adheres', 'bruins', 'drinakable', 'exprience', 'exhausted', 'harmonized', 'detached', 'orangs', 'froide', 'regualr', '06/08/2010', 'getrunken', 'underlaying', 'importing', 'beermongers', 'schultheiss', 'irrelevant', 'knny', 'maltbase', 'germ', 'teve', 'ehhhh', 'snort', 'dimished', 'bottled@one', 'benefitted', 'brewery-', 'southside', 'walmart', 'outshines', 'top.3', \"ol'\", '17/09/2010', 'crystallised', 'texans', '08/08/2008', 'crumbled', 'hausbrauerei', 'grimstad', 'showy', 'fing', 'resplendent', 'simcoes', 'doe', 'suburban', 'portery', 'waterbury', 'bodacious', 'illegible', 'vielleicht', 'centuries', 'puzzle', 'republics', 'sauv', 'roads', 'potsdam', 'brennans', '12pack', 'cly', 'herew', 'merrifield', 'tuning', 'chapter', 'drmatt', 'subsumed', 'smoothens', 'lag', 'pinesap', 'breeds', 'langsam', 'scharf', '291108', 'vollmundiger', 'coulour', 'lembra', 'aftershave', 'occassions', 'cope', 'assist', 'mdio', 'pennington', 'jamais', 'sdme', 'frugter', '-sweet', 'logged', 'intricately', 'slurp', 'rohrbachs', 'pitfruit', 'wigston', 'specialist', 'encrusted', 'durability', 'incident', 'employees', 'dart', 'head.nice', 'homey', 'intimidated', 'alcohol-', 'ambition', 'boutique', 'maid', 'cratered', '2200', 'discrte', 'leau', 'acidule', 'coriande', 'avocado', 'poudre', 'chaude', 'concasse', 'voir', 'chimney', 'minmal', 'beeriness', 'praises', 'papier', 'targeted', 'onthe', 'bustling', 'hosp', 'pomelo', 'steeping', '3/5.0score', 'eagles', 'tonge', 'organge', 'reel', 'grazie', 'deficient', 'maltines', 'baden', 'pedals', 'manassas', 'engulfs', 'michelobs', 'beersofeurope.co.uk', 'suerliche', 'legion', 'teeming', 'bourbonny', 'nicks', 'syndrome', 'sournes', 'lemonheads', 'bza', 'straighforward', 'mothballs', 'rambling', 'ward', 'eddie', 'itap', 'tynd', '35,5cl', 'fusely', 'cdumler', 'sessionnable', 'yell', 'stables', 'requirements', 'promotes', 'detest', 'golds', 'venant', 'spendy', 'excitable', 'freshman', 'novices', 'obtuse', 'tigers', 'milkchocolate', 'alls', 'egyptian', 'carafa', 'hvede', 'abrasively', '1pt9.4floz', 'answered', 'bitteren', 'vanillia', 'hophead75', 'warhead', 'puking', 'interbrew', 'homies', 'mixpack', 'aruba', 'viscousness', 'excursion', 'subtlely', 'imprint', 'osaka', 'strt', '20110802', 'paddle', 'ronnie', 'folly', 'celcius', 'speaker', '0,75l', 'tests', 'rend', 'fruitiest', 'null', 'aces', 'rosewood', 'inexplicably', 'oteyj', 'completion', 'trending', 'resonance', 'seemd', 'unlucky', 'keizer', 'orane', 'keyboard', 'jester', 'resonating', 'witty', 'aching', 'peely', 'ummmm', 'huntersville', 'virtues', 'exaggeration', 'bodes', 'slickly', 'palme', 'dry-', 'reedy', 'voto', 'primed', 'stressed', 'slicker', 'cornboy', 'poorest', 'performed', 'ticklish', 'thebeersommelier', 'poure', 'e.smag', 'cannons', 'rosty', 'bitternote', 'ahtanum', 'crispies', 'docked', 'chills', 'extrmement', 'aroma--', 'steigt', 'semblent', 'domin', 'tobaccoish', 'mastery', 'ontop', 'xtra', 'vieillie', 'coincide', 'hanssens', 'unavoidable', 'gah', 'fishermans', 'raisinny', 'petaluma', 'mich.', 'evan', 'concentrates', 'apr.', 'underfermented', 'bpa', '6/3/7/3/14', 'sweltering', 'fodder', 'sedate', 'barking', 'mde', 'genova', 'arkadia', 'liquoricey', 'penderels', 'wussy', 'megmarad', 'pilz', 'sudwerk', '7,5', 'channel', 'swath', 'tastybeer', 'erica', 'laser', 'reigned', 'mainlly', 'requiring', 'bottled@fonefan', 'retension', 'ebbs', 'meadows', 'impatient', 'worms', 'pranqster', 'caleb', 'claus', 'churchill', 'coins', 'beaches', 'unclemike', 'pannuhuone', 'mystic', 'aromai', 'raining', 'compacte', 'versione', 'invasion', 'spilt', 'ambered', 'chaps', 'hanover', 'tissue', 'caffreys', 'befejezse', 'cocos', 'evidente', 'cludy', 'iconic', 'steelers', 'mariage', 'serpents', 'rapsberry', 'dulles', 'galss', 'cameo', 'withstand', 'requested', 'gesture', 'scintillating', 'badgers', '20081004', 'pumps', 'milden', 'frische', 'disapoint', 'archetype', 'presenta', 'internal', 'ooooh', 'dattes', 'alcoho', 'kasteel', 'beauties', 'profusion', 'adventures', 'finsish', 'nestles', 'capitals', 'beglian', 'slo', 'alcoholly', 'sliky', 'bearer', 'acidula', 'startlingly', 'sligt', 'pieno', 'pods', 'unintended', 'ettersmaken', 'darkgolden', 'bsp', 'stlig', 'indefinable', 'tangier', 'talcum', 'gears', 'clarks', 'dre', 'cheltenham', 'xiii', 'bolster', 'dmac621', 'beastly', 'dimly', 'brotig', 'gbf', 'mouthfeal', 'smeel', 'shivers', 'expose', 'zitronig', 'celui', 'cependant', 'coppers', 'enticed', 'precedence', 'merritt', 'unfairly', 'nothings', 'directed', 'earthed', 'bragging', 'wouldnt', 'snca', 'scraped', 'escorted', 'majors', 'stance', 'forge', 'ncbc', 'gravesend', 'oranged', 'utzben', '040409', '10/2/10', 'proceedings', 'transferred', 'marys', 'berriness', 'nto', 'lance', 'madrid', 'yeasted', 'consistenza', 'lwenbru', 'mondo', 'prosit', 'oakish', 'annoys', 'simmer', 'torrent', 'geranium', 'vicinity', 'rfds', 'dukes', 'ecrvich', 'slime', 'gewisse', 'drunks', 'destin', 'dixen', 'dadurch', 'rom', 'employed', 'sittin', 'stuffs', 'dpinette', 'papa', '-at', 'hsc', 'wickedpete', 'awaisanen', '7.5%abv', 'neil', 'exponentially']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training (implementation of $(1)$)"
      ],
      "metadata": {
        "id": "yOc_2b-E4o0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "import datetime\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from more_itertools import grouper\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, dataset):\n",
        "        self.ds = dataset\n",
        "        self.init_weights()\n",
        "        self._optim = torch.optim.SGD(\n",
        "            params=(\n",
        "                self.theta,\n",
        "                *self.phis\n",
        "            ),\n",
        "            lr=0.00000000000001,\n",
        "            weight_decay=0.001\n",
        "        )\n",
        "\n",
        "    def init_weights(self):\n",
        "        word_count = len(self.ds.vocab.get_itos())\n",
        "        self.theta = torch.rand((word_count, self.ds.aspect_count)).to(dev)\n",
        "        # scale to [0.0, 0.9], as we enforce this weight to 1.0 for some words later on\n",
        "        self.theta = self.theta * 0.9\n",
        "        # enforce 1 initialization on aspect name (page 4)\n",
        "        aspect_ids = self.ds.vocab.lookup_indices(self.ds.aspects)\n",
        "        self.theta[aspect_ids, :] = 1\n",
        "        self.theta.requires_grad_()\n",
        "\n",
        "        # introduce separate phi for each aspect\n",
        "        self.phis = [torch.rand((word_count, self.ds.aspect_max[i])).to(dev) for i in range(self.ds.aspect_count)]\n",
        "        # normalize that sum across all words is 1 for a given aspect (eq. 7)\n",
        "        self.phis = [phi / phi.sum(dim=0) for phi in self.phis]\n",
        "        for phi in self.phis: phi.requires_grad_()\n",
        "    \n",
        "    def rev_words_thetas(self, rev_sens_ids):\n",
        "        \"\"\"\n",
        "        TODO comment sentence_aspects_likelihood_theta\n",
        "        \"\"\"\n",
        "        return [self.theta[sen_ids] for sen_ids in rev_sens_ids]\n",
        "\n",
        "    def rev_words_phis(self, rev_sens_ids):\n",
        "        \"\"\"\n",
        "        TODO comment sentence_aspects_likelihood_phi\n",
        "        \"\"\"\n",
        "        return [[self.phis[aspect_idx][sen_ids, :] for aspect_idx in range(self.ds.aspect_count)] for sen_ids in rev_sens_ids]\n",
        "    \n",
        "    def dump_weights(self, dest_dir='/drive/MyDrive/Colab Notebooks/1e100ibu/saves/'):\n",
        "        torch.save(self.phis,  f'{dest_dir}{datetime.datetime.now()}-theta')\n",
        "        torch.save(self.theta, f'{dest_dir}{datetime.datetime.now()}-phis')\n",
        "\n",
        "    def load_weights(self, src_path):\n",
        "        self.theta = torch.load(f'src_path-theta')\n",
        "        self.phis  = torch.load(f'src_path-phis')\n",
        "    \n",
        "\n",
        "    def _linear_assignement(self, costs):\n",
        "        return linear_sum_assignment(costs, maximize=True)\n",
        "    \n",
        "    def aspecs_assignments(self):\n",
        "        pass\n",
        "    \n",
        "    def train(self, epoch_count=1):\n",
        "        train_size = int(0.8 * len(self.ds))\n",
        "        # train_size = 1000 # use latter; this is for debuggin' only\n",
        "        test_size = len(self.ds) - train_size\n",
        "\n",
        "        self.train_ds, self.test_ds = random_split(self.ds, [train_size, test_size], generator=torch.Generator().manual_seed(42)) # let's fix RNG seed for now\n",
        "\n",
        "        batch_size = 5\n",
        "\n",
        "        try:\n",
        "            for epoch in range(epoch_count):\n",
        "                ic(epoch)\n",
        "                for i, batch in enumerate(tqdm(grouper(self.train_ds, batch_size), total=(len(self.train_ds) / batch_size))):\n",
        "                    sentence_ll_losses = []\n",
        "                    for (rev_sents_ids, review_aspects_scores) in batch:\n",
        "                        rev_thetas = self.rev_words_thetas(rev_sents_ids)\n",
        "                        rev_phis   = self.rev_words_phis(rev_sents_ids)\n",
        "                        res_sents_scores = torch.stack(\n",
        "                            [\n",
        "                            rev_thetas[j].sum(dim=0) + torch.tensor(tuple(rev_phis[j][a][:, review_aspects_scores[a]].sum() for a in range(self.ds.aspect_count))).to(dev) # 1 x aspect count\n",
        "                            for j in range(len(rev_sents_ids))\n",
        "                            ],\n",
        "                        ) # sent count x aspect count\n",
        "                        sents_aspect_preds_max = torch.argmax(res_sents_scores, dim=1)\n",
        "                        row_ind, col_ind = self._linear_assignement(costs=res_sents_scores.detach().cpu().numpy())\n",
        "                        sents_aspect_preds_linear = sents_aspect_preds_max\n",
        "                        \n",
        "                        sents_aspect_preds_linear[row_ind] = torch.from_numpy(col_ind).to(dev)\n",
        "                        \n",
        "                        for j in range(len(rev_sents_ids)):\n",
        "                            sen_thetas = rev_thetas[j]\n",
        "                            sen_phis   = rev_phis[j]\n",
        "\n",
        "                            # (most likely) aspect assignment (5)\n",
        "                            aspect_pred = sents_aspect_preds_linear[j]\n",
        "\n",
        "                            # sentence likelihood (6)\n",
        "                            aspect_rating = review_aspects_scores[aspect_pred]\n",
        "                            theta_score_ll = sen_thetas[:, aspect_pred].sum()\n",
        "                            phi_score_ll = sen_phis[aspect_pred][:, aspect_rating].sum()\n",
        "\n",
        "                            ic(theta_score_ll + phi_score_ll)\n",
        "                            ic(theta_score_ll)\n",
        "                            ic(phi_score_ll)\n",
        "                            ll = torch.log(theta_score_ll + phi_score_ll)\n",
        "                            sentence_ll_losses.append(-ll)\n",
        "\n",
        "                            # if 0 == i % 50000:\n",
        "                            #     ic(ll)\n",
        "                        \n",
        "                    ll_loss = torch.stack(sentence_ll_losses).sum()\n",
        "                    self._optim.zero_grad()\n",
        "                    ll_loss.backward()\n",
        "                    self._optim.step()\n",
        "\n",
        "                    if 0 == i % 100:\n",
        "                        ic(i)\n",
        "                        ic(ll_loss)\n",
        "                        self.dump_weights()\n",
        "                    \n",
        "        except KeyboardInterrupt:\n",
        "            print('Interrupted.')\n",
        "        except Exception as e:\n",
        "            raise e\n"
      ],
      "metadata": {
        "id": "05JPg4ATQTOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(rb)"
      ],
      "metadata": {
        "id": "-BbREMkrBEph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()"
      ],
      "metadata": {
        "id": "iGr0C7WLqsu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MepV7uCBul-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}