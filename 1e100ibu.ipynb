{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1e100ibu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madziejm/1e100-ibu/blob/master/1e100ibu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary"
      ],
      "metadata": {
        "id": "ZMhH5l9MeZho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dependencies"
      ],
      "metadata": {
        "id": "CT48xPyXJHy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'dev = {dev}')"
      ],
      "metadata": {
        "id": "Sl9RJDTxJOTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db677126-945f-4b3e-befe-d70aa46753de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev = cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet icecream\n",
        "from icecream import ic"
      ],
      "metadata": {
        "id": "Yet9JpBt8kcx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset representation"
      ],
      "metadata": {
        "id": "JcpSzAJmFV56"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ8cumFHEEQd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beda4d36-985b-48fa-cac5-2b507244d872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 6.0 MB 6.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 628 kB 39.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 22.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 451 kB 31.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 43.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 943 kB/s \n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "\u001b[?25hCollecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install 'spacy<3.3.0,>=3.2.0' --quiet\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show spacy | egrep Version\n",
        "# we want SpaCy 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvQgSM-fB7YP",
        "outputId": "f530c196-9922-466a-c7ff-3efc8af63372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### review example"
      ],
      "metadata": {
        "id": "38ztJX3eRuqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#     \"\"\"\n",
        "#     beer/name: John Harvards Simcoe IPA\n",
        "#     beer/beerId: 63836\n",
        "#     beer/brewerId: 8481\n",
        "#     beer/ABV: 5.4\n",
        "#     beer/style: India Pale Ale &#40;IPA&#41;\n",
        "#     review/appearance: 4/5\n",
        "#     review/aroma: 6/10\n",
        "#     review/palate: 3/5\n",
        "#     review/taste: 6/10\n",
        "#     review/overall: 13/20\n",
        "#     review/time: 1157587200\n",
        "#     review/profileName: hopdog\n",
        "#     review/text: On tap at the Springfield, PA location. Poured a deep and cloudy orange (almost a copper) color with a small sized off white head. Aromas or oranges and all around citric. Tastes of oranges, light caramel and a very light grapefruit finish. I too would not believe the 80+ IBUs - I found this one to have a very light bitterness with a medium sweetness to it. Light lacing left on the glass.\n",
        "#     \"\"\""
      ],
      "metadata": {
        "id": "1rYfSug_5Vlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dataset representation"
      ],
      "metadata": {
        "id": "_yZeL2NQcSjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import io\n",
        "import spacy # nlp toolkit\n",
        "import gc # garbage collector interface\n",
        "import re\n",
        "\n",
        "class RateBeerReviews(torch.utils.data.Dataset):\n",
        "    def __init__(self, filepath='/content/SNAP-Ratebeer.txt', reviews_max=float('inf')):\n",
        "        self.aspects = ['appearance', 'aroma', 'palate', 'taste', 'overall']\n",
        "        self.aspect_count = len(self.aspects)\n",
        "        self.aspect_max = [5 + 1, 10 + 1, 5 + 1, 10 + 1, 20 + 1]\n",
        "        self._aspect_ratings = [ [] for _ in self.aspects ]\n",
        "        self._texts = []\n",
        "        self.unkn_tok = \"<unk>\" # unknown/out of vocabulary token\n",
        "        self._len = 0\n",
        "        self._fetch_data(filepath, reviews_max)\n",
        "        self._post_process()\n",
        "\n",
        "    def _fetch_data(self, filepath, reviews_max):\n",
        "        with io.open(filepath, encoding='utf-8') as f:\n",
        "            for line in tqdm(f, total=(40938282 if reviews_max == float('inf') else reviews_max * 14), desc='Reading data'):\n",
        "                if line == '\\n': # separator\n",
        "                    self._len += 1\n",
        "                    if reviews_max <= self._len:\n",
        "                        break\n",
        "                elif line.startswith('review/appearance: '):\n",
        "                    line = line[len('review/appearance: '):]\n",
        "                    self._aspect_ratings[0].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/aroma: '):\n",
        "                    line = line[len('review/aroma: '):]\n",
        "                    self._aspect_ratings[1].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/palate: '):\n",
        "                    line = line[len('review/palate: '):]\n",
        "                    self._aspect_ratings[2].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/taste: '):\n",
        "                    line = line[len('review/taste: '):]\n",
        "                    self._aspect_ratings[3].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/overall: '):\n",
        "                    line = line[len('review/overall: '):]\n",
        "                    self._aspect_ratings[4].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/text: '):\n",
        "                    line = line[len('review/text: '):]\n",
        "                    if line.startswith('UPDATED:'):\n",
        "                        line = line[len(\"UPDATED: APR 29, 2008\"):] # drop prefix\n",
        "                    line = re.sub('~', ' ', line.strip()) # remove whitespace incl. trailing newline and tildes that can be found in data for some reason\n",
        "                    if line:\n",
        "                        self._texts.append(line)\n",
        "                    else: # some reviews do not have associated text; unwind (remove) their ratings for each aspect\n",
        "                        for aspect_ratings in self._aspect_ratings:\n",
        "                            aspect_ratings.pop()\n",
        "                        self._len -= 1\n",
        "\n",
        "    def _post_process(self):\n",
        "        nlp = spacy.util.get_lang_class('en')()\n",
        "        nlp.add_pipe(\"sentencizer\", config={\"punct_chars\": ['.', '?', '!']})\n",
        "        nlp.Defaults.stop_words |= { '-', '+'}\n",
        "        nlp.Defaults.stop_words -= {'mostly', 'whole', 'indeed', 'quite', 'ever', 'nothing', 'perhaps', 'not', 'no', 'only', 'well', 'really', 'except'}\n",
        "        print(\"Spacy pipe (tokenization&sentence split)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = [tuple(list(tok.lower_ for tok in sent if not tok.is_stop and not tok.is_punct and not tok.is_space and len(tok) > 2) for sent in doc.sents) for doc in nlp.pipe(self._texts)]\n",
        "        print(\"Building vocab (word-id mapping)..\")\n",
        "        gc.collect(generation=0) # force garbage collection\n",
        "        gc.collect(generation=1) # force garbage collection\n",
        "        gc.collect(generation=2) # force garbage collection\n",
        "        self.vocab = build_vocab_from_iterator((sent for text in self._texts for sent in text), specials=[self.unkn_tok], min_freq=5)\n",
        "        self.vocab.set_default_index(self.vocab[self.unkn_tok])\n",
        "        print(\"Mapping words to ids..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = [tuple(self.vocab.lookup_indices(sent) for sent in text) for text in self._texts]\n",
        "        gc.collect() # force garbage collection\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        sentences = self._texts[i]\n",
        "        ratings = tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count))\n",
        "        return (sentences, ratings)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len"
      ],
      "metadata": {
        "id": "C1MCM0dWB93e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to read dataset from dataset file, set FETCH_RATEBEER to true in the cell below and RECREATE_PICKLE to True. If you left them untouched, it'lle be read from serialized `RateBeerReviews` class object instead of parsing text file."
      ],
      "metadata": {
        "id": "oy1qrIH1CAlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "export FETCH_RATEBEER=false\n",
        "if [ \"$FETCH_RATEBEER\" = true ] && [ -e $RATEBEER_FILE ]\n",
        "then # original dataset\n",
        "    export RATEBEER_FILE='/content/SNAP-Ratebeer.txt'\n",
        "    gdown --id '12tEEYQcHZtg5aWyfIiWWVIDAJNT-5d_T' # https://drive.google.com/file/d/12tEEYQcHZtg5aWyfIiWWVIDAJNT-5d_T/view?usp=sharing\n",
        "    echo \"Dataset head (trailing newline makes entry end): \"\n",
        "    head -n 16 $RATEBEER_FILE\n",
        "    iconv -f ISO-8859-1 -t UTF-8 $RATEBEER_FILE -o {RATEBEER_FILE}.new && mv {RATEBEER_FILE}.new $RATEBEER_FILE\n",
        "else # pickle\n",
        "    gdown --id '1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j' # https://drive.google.com/file/d/1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j/view?usp=sharing\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_QTo-xBCelp",
        "outputId": "4ee4deb5-deb9-4740-a78e-f7af77123bc8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j\n",
            "To: /content/ratebeer.pickle\n",
            "\r  0%|          | 0.00/322M [00:00<?, ?B/s]\r  1%|1         | 4.72M/322M [00:00<00:22, 14.2MB/s]\r  7%|7         | 24.1M/322M [00:00<00:15, 19.6MB/s]\r 11%|#         | 34.1M/322M [00:00<00:12, 22.5MB/s]\r 16%|#5        | 50.9M/322M [00:00<00:08, 30.3MB/s]\r 21%|##1       | 67.6M/322M [00:01<00:07, 34.3MB/s]\r 28%|##8       | 90.2M/322M [00:01<00:05, 46.0MB/s]\r 32%|###1      | 102M/322M [00:01<00:05, 43.7MB/s] \r 38%|###8      | 124M/322M [00:01<00:03, 57.4MB/s]\r 43%|####2     | 137M/322M [00:01<00:03, 52.6MB/s]\r 49%|####8     | 156M/322M [00:02<00:02, 67.3MB/s]\r 53%|#####2    | 169M/322M [00:02<00:02, 57.2MB/s]\r 60%|######    | 193M/322M [00:02<00:01, 74.2MB/s]\r 65%|######4   | 208M/322M [00:02<00:01, 58.7MB/s]\r 68%|######8   | 220M/322M [00:03<00:01, 56.1MB/s]\r 73%|#######3  | 235M/322M [00:03<00:01, 62.1MB/s]\r 76%|#######6  | 245M/322M [00:03<00:01, 52.7MB/s]\r 81%|########  | 261M/322M [00:03<00:00, 65.7MB/s]\r 86%|########6 | 277M/322M [00:03<00:00, 72.4MB/s]\r 92%|#########2| 297M/322M [00:03<00:00, 89.4MB/s]\r 97%|#########6| 311M/322M [00:04<00:00, 82.3MB/s]\r100%|##########| 322M/322M [00:04<00:00, 77.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle # serialize lib\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/drive')\n",
        "\n",
        "DATASET_PICKLE='/content/ratebeer.pickle'\n",
        "RECREATE_PICKLE = False\n",
        "\n",
        "if RECREATE_PICKLE:\n",
        "    with open(DATASET_PICKLE, 'wb') as f:\n",
        "        rb = RateBeerReviews()\n",
        "        print('Dumping..')\n",
        "        pickle.dump(rb, f)\n",
        "else:\n",
        "    with open(DATASET_PICKLE, 'rb') as f:\n",
        "        rb = pickle.load(f)"
      ],
      "metadata": {
        "id": "N5zmSYDBCg4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5059cf21-9267-4d3b-d0a2-a8f90cdeabcc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word ID-s count and 1K of least common words\n",
        "print(len(rb.vocab.get_itos()))\n",
        "print(rb.vocab.get_itos()[-1000:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ0p4rxp2UJE",
        "outputId": "bcdb809d-9f8d-4e35-c583-efd848b0f032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "111047\n",
            "['tonto', 'too.<font', 'too^co2', 'tood', 'tooled', 'tooooooo', 'top-3', 'top.4', 'top3', 'topfermenting', 'toppd', 'topsail', 'torani', 'tord', 'torments', 'tornadoes', 'torok', 'toronto2010', 'toronto2011', 'torpedoes', 'torrada', 'torre', 'tortue', 'tosaty', 'tosay', 'tossin', 'tosta', 'tostados', 'totall', 'toughed', 'toughie', 'touhc', 'toungh', 'tourette', 'tournai', 'tournure', 'towner', 'towsend', 'tpa', 'tpa975', 'tpd', 'tpe', 'tpll', 'tpo', 'tracce', 'tracis', 'tradicionais', 'traditinal', 'traditiona', 'traight', 'trailblazer', 'trainstation', 'traipse', 'traitements', 'traitre', 'trajo', 'tramping', 'tranformed', 'tranforms', 'tranquillit', 'tranquillo', 'transaprent', 'transcendance', 'transcontinental', 'transfusion', 'translucense', 'translucient', 'trapani', 'trapissed', 'trappest', 'trappistl', 'trauben', 'travaill', 'trbung', 'treason', 'treaty', 'trembles', 'treo', 'trespass', 'trickster', 'trided', 'trinkable', 'tripelbock', 'tripels-', 'tripely', 'tripical', 'triptens', 'trket', 'trkke', 'trobo', 'trochs', 'trodsalt', 'troical', 'trolli', 'tromsdalen', 'trondhj', 'tropez', 'troppi', 'troubador', 'trovarla', 'trown', 'trpel', 'trtnet', 'trubbel', 'trudged', 'trumpington', 'trun', 'trus', 'truthfulness', 'trykbar', 'tszik', 'tterem', 'tthanks', 'tthick', 'tubular', 'tudnk', 'tuity', 'tul', 'tulajdonsga', 'tungan', 'tunnare', 'tuoksua', 'tupac', 'turbanado', 'turbolent', 'turnig', 'turnoffs', 'turntables', 'tus', 'tusen', 'tussled', 'tuttifruiti', 'tutty', 'twaagstuk', 'twatted', 'twentieth', 'twiggamortis420', 'twinging', 'twissup', 'twon', 'txhazy', 'tybee', 'tycka', 'tydligare', 'tydligen', 'tydligt', 'tyktflydende', 'tylerpow', 'tyoical', 'tyou', 'typicity', 'tyra', 'tyrannical', 'tyrolean', 'tyrone', 'tyrrell', 'uality', 'uan', 'ubbly', 'ubc', 'uberhopped', 'uberly', 'uberschwein', 'ubersweet', 'ubertavern', 'ubiqutous', 'ubt', 'ud.:rdgylden', 'uder', 'udgave', 'udn', 'udnerlying', 'udover', 'uere', 'uert', 'uglyness', 'ugshmash', 'ugualmente', 'ugyanaz', 'uhmmm', 'uis', 'uiterlijk', 'ukraines', 'ulimately', 'ulkonltn', 'ulric', 'ultimatly', 'ultradry', 'ultrasweet', 'ultraviolet', 'umass', 'umbria', 'umfeld', 'umiskendelig', 'umlaut', 'unabridged', 'unadventerous', 'unambiguously', 'unanswered', 'unappetisingly', 'unattraktiv', 'unbalaned', 'unbareable', 'unbefitting', 'unbeleiveable', 'unbelgian', 'unbelivably', 'unbittered', 'uncaps', 'uncasville', 'uncellared', 'uncharitable', 'uncharitably', 'unclog', 'uncomplexing', 'uncompliacted', 'unconventionally', 'unctuosity', 'unctuously', 'undaunting', 'undefinierbar', 'underatted', 'underbaked', 'underberg', 'undercharged', 'undergirds', 'underlayment', 'underlied', 'underones', 'underpass', 'underplay', 'underroasted', 'undertones-', 'undertons', 'underyling', 'undewhelming', 'undignified', 'undoes', 'undoubted', 'undrinkable-', 'undrunk', 'unearthing', 'uneasiness', 'unecessarily', 'unedited', 'unenviable', 'unerneath', 'unexisting', 'unexpectedness', 'unexpectingly', 'unexpensive', 'unfashionable', 'unfitered', 'unforchinley', 'unfortnately', 'unfortunaly', 'unfortunatlly', 'unfortunetely', 'unfortunitly', 'unfront', 'unfullfilled', 'ungeeignet', 'ungefr', 'ungewhnliche', 'ungiven', 'unglazed', 'unheated', 'unicum', 'unicycle', 'unidentifiably', 'unido', 'unification', 'uniforms', 'unimodal', 'unimpress', 'unimpressing', 'unintelligible', 'uninterupted', 'uninticing', 'uniqu', 'universities', 'unky', 'unlcear', 'unlikley', 'unlke', 'unlovely', 'unmentionables', 'unmentioned', 'unmodified', 'unnamable', 'unnatrlich', 'unnoticeably', 'unpainted', 'unpatriotic', 'unpleassant', 'unpoured', 'unpredictably', 'unpretensious', 'unpronounceable', 'unraveled', 'unrealistically', 'unremarable', 'unremarkeable', 'unremitting', 'unrepresentative', 'unrest', 'unsanitary', 'unseasonable', 'unsettles', 'unshaken', 'unshelled', 'unsparing', 'unspectacularly', 'unstimulating', 'unstopable', 'unstouty', 'unsually', 'unsugared', 'unsustained', 'unsweeten', 'unsyrupy', 'untapped', 'untart', 'untempered', 'untenable', 'unterer', 'unterordnet', 'unterstzt', 'untertniger', 'untied', 'untight', 'untypischer', 'unusualness', 'unwaveringly', 'unwieldly', 'unwillingness', 'unwise', 'unwitting', 'unworldly', 'uop', 'up)', 'up@lkonsortiet', 'upa', 'upcharge', 'upcider', 'upcomming', 'updrafts', 'upend', 'upfornt', 'uprooted', 'upsides', 'upsurge', 'uraljk', 'urals', 'urbandale', 'urbane', 'urbeer', 'urbnhaut', 'urethra', 'uri', 'urinates', 'urinish', 'urk', 'urlo', 'ursa', 'urthels', 'us-', 'uscire', 'useable', 'usefulness', 'user-49371.htm&amp;g', 'user-7472.htm', 'usofa', 'usre', 'usual-', 'usurping', 'utenos', 'uterly', 'uti', 'utilises', 'utlevert', 'utrymme', 'utseendet', 'uttery', 'utty', 'uumph', 'uuugh', 'uwe', 'uzbekistan', 'v.2008', 'v.good', 'v.v', 'vabf', 'vabf10', 'vacsora', 'vaction', 'vacuumed', 'vadalia', 'vaders', 'vagaries', 'vaginal', 'vagyis', 'vailima', 'vainglorious', 'vaker', 'valaki', 'valero', 'valetta', 'validates', 'valpo', 'valults', 'vammel', 'vamp', 'vanallia', 'vanashing', 'vande', 'vanescent', 'vanha', 'vanilin', 'vanilla.the', 'vanillabeans', 'vanillamike', 'vaniulla', 'vanlse', 'vant', 'varbonation', 'vare', 'varende', 'varken', 'varmasti', 'vart', 'vasas', 'vases', 'vasten', 'vatican', 'vatted', 'vatten', 'vaudree', 'vautour', 'vax', 'vborn', 'vecchia', 'vega', 'vegasamber', 'vegasblack', 'vegeatbles', 'vegetablely', 'vegetativeness', 'vegis', 'vegstable', 'veilbronn', 'veiling', 'velkyal', 'vellavet', 'velsmagende', 'veneto', 'verall', 'verbessern', 'verbesserungswrdig', 'verbirgt', 'verbranntes', 'verdade', 'verdient', 'verdrngen', 'verfllt', 'vergisst', 'verhindern', 'verhunzt', 'verit', 'verkauft', 'vermischen', 'verra', 'verraskning', 'versato', 'versin', 'versio', 'version)', 'verticalbaconstrips', 'verwaschen', 'veryh', 'veryl', 'verysmooth', 'vessle', 'vetro', 'vexes', 'viajo', 'vibing', 'vick', 'victory-', 'vidare', 'viegar', 'viellissement', 'vielschichtiger', 'viera', 'viernes', 'viewable', 'viewuser.asp?userid=5', 'viggio', 'vigs', 'vilest', 'vilja', 'vilken', 'villainous', 'villains', 'vinagerette', 'vinagry', 'vinbrsen', 'vindicate', 'vindication', 'vinegums', 'vinella', 'vinetage', 'vinhandlen', 'vinifera', 'vinnare', 'vintage08aa', 'vintagecellar', 'vintagesampled', 'vinterpplen', 'viogner', 'violaceo', 'violente', 'virbrant', 'virgmz', 'virke', 'virtaully', 'virtualmente', 'virturally', 'viru', 'visble', 'viscouse', 'visionaries', 'visionary', 'vismara', 'visoucs', 'visqueuses', 'visquous', 'vissa', 'vistas', 'viste', 'vitae', 'vitals', 'vitter', 'viturally', 'vladivostok', 'vlavvgd', 'vlavvgt', 'vlo', 'vnilla', 'vnta', 'voer', 'voghera', 'voiced', 'voiles', 'voimakkaan', 'vokser', 'volante', 'volcanoes', 'voldsom', 'volen', 'volg', 'volks', 'volkswagen', 'volonte', 'volour', 'voluminse', 'volutamente', 'volvo', 'voraciously', 'vorbei', 'vorherigen', 'vorrebbe', 'vorweg', 'voulais', 'voulez', 'vrkar', 'vroom', 'vrsen', 'vrsessrga', 'vrste', 'vsen', 'vulture', 'vwey', 'vwry', 'vxj', 'vypt', 'w.o.s.f.', 'w10', 'w12', 'waaaayyyyy', 'waaayyyy', 'waage', 'waagstuck', 'wach', 'wackys', 'waclear', 'wacohazy', 'waddington', 'wadsworth', 'waffel', 'waffers', 'waffing', 'wagger', 'wahoos', 'waitng', 'waizen', 'wakeandbake', 'waken', 'walden', 'waldmnchen', 'waldschmidt', 'walgett', 'walhalla', 'wallaces', 'waller', 'wallerstein', 'wallflowers', 'wallhall', 'waltzes', 'wamred', 'wann', 'wantabe', 'wantsum', 'warery', 'warg', 'wariness', 'warlord', 'warm.<p', 'warmen', 'warmer.<font', 'warmgolden', 'warmig', 'warminster', 'warmong', 'warms--', 'warten', 'wassabi', 'wasw', 'watauga', 'waterchestnut', 'wateredge', 'watership', 'waterz', 'wavers1-', 'waxpaper', 'waya', 'wayland', 'ways-', 'wayyyyyyyy', 'wc2a', 'wcb', 'wdc', 'wealthiest', 'weard', 'wearisome', 'wearying', 'weavers', 'weavings', 'webmaster', 'wed', 'weeare', 'weeobese', 'weighter', 'weighti', 'weihestephaner', 'weinartigen', 'weine', 'weinfelden', 'weisens', 'weissbieren', 'weissenohe', 'weisss', 'weiterfhrend', 'weizebock', 'weizen-', 'weizenbockish', 'weizenesque', 'weizze', 'welballanced', 'well.honestly', 'wella', 'welle', 'weller', 'wellie', 'wellspring', 'welosiped', 'welton', 'wend', 'wende', 'wending', 'wensleydale', 'went-', 'werewolves', \"wern't\", 'wernesgruener', 'westheimer', 'westvelteren', 'westville', 'westward', 'weth', 'wetteren', 'wghite', 'whaite', 'whalemans', 'whallops', 'whammer', 'whar', 'what?', 'whatl', 'whatver', 'whealt', 'wheat-)malty', 'wheat--', 'wheatmeal', 'wheatmiser', 'wheatyeast', 'wheetabix', 'whete', 'whiile', 'whikey', 'whiksy', 'while.<font', 'whiles', 'whille', 'whimps', 'whinos', 'whipcrack', 'whirlwinds', 'whiskey/', 'whiskymalt', 'whist', 'whistlers', 'whita', 'whittier', 'whittington', 'whizzing', 'whkingery', 'whodda', 'whoevers', 'whoopin', 'whos', 'whowouldathunkit', 'whre', 'whta', 'wickham', 'widened', 'widerlichen', 'widgetised', 'widgety', 'wiehenstephaner', 'wife.<font', 'wiffing', 'wifi', 'wiggum', 'wiggy', 'wiil', 'wiill', 'wild4mild', 'wildfires', 'wildman', 'wilhelm', 'williamsburgs', 'willimantics', 'willingen', 'willinger', 'wilma', 'wilmarths', 'wilmer', 'wime', 'wimped', 'winderful', 'windsheim', 'wine--', 'wineandcheeseplace.com', 'winegar', 'winerous', 'winet', 'winked', 'winnemucca', 'wint', 'wintefest', 'wintertraum', 'wipa', 'wiregrass', 'wirey', 'wirt', 'wisconsonite', 'wised', 'wislpy', 'witbiery', 'withclean', 'withdraw', 'withdrawals', 'withit', 'withmore', 'withstegosaurus', 'withut', 'withvery', 'withwith', 'witless', 'wittekirk', 'wittersham', 'wixa', 'wk2', 'wk4', 'wknd', 'wnsche', 'woddsy', 'wodnerful', 'woft', 'wohlschmeckendes', 'wolfberries', 'wolters', 'wolverine', 'wombwell', 'wonderbready', 'woner', 'woodburning', 'woodd', 'woodmaster', 'woodpile', 'woodsetton', 'woodsman52', 'woodstain', 'woodstainer', 'woolies', 'woolworths', 'wooooow', 'woooow', 'wooy', 'wopuld', 'worchestshire', 'word-', 'words-', 'wordsley', 'workbench', 'workmate', 'workmates', 'wormtown', 'worng', 'worstest', 'wortyness', 'wos', 'wost', 'wouldbe', 'wouldve', 'woule', 'woulndt', 'wounderfull', 'wouuld', 'wove', 'wower', 'wowsa', 'wowy', 'wracked', 'wrangle', 'wrangling', 'wrapup', 'wresting', 'wriggleyville', 'wrings', 'wristed', 'writhes', 'wrmender', 'wsith', 'wsnt', 'wsweet', 'wtffffdotf', 'wtihout', 'wuick', 'wuppertaler', 'wurstel', 'wurzel', 'wweet', 'wwod', 'wwstout', 'www.erzbiershop.ch', 'wykeham', 'xcel', 'xmass', 'xmastasting', 'xtal', 'xxxl', 'yabba', 'yachats', 'yaks', 'yakult', 'yangshuo', 'yankton', 'yap', 'yaphet', 'yardish', 'yargh', 'yarlington', 'yarr', 'yasawa', 'yeadt', 'yeahh', 'yeahhhh', 'yeard', 'yeart', 'yeast(s', 'yeast)', 'yeasta', 'yeasterday', 'yeastlike', 'yeastty', 'yeastu', 'yeastys', 'yebisus', 'yechh', 'yelloew', 'yellos', 'yellowbelly', 'yellowe', 'yellowgold', 'yellowm', 'yellowtinted', 'yelof77', 'yeloowish', 'yeow', 'yerevan', 'yerk', 'yesprscloudy', 'yet-', 'yetmans', 'yetti', 'yhellow', 'yona', 'yoou', 'yorkamber', 'yorkshires', 'you--', 'you.<font', 'youlike', 'younge', 'youngen', 'younger35', 'youngin', 'youngun', 'yowser', 'ypu', 'yucca', 'yuenglingish', 'yum!', 'yummily', 'yummy-', 'yumy', 'yunegling', 'zam', 'zanthrus13', 'zauberberg', 'zay', 'zdravi', 'zealanders', 'zealousness', 'zeichen', 'zenwolf', 'zestett', 'zetsy', 'zhats', 'zhigulevskoye', 'zhuhai', 'ziel', 'zigzags', 'zimt', 'ziplock', 'zischke', 'zitronenartig', 'zitronennoten', 'zlden', 'zlsemnek', 'zminor', 'znovu', 'zodos', 'zoersel', 'zoiglbier', 'zoiglhaus', 'zomerbierfestival', 'zommer', 'zounds', 'zuccheri', 'zuccherino', 'zuccherosa', 'zuchinni', 'zugabe', 'zuid', 'zuidentrein', 'zurckhaltenden', 'zustande', 'zvilga', 'zwanzger', 'zweet', 'zwiefalter', 'zwlf', 'zwr', 'zymergy', 'zytholoog', 'zzzzzzzz', 'zzzzzzzzzz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training (implementation of $(1)$)"
      ],
      "metadata": {
        "id": "yOc_2b-E4o0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "import datetime\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, dataset):\n",
        "        self.ds = dataset\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        word_count = len(self.ds.vocab.get_itos())\n",
        "        self.theta = torch.rand((word_count, self.ds.aspect_count), requires_grad=True).to(dev)\n",
        "        with torch.no_grad():\n",
        "            self.theta *= 0.9\n",
        "            # enforce 1 initialization on aspect name (page 4)\n",
        "            aspect_ids = self.ds.vocab.lookup_indices(self.ds.aspects)\n",
        "            self.theta[aspect_ids, :] = 1\n",
        "        self.theta.grad = torch.zeros_like(self.theta)\n",
        "\n",
        "        aspect_rating_count = [6, 11, 6, 11, 21] # TODO remove\n",
        "\n",
        "        # introduce separate phi for each aspect\n",
        "        self.phis = [torch.rand((word_count, aspect_rating_count[i]), requires_grad=True).to(dev) for i in range(self.ds.aspect_count)]\n",
        "\n",
        "        # normalize that sum across all words is 1 for a given aspect (eq. 7)\n",
        "        self.phis = [phi / phi.sum(dim=0) for phi in self.phis]\n",
        "        for phi in self.phis:\n",
        "            phi.grad = torch.zeros_like(phi) # otherwise we would got \"'NoneType' object...\" in the first train iteration\n",
        "    \n",
        "    def rev_words_thetas(self, rev_sens_ids):\n",
        "        \"\"\"\n",
        "        TODO comment sentence_aspects_likelihood_theta\n",
        "        \"\"\"\n",
        "        return [self.theta[sen_ids] for sen_ids in rev_sens_ids]\n",
        "\n",
        "    def rev_words_phis(self, rev_sens_ids):\n",
        "        \"\"\"\n",
        "        TODO comment sentence_aspects_likelihood_phi\n",
        "        \"\"\"\n",
        "        return [[self.phis[aspect_idx][sen_ids, :] for aspect_idx in range(self.ds.aspect_count)] for sen_ids in rev_sens_ids]\n",
        "    \n",
        "    def dump_weights(self, dest_dir='/drive/MyDrive/Colab Notebooks/1e100ibu/saves/'):\n",
        "        torch.save(self.phis,  f'{dest_dir}{datetime.datetime.now()}-theta')\n",
        "        torch.save(self.theta, f'{dest_dir}{datetime.datetime.now()}-phis')\n",
        "\n",
        "    def load_weights(self, src_path):\n",
        "        self.theta = torch.load(f'src_path-theta')\n",
        "        self.phis  = torch.load(f'src_path-phis')\n",
        "    \n",
        "\n",
        "    # def linear_assignement(self, rev_sens, rev_scores):\n",
        "    #     costs = np.zero(shape=(len(rev_sens), self.ds.aspect_count))\n",
        "\n",
        "    #     for k in range(self.ds.aspect_count):\n",
        "    #         # \n",
        "    #     linear_sum_assignment(costs)\n",
        "    \n",
        "    \n",
        "    # def sentence_likelihood(self, sen_ids): # TODO remove?\n",
        "    #   pass\n",
        "    \n",
        "    def train(self, epoch_count=1):\n",
        "        train_size = int(0.8 * len(self.ds))\n",
        "        # train_size = 1000 # use latter; this is for debuggin' only\n",
        "        test_size = len(self.ds) - train_size\n",
        "\n",
        "        self.train_ds, self.test_ds = random_split(self.ds, [train_size, test_size], generator=torch.Generator().manual_seed(42)) # let's fix RNG seed for now\n",
        "\n",
        "        try:\n",
        "            for epoch in range(epoch_count):\n",
        "                ic(epoch)\n",
        "                for i, (rev_sents_ids, review_aspects_scores) in enumerate(tqdm(self.train_ds)):\n",
        "                    rev_thetas = self.rev_words_thetas(rev_sents_ids)\n",
        "                    rev_phis   = self.rev_words_phis(rev_sents_ids)\n",
        "                    sentence_ll_losses = []\n",
        "                    for j in range(len(rev_sents_ids)):\n",
        "                        sen_thetas = rev_thetas[j]\n",
        "                        sen_phis   = rev_phis[j]\n",
        "\n",
        "                        # (most likely) aspect assignment (5)\n",
        "                        phi_score_aa = (list((sen_phis[a][:, review_aspects_scores[a]]).sum() for a in range(self.ds.aspect_count)))\n",
        "                        phi_score_aa = torch.tensor(phi_score_aa).to(dev)\n",
        "                        theta_score_aa = sen_thetas.sum(dim=0) # sum across words, keep aspects dim\n",
        "\n",
        "                        aspect_pred = int(torch.argmax(torch.nn.functional.softmax(theta_score_aa + phi_score_aa, dim=-1)).item())\n",
        "\n",
        "                        # sentence likelihood (6)\n",
        "                        aspect_rating = review_aspects_scores[aspect_pred]\n",
        "                        theta_score_ll = sen_thetas[:, aspect_pred].sum()\n",
        "                        phi_score_ll = sen_phis[aspect_pred][:, aspect_rating].sum()\n",
        "\n",
        "                        ll = torch.log(theta_score_ll + phi_score_ll)\n",
        "                        sentence_ll_losses.append(-ll)\n",
        "\n",
        "                        if 0 == i % 50000:\n",
        "                            ic(ll)\n",
        "                    \n",
        "                    regularization_loss = torch.mean(torch.square(self.theta))\n",
        "                    for phi in self.phis:\n",
        "                        regularization_loss = regularization_loss + torch.mean(torch.square(phi))\n",
        "                    regularization_loss = regularization_loss * 0.1\n",
        "                    ll_loss = torch.stack(sentence_ll_losses).mean()\n",
        "                    loss = regularization_loss + ll_loss\n",
        "                    loss.backward()\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        self.theta -= 0.00001 * self.theta.grad\n",
        "                        self.phis = [phi - 0.00001 * phi.grad if phi.grad != None else phi for phi in self.phis]\n",
        "                        if 0 == i % 50000:\n",
        "                            ic(i)\n",
        "                            ic(regularization_loss)\n",
        "                            ic(ll_loss)\n",
        "                            ic(loss)\n",
        "                            self.dump_weights()\n",
        "                        self.theta.grad.zero_()\n",
        "                        for phi in self.phis:\n",
        "                            if phi.grad is not None:\n",
        "                                phi.grad.zero_()\n",
        "        except KeyboardInterrupt:\n",
        "            print('Interrupted.')\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "        \n",
        "\n",
        "    # def sentence_aspects_likelihood(sen_ids): # TODO not needed?\n",
        "    #     theta_score = sentence_aspects_likelihood_theta(sen_ids)\n",
        "    #     phi_scores = sentence_aspects_likelihood_phi(sen_ids)\n",
        "    #     ic(theta_score, phi_scores)\n",
        "        # score = torch.exp( + sentence_aspects_likelihood_phi(sen_ids, ratings))\n",
        "        # return score / score.sum()\n"
      ],
      "metadata": {
        "id": "05JPg4ATQTOW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(rb)"
      ],
      "metadata": {
        "id": "-BbREMkrBEph"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()"
      ],
      "metadata": {
        "id": "iGr0C7WLqsu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5bAzkDZ7eQZ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}