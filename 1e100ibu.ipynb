{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madziejm/1e100-ibu/blob/master/1e100ibu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMhH5l9MeZho"
      },
      "source": [
        "## Preliminary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCm7saFh0cNB"
      },
      "outputs": [],
      "source": [
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT48xPyXJHy9"
      },
      "source": [
        "#### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl9RJDTxJOTY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'dev = {dev}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMaGx0YYuCGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade3b593-dacc-4c36-ab15-5e8e51e86e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ],
      "source": [
        "try: # mount user's Google Drive if on Colab to save training artifacts\n",
        "    from google.colab import drive\n",
        "    drive.mount('/drive')\n",
        "    ROOT_DIR = '/content/'\n",
        "    MODEL_ROOT_DIR = '/drive/MyDrive/Colab Notebooks/1e100ibu/saves/'\n",
        "except ImportError:\n",
        "    ROOT_DIR = './'\n",
        "    MODEL_ROOT_DIR = './saves/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j-5-Ctv0cNJ"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet icecream\n",
        "!pip install --quiet -Iv torch==1.10.1\n",
        "!pip install --quiet -Iv torchtext==0.11.1\n",
        "!pip install --quiet gdown\n",
        "!pip install --quiet wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yet9JpBt8kcx"
      },
      "outputs": [],
      "source": [
        "from icecream import ic\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ8cumFHEEQd"
      },
      "outputs": [],
      "source": [
        "!pip install 'spacy<3.3.0,>=3.2.0' --quiet\n",
        "!python -m spacy download en_core_web_sm --quiet\n",
        "!python -m spacy download pl_core_news_md --quiet\n",
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvQgSM-fB7YP"
      },
      "outputs": [],
      "source": [
        "!pip show spacy | egrep Version\n",
        "# we want SpaCy 3, crucial!\n",
        "!pip show torch | egrep Version\n",
        "!pip show torchtext | egrep Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yZeL2NQcSjP"
      },
      "source": [
        "## Dataset representation\n",
        "Here we introduce `BaseReviews`, `RateBeerReviews`, `OcenPiwoReviews` classes. For each dataset we pick 20k most common words vocabulary and then then map them to numerical ID-s. 0th index is used for unknown token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1MCM0dWB93e"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from torchtext._torchtext import (Vocab as VocabPybind) # make use of some hidden interface\n",
        "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import gc # garbage collector interface\n",
        "import io\n",
        "import re\n",
        "import spacy # nlp toolkit\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "\n",
        "class BaseReviews(torch.utils.data.Dataset):\n",
        "    def __init__(self, aspects, aspect_max, aspect_ratings, texts, unkn_tok, _len, anchor_words):\n",
        "        self.aspects = aspects\n",
        "        self.aspect_count = len(aspects)\n",
        "        self.aspect_max = aspect_max\n",
        "        self._aspect_ratings = aspect_ratings\n",
        "        self._texts = texts\n",
        "        self.unkn_tok = unkn_tok\n",
        "        self._len = _len\n",
        "        self.anchor_words = anchor_words\n",
        "        self.vocab = None\n",
        "\n",
        "    def dump(self, dest_path, filename):\n",
        "        contents = {\n",
        "            'aspects'        : self.aspects,\n",
        "            'aspect_max'     : self.aspect_max,\n",
        "            '_aspect_ratings': self._aspect_ratings,\n",
        "            '_texts'         : self._texts,\n",
        "            'unkn_tok'       : self.unkn_tok,\n",
        "            '_len'           : self._len,\n",
        "            'anchor_words'   : self.anchor_words,\n",
        "            'vocab'          : self.vocab,\n",
        "        }\n",
        "        with open(f'{dest_path}/{filename}', 'wb') as f:\n",
        "            pickle.dump(contents, f)\n",
        "    \n",
        "    def load(self, dest_path, filename):\n",
        "        with open(f'{dest_path}/{filename}', 'rb') as f:\n",
        "            contents = pickle.load(f)\n",
        "            self.aspects         = contents['aspects']\n",
        "            self.aspect_max      = contents['aspect_max']\n",
        "            self._aspect_ratings = contents['_aspect_ratings']\n",
        "            self._texts          = contents['_texts']\n",
        "            self.unkn_tok        = contents['unkn_tok']\n",
        "            self._len            = contents['_len']\n",
        "            self.anchor_words    = contents['anchor_words']\n",
        "            self.vocab           = contents['vocab']\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # # 1 # python\n",
        "        # sentences = tuple(sent for sent in self._texts[i])\n",
        "        # ratings = tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count))\n",
        "        # 2 # tensor\n",
        "        sentences = tuple(torch.LongTensor(sent) for sent in self._texts[i])\n",
        "        # changed LongTensor to IntTensor\n",
        "        ratings = torch.IntTensor(tuple(int(self._aspect_ratings[a][i]) for a in range(self.aspect_count)))\n",
        "        # # 3 # dev\n",
        "        # sentences = tuple(torch.tensor(sent) for sent in self._texts[i])\n",
        "        # ratings = torch.tensor(tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count)))\n",
        "        return (sentences, ratings)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaRC_7rAuCG0"
      },
      "outputs": [],
      "source": [
        "class RateBeerReviews(BaseReviews):\n",
        "    \"\"\"\n",
        "    beer/name: John Harvards Simcoe IPA\n",
        "    beer/beerId: 63836\n",
        "    beer/brewerId: 8481\n",
        "    beer/ABV: 5.4\n",
        "    beer/style: India Pale Ale &#40;IPA&#41;\n",
        "    review/appearance: 4/5\n",
        "    review/aroma: 6/10\n",
        "    review/palate: 3/5\n",
        "    review/taste: 6/10\n",
        "    review/overall: 13/20\n",
        "    review/time: 1157587200\n",
        "    review/profileName: hopdog\n",
        "    review/text: On tap at the Springfield, PA location. Poured a deep and cloudy orange (almost a copper) color with a small sized off white head. Aromas or oranges and all around citric. Tastes of oranges, light caramel and a very light grapefruit finish. I too would not believe the 80+ IBUs - I found this one to have a very light bitterness with a medium sweetness to it. Light lacing left on the glass.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        aspects = ['appearance', 'aroma', 'palate', 'taste', 'overall']\n",
        "        super().__init__(\n",
        "            aspects        = aspects,\n",
        "            aspect_max     = [5, 10, 5, 10, 20],\n",
        "            aspect_ratings = [ [] for _ in aspects ],\n",
        "            texts          = [],\n",
        "            unkn_tok       = '<unk>', # unknown/out of vocabulary token\n",
        "            _len            = 0,\n",
        "            anchor_words = {\n",
        "                'appearance' : ('appearance', 'color'),\n",
        "                'aroma'      : ('aroma'),\n",
        "                'palate'     : ('palate', 'mouthfeel'),\n",
        "                'taste'      : ('taste'),\n",
        "                'overall'    : ('overall'),\n",
        "            },\n",
        "        )\n",
        "        self.pipe = None\n",
        "\n",
        "    def build(self, filepath=f'{ROOT_DIR}/SNAP-Ratebeer.txt', max_reviews=float('inf'), min_word_freq=None, max_word_count=None):\n",
        "        with io.open(filepath, encoding='utf-8') as f:\n",
        "            for line in tqdm(f, total=(40938282 if max_reviews == float('inf') else max_reviews * 14), desc='Reading data'):\n",
        "                if line == '\\n': # separator\n",
        "                    self._len += 1\n",
        "                    if max_reviews <= self._len:\n",
        "                        break\n",
        "                elif line.startswith('review/appearance: '):\n",
        "                    line = line[len('review/appearance: '):]\n",
        "                    self._aspect_ratings[0].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/aroma: '):\n",
        "                    line = line[len('review/aroma: '):]\n",
        "                    self._aspect_ratings[1].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/palate: '):\n",
        "                    line = line[len('review/palate: '):]\n",
        "                    self._aspect_ratings[2].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/taste: '):\n",
        "                    line = line[len('review/taste: '):]\n",
        "                    self._aspect_ratings[3].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/overall: '):\n",
        "                    line = line[len('review/overall: '):]\n",
        "                    self._aspect_ratings[4].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/text: '):\n",
        "                    line = line[len('review/text: '):]\n",
        "                    if line.startswith('UPDATED:'):\n",
        "                        line = line[len(\"UPDATED: APR 29, 2008\"):] # drop prefix\n",
        "                    line = re.sub('~', ' ', line.strip()) # remove whitespace incl. trailing newline and tildes that can be found in data for some reason\n",
        "                    if line:\n",
        "                        self._texts.append(line)\n",
        "                    else: # some reviews do not have associated text; unwind (remove) their ratings for each aspect\n",
        "                        for aspect_ratings in self._aspect_ratings:\n",
        "                            aspect_ratings.pop()\n",
        "                        self._len -= 1\n",
        "        self._post_process(min_word_freq, max_word_count) # 20K words should be okay\n",
        "    \n",
        "    def _fetch_nlp_pipeline(self):\n",
        "        if not self.pipe:\n",
        "            nlp = spacy.util.get_lang_class('en')()\n",
        "            nlp.add_pipe(\"sentencizer\", config={\"punct_chars\": ['.', '?', '!']})\n",
        "            nlp.Defaults.stop_words |= { '-', '+'}\n",
        "            nlp.Defaults.stop_words -= {'mostly', 'whole', 'indeed', 'quite', 'ever', 'nothing', 'perhaps', 'not', 'no', 'only', 'well', 'really', 'except'}\n",
        "            self.pipe = lambda reviews: nlp.pipe(reviews)\n",
        "    \n",
        "    def _free_nlp_pipeline(self):\n",
        "        self.nlp = None\n",
        "\n",
        "    def tokenize_reviews(self, reviews_texts: str):\n",
        "        return [tuple(list(tok.lower_ for tok in sent if not tok.is_stop and not tok.is_punct and not tok.is_space and len(tok) > 2) for sent in doc.sents if 0 != len(sent)) for doc in self.pipe(reviews_texts)]\n",
        "    \n",
        "    def id_map_reviews(self, texts):\n",
        "        return [tuple(self.vocab.lookup_indices(sent) for sent in text) for text in texts]\n",
        "    \n",
        "    def _post_process(self, min_word_freq=None, max_word_count=None):\n",
        "        assert (min_word_freq is not None) ^ bool(max_word_count is not None), \"provide one of min_word_freq and max_word_count\"\n",
        "        self._fetch_nlp_pipeline()\n",
        "        print(\"Spacy pipe (tokenization&sentence split)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = self.tokenize_reviews(self._texts)\n",
        "        for i, text in enumerate(self._texts):\n",
        "            assert 0 != len(text) # make sure no empty reviews again (new could be introduced by removing stop words unfortunately)\n",
        "        print(\"Building vocab (word-id mapping)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        sent_gen = (sent for text in self._texts for sent in text)\n",
        "        if min_word_freq:\n",
        "            self.vocab = build_vocab_from_iterator(sent_gen, specials=[self.unkn_tok], min_word_freq=5)\n",
        "        else:\n",
        "            words = Counter()\n",
        "            for tokens in sent_gen:\n",
        "                words.update(tokens)\n",
        "            words = [word for word, freq in words.most_common(max_word_count)] # list sorted by frequency yikees\n",
        "            self.vocab = Vocab(VocabPybind(words, None))\n",
        "        self.vocab.insert_token(self.unkn_tok, 0)\n",
        "        self.vocab.set_default_index(self.vocab[self.unkn_tok]) # set index for out-of-vocabulary words\n",
        "        print(\"Mapping words to ids..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = self.id_map_reviews(self._texts)\n",
        "        gc.collect() # force garbage collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy1qrIH1CAlx"
      },
      "source": [
        "If you want to read dataset from dataset file, set `USE_RATEBEER_PICKLE` to true in the cell below. If you left them untouched, it'll be read from serialized `RateBeerReviews` class object instead of parsing text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tO_xtf2XuCG3"
      },
      "outputs": [],
      "source": [
        "%env USE_RATEBEER_PICKLE=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_QTo-xBCelp"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "if [ \"$USE_RATEBEER_PICKLE\" = true ]\n",
        "then # download pickle\n",
        "    if [ ! -f './ratebeer-20K-vocab.pickle' ]\n",
        "    then\n",
        "        gdown --id '1VBDjyR4jpzAgzcDUGNQFguOfLC3rtOV_' # https://drive.google.com/file/d/1VBDjyR4jpzAgzcDUGNQFguOfLC3rtOV_/view?usp=sharing  # 20K words dataset\n",
        "        # gdown --id '1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j' # https://drive.google.com/file/d/1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j/view?usp=sharing # min frequency 5 words dataset\n",
        "    fi\n",
        "else # download original dataset\n",
        "    if [ ! -f './SNAP-Ratebeer.txt' ]\n",
        "    then\n",
        "        gdown --id '12tEEYQcHZtg5aWyfIiWWVIDAJNT-5d_T' # https://drive.google.com/file/d/12tEEYQcHZtg5aWyfIiWWVIDAJNT-5d_T/view?usp=sharing\n",
        "        echo \"Dataset head (trailing newline makes entry end): \"\n",
        "        export $RATEBEER_FILE='./SNAP-Ratebeer.txt'\n",
        "        head -n 16 $RATEBEER_FILE\n",
        "        iconv -f ISO-8859-1 -t UTF-8 $RATEBEER_FILE -o {RATEBEER_FILE}.new && mv {RATEBEER_FILE}.new $RATEBEER_FILE\n",
        "    fi\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5zmSYDBCg4B"
      },
      "outputs": [],
      "source": [
        "rb = RateBeerReviews()\n",
        "\n",
        "if os.environ.get('USE_RATEBEER_PICKLE') == 'true':\n",
        "    rb.load('./', 'ratebeer-20K-vocab.pickle')\n",
        "else: # build pickle\n",
        "    rb.build('./SNAP-Ratebeer.txt', max_word_count=20000)\n",
        "    print('Dumping..')\n",
        "    rb.dump('./', 'ratebeer-20K-vocab.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCLRTvJYV2So"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from torchtext._torchtext import (Vocab as VocabPybind) # make use of some hidden interface\n",
        "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import gc # garbage collector interface\n",
        "import io\n",
        "import re\n",
        "import spacy # nlp toolkit\n",
        "import torch\n",
        "import json\n",
        "\n",
        "class OcenPiwoReviews(BaseReviews):\n",
        "    def __init__(self):\n",
        "        aspects = ['ogólny', 'smak', 'zapach', 'wygląd',]\n",
        "        super().__init__(\n",
        "            aspects        = aspects,\n",
        "            aspect_max     = [10, 10, 10, 10],\n",
        "            aspect_ratings = [ [] for _ in aspects ],\n",
        "            texts          = [],\n",
        "            unkn_tok       = '<unk>', # unknown/out of vocabulary token\n",
        "            _len            = 0,\n",
        "            anchor_words = {\n",
        "                'ogólny'     : ('ogólnie'),\n",
        "                'smak'       : ('smak'),\n",
        "                'zapach'     : ('zapach'),\n",
        "                'wygląd'     : ('wygląd', 'wygląda')\n",
        "            },\n",
        "        )\n",
        "        self.pipe = None\n",
        "\n",
        "    def build(self, filepath=f'{ROOT_DIR}/ocen-piwo-utf8.json', min_word_freq=None, max_word_count=None):\n",
        "        with io.open(filepath, encoding='utf-8') as f:\n",
        "            json_dict = json.loads(f.read())\n",
        "\n",
        "            for i, reviews in enumerate(json_dict.values()):\n",
        "                for sentences, ratings in reviews:\n",
        "                    self._len += 1\n",
        "\n",
        "                    for aspect in range(self.aspect_count):\n",
        "                        self._aspect_ratings[aspect].append(int(ratings[aspect]))\n",
        "\n",
        "                    self._texts.append(sentences)\n",
        "        self._post_process(min_word_freq, max_word_count)\n",
        "\n",
        "    def _fetch_nlp_pipeline(self):\n",
        "        if not self.pipe:\n",
        "            nlp = spacy.load('pl_core_news_md')\n",
        "            nlp.add_pipe(\"sentencizer\", config={\"punct_chars\": ['.', '?', '!']})\n",
        "            nlp.Defaults.stop_words |= { '-', '+'}\n",
        "            self.pipe = lambda reviews: nlp.pipe(reviews)\n",
        "    \n",
        "    def _free_nlp_pipeline(self):\n",
        "        self.nlp = None\n",
        "\n",
        "    def tokenize_reviews(self, reviews_texts: str):\n",
        "        return [tuple(list(tok.lemma_ for tok in sent if not tok.is_stop and not tok.is_punct and not tok.is_space and len(tok) > 2) \\\n",
        "            for sent in doc.sents if 0 != len(sent)) for doc in self.pipe(reviews_texts)]\n",
        "    \n",
        "    def id_map_reviews(self, texts):\n",
        "        return [tuple(self.vocab.lookup_indices(sent) for sent in text) for text in texts]\n",
        "    \n",
        "    def _post_process(self, min_word_freq=None, max_word_count=None):\n",
        "        assert (min_word_freq is not None) ^ bool(max_word_count is not None), \"provide one of min_word_freq and max_word_count\"\n",
        "        self._fetch_nlp_pipeline()\n",
        "        print(\"Spacy pipe (tokenization&sentence split)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = self.tokenize_reviews(self._texts)\n",
        "        for i, text in enumerate(self._texts):\n",
        "            assert 0 != len(text) # make sure no empty reviews again (new could be introduced by removing stop words unfortunately)\n",
        "        print(\"Building vocab (word-id mapping)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        sent_gen = (sent for text in self._texts for sent in text)\n",
        "        if min_word_freq:\n",
        "            self.vocab = build_vocab_from_iterator(sent_gen, specials=[self.unkn_tok], min_word_freq=5)\n",
        "        else:\n",
        "            words = Counter()\n",
        "            for tokens in sent_gen:\n",
        "                words.update(tokens)\n",
        "            words = [word for word, freq in words.most_common(max_word_count)] # list sorted by frequency yikees\n",
        "            self.vocab = Vocab(VocabPybind(words, None))\n",
        "        self.vocab.insert_token(self.unkn_tok, 0)\n",
        "        self.vocab.set_default_index(self.vocab[self.unkn_tok]) # set index for out-of-vocabulary words\n",
        "        print(\"Mapping words to ids..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = [tuple(self.vocab.lookup_indices(sent) for sent in text) for text in self._texts]\n",
        "        gc.collect() # force garbage collection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to read dataset from dataset file, set `USE_OCENPIWO_PICKLE` to true in the cell bellow. If you left them untouched, it'll be read from serialized `OcenPiwoReviews` class object instead of parsing text file.\n"
      ],
      "metadata": {
        "id": "XeMaasi5rkKl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0YlFh34widy"
      },
      "outputs": [],
      "source": [
        "%env USE_OCENPIWO_PICKLE=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhGr49H6wmVp"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "if [ \"$USE_OCENPIWO_PICKLE\" = true ]\n",
        "then # download pickle\n",
        "    if [ ! -f './ocenpiwo-20K-vocab.pickle' ]\n",
        "    then\n",
        "        gdown --id '1zUXnY0UqvevNnePThTfquF6hUxxPOls9' # https://drive.google.com/file/d/1zUXnY0UqvevNnePThTfquF6hUxxPOls9/view  # 20K words dataset\n",
        "    fi\n",
        "else # download original dataset\n",
        "    if [ ! -f './ocen-piwo-utf8.json' ]\n",
        "    then\n",
        "        gdown --id '1RM_Sk8QeOQnjnje0gwxQfJIOIK0KLLWV' # https://drive.google.com/file/d/1RM_Sk8QeOQnjnje0gwxQfJIOIK0KLLWV\n",
        "    fi\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mRsWfMxrst_"
      },
      "outputs": [],
      "source": [
        "op = OcenPiwoReviews()\n",
        "\n",
        "if os.environ.get('USE_OCENPIWO_PICKLE') == 'true':\n",
        "    op.load('./', 'ocenpiwo-20K-vocab.pickle')\n",
        "else: # build pickle\n",
        "    op.build('./ocen-piwo-utf8.json', max_word_count=20000)\n",
        "    print('Dumping..')\n",
        "    op.dump('./', 'ocenpiwo-20K-vocab.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOc_2b-E4o0c"
      },
      "source": [
        "## Aspect assignement training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithm\n",
        "\n",
        "This part is strictly based on the PALELAGER paper.\n",
        "\n",
        "Two types of weights are introduced: \n",
        "- $\\Theta_{kw}$ indexed by $w$ - words and $k$ - aspects.\n",
        "- $\\phi_{kv_{k}w}$ indexed by $w$ - words, $k$ - aspects and $v_{k}$ - aspect rating.\n",
        "\n",
        "Assume that all ratings are known for a set of reviews and a single review's sentence $s$ is being considered. Then one can come up with a likelihood score $L$ that the sentence describes some particular aspect, namely $k$.\n",
        "\n",
        "$$\n",
        "    L^{(\\Theta, \\phi)}(\\text{aspect}(s) = k \\ | \\ \\text{sentence} \\ s,     \\text{rating} \\ v) =\n",
        "    \\begin{equation}\n",
        "        \\sum_{w \\in s}^{} \\Bigg\\{ \\Theta_{kw} + \\phi_{kv_{k}w} \\Bigg\\}\n",
        "    \\end{equation}\n",
        "$$\n",
        "\n",
        "We can softmax the likelihoods for all target aspects\n",
        "\n",
        "$$\n",
        "    P^{(\\Theta, \\phi)}(\\text{aspect}(s) = k \\ | \\ \\text{sentence} \\ s,     \\text{rating} \\ v) = \\frac{1}{Z_s^{(\\Theta, \\phi)}} \\exp{}\n",
        "    \\begin{equation}\n",
        "        \\sum_{w \\in s}^{} \\Bigg\\{ \\Theta_{kw} + \\phi_{kv_{k}w} \\Bigg\\}\n",
        "    \\end{equation}\n",
        "$$\n",
        "where $Z$ is softmax normalization factor is:\n",
        "$$\n",
        "    Z_s^{(\\Theta, \\phi)} = \n",
        "    \\begin{equation}\n",
        "        \\sum_{k = 1}^{K} \\exp \\sum_{w \\in s}^{} \\Bigg\\{ \\Theta_{kw} + \\phi_{kv_{k}w} \\Bigg\\}\n",
        "    \\end{equation}\n",
        "$$\n",
        "\n",
        "Example:\n",
        "\n",
        "\"I like the smell of pineapple and ham in the morning. Very cool looking box. I love this ***pizza***.\" \n",
        "~ Decartes\n",
        "\n",
        "Aspects ratings:\n",
        "Aroma: 5/5\n",
        "Box: 5/5\n",
        "Overall: 5/5\n",
        "\n",
        "It is trivially intuitive, that the 1st sentence discusses aroma aspect, the 2nd one discussed box raiting and the last sentence discusses overall rating. For each word we assign a score that is a sum of appropriate weights.\n",
        "\n",
        "\n",
        "| aspect/word |  I   |                                              love                                              | this | pizza |\n",
        "|:-----------:|:----:|:----------------------------------------------------------------------------------------------:|:----:|:-----:|\n",
        "|     Box     | 0.3  |                                               0                                                | 0.3  |  0.2  |\n",
        "|    Aroma    | 0.1  | $$\\theta_{k=\\text{aroma}, \\ w=\\text{love}} + \\Phi_{k=\\text{aroma}, \\ v_k=5/5, \\ w=\\text{love}}$$ | 0.2  |   2   |\n",
        "|   Overall   | -0.2 |                                               10                                               | -0.3 |   3   |\n",
        "\n",
        "In the middle cell we wrote down the formula for calculating the score.  \n",
        "Based on the scores a probability of sentence discussing aspect is calculated.\n",
        "\n",
        "\n",
        "We initialize Thetas that explicitly mention aspect names to ones:\n",
        "```\n",
        "anchor_words = {\n",
        "                'appearance' : ('appearance', 'color'),\n",
        "                'aroma'      : ('aroma'),\n",
        "                'palate'     : ('palate', 'mouthfeel'),\n",
        "                'taste'      : ('taste'),\n",
        "                'overall'    : ('overall'),\n",
        "            },\n",
        "```\n",
        "That's why at first training will proceed with promoting all words that occur in senctences with the words mentioned before. Later training can proceed by simply propagating this information by further lowering negative log-likelihood.\n",
        "\n",
        "We minimise the objective proceeding with stochastic gradient descent implemented in PyTorch framework."
      ],
      "metadata": {
        "id": "FBwgjO0B0feL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "PH3Mq4nN7tTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05JPg4ATQTOW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "import datetime\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "# for wordcloudsdest_path=dest_path\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from PIL import Image\n",
        "from os import path\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import functools\n",
        "from operator import iadd\n",
        "from typing import List\n",
        "import unicodedata\n",
        "from math import ceil\n",
        "\n",
        "class AspectModel():\n",
        "    def __init__(self, dataset):\n",
        "        self.ds = dataset\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        word_count = len(self.ds.vocab.get_itos())\n",
        "        self.theta = torch.rand((word_count, self.ds.aspect_count), device=dev)\n",
        "        # scale to [-0.1, 0.0], as we enforce this weight to 1.0 for some words later on\n",
        "        self.theta = self.theta * -0.1\n",
        "        # enforce 1 initialization on aspect name (page 4)\n",
        "        for aspect_idx, aspect in enumerate(self.ds.aspects):\n",
        "            words = self.ds.anchor_words[aspect]\n",
        "\n",
        "            if isinstance(words, str):\n",
        "                words = [words]\n",
        "            else:\n",
        "                words = list(words)\n",
        "\n",
        "            words_ids = self.ds.vocab.lookup_indices(words)\n",
        "            for word_id in words_ids:\n",
        "                self.theta[word_id, aspect_idx] = 1.0\n",
        "        self.theta.requires_grad_()\n",
        "\n",
        "        # introduce separate phi for each aspect\n",
        "        # self.phis = [torch.rand((word_count, self.ds.aspect_max[i])).to(dev) for i in range(self.ds.aspect_count)]\n",
        "        self.phis = [torch.zeros((word_count, self.ds.aspect_max[i]), device=dev, dtype=self.theta.dtype) for i in range(self.ds.aspect_count)]\n",
        "        # normalize that sum across all words is 1 for a given aspect (eq. 7) # do not normalize as for now # and forever and ever\n",
        "        # self.phis = [phi / phi.sum(dim=0) for phi in self.phis]\n",
        "        for phi in self.phis: phi.requires_grad_()\n",
        "    \n",
        "    def word_clouds(self, dest_path=MODEL_ROOT_DIR, filename='words.png', show=True):\n",
        "        # words = list(str(unicodedata.normalize('NFD', x).encode('ascii', 'ignore'))[2:-1] for x in self.ds.vocab.get_itos())\n",
        "        words = self.ds.vocab.get_itos()\n",
        "        fig = plt.figure(figsize=(max(self.ds.aspect_max) + 1, len(self.ds.aspects)))\n",
        "        plt.subplots_adjust(wspace=0.2)\n",
        "        i = 0\n",
        "\n",
        "        for aspect in range(self.ds.aspect_count):\n",
        "            aspect_name = self.ds.aspects[aspect]\n",
        "            atheta = self.theta[:, aspect].tolist()\n",
        "            \n",
        "            zipped = list(zip(words, atheta))\n",
        "\n",
        "            wc = WordCloud(background_color=\"white\", color_func=lambda *args, **kwargs: 'black', width=1000, height=1000)\n",
        "            wc.generate_from_frequencies(dict(zipped))\n",
        "\n",
        "            fig.add_subplot(self.ds.aspect_count, max(self.ds.aspect_max) + 1, i * (max(self.ds.aspect_max) + 1) + 1)\n",
        "            plt.imshow(wc)\n",
        "            plt.title(f'{aspect_name} Theta', fontsize=2)\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            for rating in range(self.ds.aspect_max[aspect]):\n",
        "                aphi = self.phis[aspect][:, rating].tolist()\n",
        "\n",
        "                zipped = list(zip(words, aphi))\n",
        "                sorted_zip = sorted(zipped, reverse=True, key=lambda x: x[1])[:200]\n",
        "\n",
        "                if any(a == 0 for _, a in sorted_zip):\n",
        "                    print(f'Omitting {aspect_name} Phi {str(rating + 1)}')\n",
        "                    continue\n",
        "\n",
        "                wc = WordCloud(background_color=\"white\", color_func=lambda *args, **kwargs: 'black', width=1000, height=1000)\n",
        "                wc.generate_from_frequencies(dict(zipped))\n",
        "                fig.add_subplot(self.ds.aspect_count, max(self.ds.aspect_max) + 1, i * (max(self.ds.aspect_max) + 1) + rating + 2)\n",
        "\n",
        "                plt.imshow(wc)\n",
        "                plt.title(f'{aspect_name} Phi {str(rating + 1)}', fontsize=2)\n",
        "                plt.axis(\"off\")\n",
        "\n",
        "            i += 1\n",
        "        plt.savefig(f'{dest_path}/{filename}', dpi=600)\n",
        "        plt.show(block=show)\n",
        "    \n",
        "    def rev_words_thetas(self, rev_sens_ids):\n",
        "        \"\"\"\n",
        "        Fetch Thetas for a review\n",
        "        \"\"\"\n",
        "        return [self.theta[sen_ids] for sen_ids in rev_sens_ids]\n",
        "\n",
        "    def rev_words_phis(self, rev_sens_ids):\n",
        "        \"\"\"\n",
        "        Fetch Phis for a review\n",
        "        \"\"\"\n",
        "        return [[self.phis[aspect_idx][sen_ids, :] for aspect_idx in range(self.ds.aspect_count)] for sen_ids in rev_sens_ids]\n",
        "    \n",
        "    def dump_weights(self, dest_path=MODEL_ROOT_DIR, filename=''):\n",
        "        weights = {'phis': self.phis, 'theta': self.theta}\n",
        "        torch.save(weights,  f'{dest_path}/{filename}')\n",
        "\n",
        "    def load_weights(self, src_path):\n",
        "        weights = torch.load(src_path, map_location=torch.device(dev))\n",
        "        self.theta = weights['theta']\n",
        "        self.phis  = weights['phis']\n",
        "    \n",
        "    def _linear_assignement(self, costs):\n",
        "        # for nll we want to minimize\n",
        "        return linear_sum_assignment(costs, maximize=False)\n",
        "    \n",
        "    def plot_nll_history(self, dir, filename):\n",
        "        plt.figure().set_facecolor('white') # no alpha please\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('NLL')\n",
        "        plt.xlim(left=0.0, right=self.train_epoch_history[-1])\n",
        "        plt.ylim(bottom=min(0.0, min(self.train_nll_history)), top=max(max(self.train_nll_history), max(self.valid_nll_history)))\n",
        "        plt.plot(self.train_epoch_history, self.train_nll_history, '-o')\n",
        "        plt.plot(self.valid_epoch_history, self.valid_nll_history, '-o')\n",
        "        plt.legend(['train batch NLL', 'valid mean batch NLL'], loc='upper right')\n",
        "        plt.title('Train and valid dataset NLL')\n",
        "        plt.savefig(os.path.join(dir, filename))\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def experiments(dataset, root_dir=MODEL_ROOT_DIR, max_batches=None, save_freq=5000, train_split=0.8):\n",
        "        assert max_batches # we stop training at max_batches batches done, because epochs differ much between datasets\n",
        "\n",
        "        for _ in range(1):\n",
        "            directory = str(datetime.datetime.now()) + f'-{type(dataset)}'\n",
        "            path = os.path.join(root_dir, directory)\n",
        "            os.mkdir(path)\n",
        "            \n",
        "            model = AspectModel(dataset)\n",
        "            model.train(dest_path=path, max_batches=max_batches, save_freq=save_freq, train_split=train_split)\n",
        "            \n",
        "        for _ in range(1):\n",
        "            directory = str(datetime.datetime.now()) + '-cyclic' + f'-{type(dataset)}'\n",
        "            path = os.path.join(root_dir, directory)\n",
        "            os.mkdir(path)\n",
        "\n",
        "            model = AspectModel(dataset)\n",
        "            model.train(dest_path=path, sched='cyclic', max_batches=max_batches, save_freq=save_freq, train_split=train_split)\n",
        "            \n",
        "        for _ in range(1):\n",
        "            directory = str(datetime.datetime.now()) + '-adam' + f'-{type(dataset)}'\n",
        "            path = os.path.join(root_dir, directory)\n",
        "            os.mkdir(path)\n",
        "\n",
        "            model = AspectModel(dataset)\n",
        "            model.train(dest_path=path, optim='adam', max_batches=max_batches, save_freq=save_freq, train_split=train_split)\n",
        "\n",
        "    def valid_nll(self, debug=False):\n",
        "        nlls = []\n",
        "        with torch.no_grad():\n",
        "            valid_loader = DataLoader(self.valid_ds, batch_size=100, collate_fn=lambda x: x) # do not use default collate function as it requires fixed-length input and raises this exception otherwise https://github.com/pytorch/pytorch/issues/42654\n",
        "            for i, batch in enumerate(tqdm(valid_loader, leave=None, desc='validation')):\n",
        "                batch_nlls = []\n",
        "                for (rev_sents_ids, review_aspects_scores) in batch:\n",
        "                    rev_thetas = self.rev_words_thetas(rev_sents_ids)\n",
        "                    rev_phis   = self.rev_words_phis(rev_sents_ids)\n",
        "                    aspect_sents_scores = torch.stack(\n",
        "                        [\n",
        "                        rev_thetas[j].sum(dim=0) + torch.stack(tuple(rev_phis[j][a][:, review_aspects_scores[a] - 1].sum() for a in range(self.ds.aspect_count))) # 1 x aspect count\n",
        "                        for j in range(len(rev_sents_ids))\n",
        "                        ],\n",
        "                    ) # sent count x aspect count\n",
        "                    softmax = -torch.nn.functional.log_softmax(aspect_sents_scores, dim=1)\n",
        "\n",
        "                    sents_aspect_preds_max = torch.argmin(softmax, dim=1)\n",
        "                    row_ind, col_ind = self._linear_assignement(costs=softmax.detach().cpu().numpy())\n",
        "                    sents_aspect_preds_linear = sents_aspect_preds_max\n",
        "\n",
        "                    sents_aspect_preds_linear[row_ind] = torch.from_numpy(col_ind).to(dev)\n",
        "                    \n",
        "                    batch_nlls.append(softmax.take_along_dim(sents_aspect_preds_linear[:, None], dim=1).sum().cpu().item())\n",
        "                nlls.append(functools.reduce(lambda a, b: a + b, batch_nlls))\n",
        "        nll_sum = functools.reduce(lambda a, b: a + b, nlls)\n",
        "        if debug: ic(nll_sum)\n",
        "        if debug: ic(torch.exp(-torch.tensor(nll_sum)))\n",
        "        mean_batch_nll = nll_sum / len(nlls)\n",
        "        return mean_batch_nll\n",
        "\n",
        "    # doesnt really work without ratings\n",
        "    def show_inference(self, reviews: List[str], ratings: List[List[int]]):\n",
        "        self.ds._fetch_nlp_pipeline()\n",
        "        tokenized_reviews = self.ds.tokenize_reviews(reviews)\n",
        "        \n",
        "        ided_reviews = self.ds.id_map_reviews(tokenized_reviews)\n",
        "\n",
        "        revs_sents_aspect_preds = []\n",
        "        revs_sents_rating_preds = []\n",
        "        # aspect inference\n",
        "        for rev_sents_ids, review_aspects_scores in zip(ided_reviews, ratings):\n",
        "            rev_thetas = self.rev_words_thetas(rev_sents_ids)\n",
        "            rev_phis   = self.rev_words_phis(rev_sents_ids)\n",
        "            aspect_sents_scores = torch.stack(\n",
        "                [\n",
        "                rev_thetas[j].sum(dim=0) + torch.stack(tuple(rev_phis[j][a][:, review_aspects_scores[a] - 1].sum() for a in range(self.ds.aspect_count))) # 1 x aspect count\n",
        "                for j in range(len(rev_sents_ids))\n",
        "                ],\n",
        "            ) # sent count x aspect count\n",
        "\n",
        "            # we have to introduce the sign because of *negative* likelihood\n",
        "            softmax = -torch.nn.functional.log_softmax(aspect_sents_scores, dim=1)\n",
        "\n",
        "            sents_aspect_preds_max = torch.argmin(softmax, dim=1) # min of nll === max likelihood\n",
        "            row_ind, col_ind = self._linear_assignement(costs=softmax.detach().cpu().numpy())\n",
        "            sents_aspect_preds_linear = sents_aspect_preds_max\n",
        "\n",
        "            sents_aspect_preds_linear[row_ind] = torch.from_numpy(col_ind).to(dev)\n",
        "            revs_sents_aspect_preds.append(sents_aspect_preds_linear)\n",
        "\n",
        "        for i in range(len(reviews)):\n",
        "            print(f'Review {i}: ')\n",
        "            print(f'Tokenized review text (stop words removed):')\n",
        "            print(*(tokenized_reviews[i]))\n",
        "            print(f'Review text mapped to word ID-s:', *(ided_reviews[i]))\n",
        "            print(f'Predicted sentences aspects: ', *[self.ds.aspects[target_idx] for target_idx in (revs_sents_aspect_preds[i])])\n",
        "            print()\n",
        "\n",
        "    def train(self, dest_path=MODEL_ROOT_DIR, max_batches=None, optim='sgd', sched=None, lr=None, save_freq=5000, train_split=0.8):\n",
        "        train_size = int(train_split * len(self.ds))\n",
        "        valid_size = len(self.ds) - train_size\n",
        "\n",
        "        params = (\n",
        "            self.theta,\n",
        "            *self.phis\n",
        "        )\n",
        "        \n",
        "        batch_size = 100\n",
        "        if lr is None:\n",
        "            lr = sum(self.ds.aspect_max) * 1e-2 / batch_size\n",
        "\n",
        "        weight_decay = 0.0001\n",
        "        momentum = 0.1\n",
        "        if optim == 'sgd':\n",
        "            self._optim = torch.optim.SGD(\n",
        "                params=params,\n",
        "                lr=lr,\n",
        "                weight_decay=weight_decay,\n",
        "                momentum=momentum\n",
        "            )\n",
        "        elif optim == 'adam':\n",
        "            self._optim = torch.optim.Adam(\n",
        "                params=params,\n",
        "                lr=lr,\n",
        "                weight_decay=weight_decay,\n",
        "                betas=(momentum, 0.999) # the first is params momentum, second RMSProp momentum (for now set to 0.999 which is default Pytorch value)\n",
        "            )\n",
        "        else:\n",
        "            assert False\n",
        "        if sched == 'cyclic':\n",
        "            self._sched = torch.optim.lr_scheduler.CyclicLR(\n",
        "                self._optim,\n",
        "                base_lr=lr*0.5,\n",
        "                max_lr=2*lr\n",
        "            )\n",
        "        else:\n",
        "            self._sched = torch.optim.lr_scheduler.StepLR(self._optim, step_size=2000, gamma=0.999)\n",
        "\n",
        "        self.train_ds, self.valid_ds = random_split(self.ds, [train_size, valid_size])\n",
        "        train_loader = DataLoader(self.train_ds, batch_size=100, shuffle=True, collate_fn=lambda x: x) # do not use default collate function as it requires fixed-length input and raises this exception otherwise https://github.com/pytorch/pytorch/issues/4265\n",
        "        \n",
        "        assert max_batches\n",
        "        \n",
        "        self.train_epoch_history = []\n",
        "        self.valid_epoch_history = []\n",
        "        self.train_nll_history = []\n",
        "        self.valid_nll_history = []\n",
        "        batches_done = 0\n",
        "        batch_count = len(train_loader) # number of batches in a single epoch sorry for bad naming\n",
        "        epoch_count = int(ceil(max_batches /batch_count)) # max number of epochs\n",
        "        try:\n",
        "            for epoch in range(epoch_count):\n",
        "                for i, batch in enumerate(tqdm(train_loader, desc=f'train epoch {epoch}/{epoch_count}', leave=None)):\n",
        "                    batch_nlls = []\n",
        "                    for (rev_sents_ids, review_aspects_scores) in batch:\n",
        "                        # for sent_ids in rev_sents_ids: sent_ids.to(dev)\n",
        "                        rev_thetas = self.rev_words_thetas(rev_sents_ids)\n",
        "                        rev_phis   = self.rev_words_phis(rev_sents_ids)\n",
        "\n",
        "                        aspect_sents_scores = torch.stack(\n",
        "                            [\n",
        "                            rev_thetas[j].sum(dim=0) + torch.stack(tuple(rev_phis[j][a][:, review_aspects_scores[a] - 1].sum() for a in range(self.ds.aspect_count))) # 1 x aspect count\n",
        "                            for j in range(len(rev_sents_ids))\n",
        "                            ],\n",
        "                        ) # sent count x aspect count\n",
        "                        softmax = -torch.nn.functional.log_softmax(aspect_sents_scores, dim=1)\n",
        "\n",
        "                        sents_aspect_preds_max = torch.argmin(softmax, dim=1)\n",
        "                        row_ind, col_ind = self._linear_assignement(costs=softmax.detach().cpu().numpy())\n",
        "                        sents_aspect_preds_linear = sents_aspect_preds_max\n",
        "\n",
        "                        # (most likely) aspect assignments (5)\n",
        "                        sents_aspect_preds_linear[row_ind] = torch.from_numpy(col_ind).to(dev)\n",
        "                        \n",
        "                        # sentence likelihood (6)\n",
        "                        batch_nlls.append(softmax.take_along_dim(sents_aspect_preds_linear[:, None], dim=1).sum())\n",
        "\n",
        "                    batch_nll = torch.stack(batch_nlls).sum()\n",
        "                    self._optim.zero_grad(set_to_none=True)\n",
        "                    if 0 == i % 100:\n",
        "                        self.train_nll_history.append(batch_nll.cpu().detach().item())\n",
        "                        self.train_epoch_history.append(epoch + i / batch_count)\n",
        "                    if 0 == i % save_freq:\n",
        "                        if i != 0:\n",
        "                            self.dump_weights(dest_path=dest_path, filename=(f'-epoch-{epoch}-{epoch_count}-{int(i)}'))\n",
        "                            # self.word_clouds(dest_path=dest_path, filename=(f'cloud-epoch-{epoch}-{epoch_count}-{int(i)}.png'))\n",
        "                            self.plot_nll_history(dest_path, (f'plot-epoch-{epoch}-{epoch_count}-{int(i)}.png'))\n",
        "                        self.valid_nll_history.append(self.valid_nll())\n",
        "                        self.valid_epoch_history.append(epoch + i / batch_count)\n",
        "                    batch_nll.backward()\n",
        "\n",
        "                    self._optim.step()\n",
        "                    self._sched.step()\n",
        "                    batches_done += 1\n",
        "                    if max_batches <= batches_done:\n",
        "                        break\n",
        "                if max_batches <= batches_done:\n",
        "                    break\n",
        "\n",
        "            self.dump_weights(dest_path=dest_path, filename=(f'weights-end'))\n",
        "            # self.word_clouds(dest_path=dest_path, filename=(f'cloud-end.png'))\n",
        "            self.valid_nll_history.append(self.valid_nll())\n",
        "            self.valid_epoch_history.append(epoch + i / batch_count)\n",
        "            self.plot_nll_history(dest_path, (f'plot-end.png'))\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print('Interrupted.')\n",
        "\n",
        "            self.dump_weights(dest_path=dest_path, filename=(f'weights-interrupt'))\n",
        "            # self.word_clouds(dest_path=dest_path, filename=(f'cloud-interrupt.png'))\n",
        "            self.plot_nll_history(dest_path, (f'plot-interrupt.png'))\n",
        "\n",
        "        except Exception as e:\n",
        "            # ic(len(batch_nlls))\n",
        "            # ic(rev_sents_ids)\n",
        "\n",
        "            self.dump_weights(dest_path=dest_path, filename=(f'weights-exception'))\n",
        "            # self.word_clouds(dest_path=dest_path, filename=(f'cloud-exception.png'))\n",
        "            self.plot_nll_history(dest_path, (f'plot-exception.png'))\n",
        "\n",
        "            ic(batch_nll)\n",
        "            ic((len(rev_sents_ids),))\n",
        "            raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyP8T2ASG4ZW"
      },
      "outputs": [],
      "source": [
        "# %%capture output\n",
        "# %%script python --no-raise-error\n",
        "# model = AspectModel(op)\n",
        "# model.train()\n",
        "AspectModel.experiments(op, root_dir='./saves/', max_batches=10000, save_freq=412)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tAIxkej0cNV"
      },
      "outputs": [],
      "source": [
        "# %%capture output\n",
        "# %%script python --no-raise-error\n",
        "# model = AspectModel(rb)\n",
        "# model.train()\n",
        "AspectModel.experiments(rb, root_dir='./saves/', max_batches=10000, save_freq=1000, train_split=0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYr-3QAhLRga"
      },
      "outputs": [],
      "source": [
        "# %store output > output_log\n",
        "import pickle\n",
        "with open('output', 'wb') as f:\n",
        "    pickle.dump(output, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddNbPIhSXUYv"
      },
      "outputs": [],
      "source": [
        "# danger, for some reason this line causes notebook to crash\n",
        "# model.ds.vocab.lookup_indices([0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rb_model = AspectModel(rb)\n",
        "rb_model.load_weights('/content/saves/2022-02-07 18:16:04.214631-<class \\'__main__.RateBeerReviews\\'>/weights-end')"
      ],
      "metadata": {
        "id": "oD3BdI7wIC6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PALELAGER based rating prediction"
      ],
      "metadata": {
        "id": "fZ1opvIN7yf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithm\n",
        "\n",
        "Rating prediction is also described in the PALELAGER paper. The publication provides a formula the following formula for calculating the probability that an aspect of a review was marked with some specific numerical rating value. It assumes that the overall rating is known and we want to infer the remaining ratings.\n",
        "\n",
        "Another two types of weight are introduced:\n",
        "- $\\gamma_{kv_{k}w}$ indexed by $w$ - words and $k$ - aspects $v_{k}$ - aspect rating. Meaning is the same as before: how much does a word influence probability of a rating for a given aspect.\n",
        "- $\\alpha_{ijv_{i}v_{j}}$ indexed by $i$ - first aspect $j$ - second aspect, $v_i$ - rating of the first aspect and $v_j$ - rating of the second aspect. This should measure co-occurence of these two ratings for a given pair of aspects.\n",
        "\n",
        "Using the weights we can find correlation between ratings of any two aspect, sum over it and find the rating that maximises this sum.\n",
        "\n",
        "$$\n",
        "v_{i}^{(\\gamma, \\alpha)} = \\text{argmax}\n",
        "\\begin{equation}\n",
        "    \\sum_{k}^{} \\sum_{s \\in r_{i}}^{} \\delta(t_{ti = k}) \\sum_{w \\in s}^{} \\gamma_{kv_{k}w} + \\sum_{i \\neq j}^{} \\alpha_{ijv_{i}v_{j}}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Then they propose to minimize the following loss for whole dataset:\n",
        "\n",
        "$$\n",
        "    (\\hat{\\gamma}, \\hat{\\alpha}) = \\text{argmin} \\sum_{i=1}^{R'} \\sum_{k \\neq \\text{overall}}^{} || v_{i}^{(\\gamma, \\alpha)} - v_{ik}||_2 + \\Omega(\\gamma, \\alpha)\n",
        "$$\n",
        "\n",
        "Though it sounds interesting, we do not see any way to optimize this objective using gradient descent. That's the most probable reason why they are computing these gamma and alpha parameters using modified SVM approach given in [\"Large margin methods for structured and interdependent output variables\"](http://luthuli.cs.uiuc.edu/~daf/courses/learning/StructureLearning/tsochantaridis05a.pdf) (referred by them 31). We did not have time budget for explore both the approach and optimization algorithm it uses.\n",
        "\n",
        "Nevertheless we decided to adapt it.\n",
        "\n",
        "We initialized gammas with phis computed at the previous aspect assignment step. Then we calculate the probability of every two ratings occuring at the same time in our data set. The results were quite intuitive and interpretable.\n",
        "\n",
        "For example we found that if a beer has overall rating 20/20 then it is higly probable that it also has 5/5 aroma rating, 5/5 appearance rating and so on. It's really tempting to exploit this known distribution and then use it to predict new ratings with some noise. It was effective and unsatysfying at the same time as the predicted result were pretty similar to each other.\n",
        "\n"
      ],
      "metadata": {
        "id": "kr8ZGOlV8IfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "qEWG8eIQ8XDb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IUhdJwf6bsj"
      },
      "outputs": [],
      "source": [
        "from torch.cuda import device\n",
        "from traitlets.traitlets import Float\n",
        "from itertools import combinations, product, permutations\n",
        "from functools import reduce\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "class RatingModel():\n",
        "    def __init__(self, dataset, aspect_model):\n",
        "        self.ds = dataset\n",
        "        self.model = aspect_model\n",
        "        self.init_weights(aspect_model)\n",
        "\n",
        "    def init_weights(self, aspect_model):\n",
        "        word_count = len(self.ds.vocab.get_itos())\n",
        "\n",
        "        if not aspect_model:\n",
        "            self.gammas = [torch.zeros((word_count, self.ds.aspect_max[i]), requires_grad=False, device=dev) for i in range(self.ds.aspect_count)]\n",
        "        else:\n",
        "            self.gammas = [phi.clone().detach().requires_grad_(False).to(device=dev) for phi in aspect_model.phis]\n",
        "\n",
        "        # self.alphas = {comb : torch.zeros((self.ds.aspect_max[comb[0]], self.ds.aspect_max[comb[1]]), \n",
        "        #                                  requires_grad=True, device=dev, dtype=self.gammas[0].dtype) for comb in combinations(range(self.ds.aspect_count), 2)}\n",
        "        self.alphas = {}\n",
        "\n",
        "        # TODO: we only need correlations with overall\n",
        "        for comb in combinations(range(self.ds.aspect_count), 2):\n",
        "            pairs = Counter(zip(rb._aspect_ratings[comb[0]], rb._aspect_ratings[comb[1]]))\n",
        "\n",
        "            l = []\n",
        "            for i in range(1, rb.aspect_max[comb[0]] + 1):\n",
        "                l.append([val for _, val in sorted([(k[1], value) for k, value in pairs.items() if k[0] == i])])\n",
        "\n",
        "            t = torch.tensor(l, dtype=torch.float)\n",
        "            normalized = torch.nn.functional.normalize(t, dim=1)\n",
        "\n",
        "            self.alphas[comb] = normalized\n",
        "            \n",
        "        # we keep references to alphas for reversed keys\n",
        "        # optimization stonks\n",
        "        for comb in list(self.alphas.keys()):\n",
        "            self.alphas[(comb[1], comb[0])] = self.alphas[comb]\n",
        "\n",
        "        self.rating_tuples = torch.stack([torch.tensor(prod, dtype=torch.int) for prod in product(*[range(aspect_max) for aspect_max in self.ds.aspect_max])])\n",
        "\n",
        "    def rev_words_gammas(self, rev_sens_ids):\n",
        "        return [[self.gammas[aspect_idx][sen_ids, :] for aspect_idx in range(self.ds.aspect_count)] for sen_ids in rev_sens_ids]\n",
        "\n",
        "    def valid_nll(self, debug=False):\n",
        "        train_size = int(0.99955 * len(self.ds))\n",
        "        valid_size = len(self.ds) - train_size\n",
        "\n",
        "        self.train_ds, self.valid_ds = random_split(self.ds, [train_size, valid_size])\n",
        "\n",
        "        mean_l2 = []\n",
        "        guesses = correctly_guessed = 0\n",
        "        with torch.no_grad():\n",
        "            valid_loader = DataLoader(self.valid_ds, batch_size=100, collate_fn=lambda x: x) # do not use default collate function as it requires fixed-length input and raises this exception otherwise https://github.com/pytorch/pytorch/issues/42654\n",
        "            for i, batch in enumerate(tqdm(valid_loader, desc='validation')):\n",
        "                for (rev_sents_ids, review_aspects_scores) in batch:\n",
        "                    \n",
        "                    # we assume that we know overall rating\n",
        "                    rev_gammas = self.rev_words_gammas(rev_sents_ids)\n",
        "\n",
        "                    ratings_scores = [\n",
        "                        torch.stack(tuple(rev_gammas[j][a][:, :].sum(dim=0)\n",
        "                            for j in range(len(rev_sents_ids)))) # number_of_sentences x aspect_ratings\n",
        "                            .sum(dim=0)\n",
        "                            for a in range(self.ds.aspect_count)\n",
        "                        ] # aspect x aspect_ratings\n",
        "\n",
        "                    fixed_overall_ratings = list(filter(lambda v: v[-1] == (review_aspects_scores[-1] - 1), self.rating_tuples))\n",
        "\n",
        "                    max_alphas = sorted([(v, torch.stack([self.alphas[pair][v[pair[0]], v[pair[1]]] for pair in [(0, 4), (1, 4), (2, 4), (3, 4)]]).sum()) for v in fixed_overall_ratings], reverse=True, key=lambda x: x[1])[:20]\n",
        "                \n",
        "                    scores = map(\n",
        "                        lambda v_score:\n",
        "                            (v_score[0], v_score[1] + torch.stack([rating_score[v_score[0][a]] for a, rating_score in enumerate(ratings_scores)]).sum()),\n",
        "                        max_alphas\n",
        "                    )\n",
        "\n",
        "                    v, score = max(scores, key=lambda x: x[1])\n",
        "                    \n",
        "                    correctly_guessed += (torch.tensor(v + 1, dtype=torch.float) == torch.tensor(review_aspects_scores, dtype=torch.float)).sum() - 1\n",
        "                    guesses += 4\n",
        "\n",
        "                    mean_l2.append(torch.nn.functional.mse_loss(torch.tensor(v + 1, dtype=torch.float), \n",
        "                                                                torch.tensor(review_aspects_scores, dtype=torch.float)) * 5 / 4)\n",
        "                    \n",
        "        print(\"Correcly guessed: \", correctly_guessed.item(), \" out of: \", guesses, \" Percentage: \", (correctly_guessed / guesses).item())\n",
        "        return torch.stack(mean_l2).mean()\n",
        "\n",
        "    def show_inference(self, reviews: List[str], overall_ratings: List[int]):\n",
        "        self.ds._fetch_nlp_pipeline()\n",
        "        tokenized_reviews = self.ds.tokenize_reviews(reviews)\n",
        "        \n",
        "        ided_reviews = self.ds.id_map_reviews(tokenized_reviews)\n",
        "\n",
        "        revs_sents_rating_preds = []\n",
        "        for overall_rating, rev_sents_ids in zip(overall_ratings, ided_reviews):\n",
        "            with torch.no_grad():\n",
        "                rev_gammas = self.rev_words_gammas(rev_sents_ids)\n",
        "\n",
        "                ratings_scores = [\n",
        "                    torch.stack(tuple(rev_gammas[j][a][:, :].sum(dim=0)\n",
        "                        for j in range(len(rev_sents_ids)))) # number_of_sentences x aspect_ratings\n",
        "                        .sum(dim=0)\n",
        "                        for a in range(self.ds.aspect_count)\n",
        "                ] # aspect x aspect_ratings\n",
        "\n",
        "                fixed_overall_ratings = list(filter(lambda v: v[-1] == (overall_rating - 1), self.rating_tuples))\n",
        "\n",
        "                max_alphas = sorted([(v, 0.5 * torch.stack([self.alphas[pair][v[pair[0]], v[pair[1]]] for pair in [(0, 4), (1, 4), (2, 4), (3, 4)]]).sum()) for v in fixed_overall_ratings], reverse=True, key=lambda x: x[1])[:20]\n",
        "                \n",
        "                scores = map(\n",
        "                    lambda v_score:\n",
        "                        (v_score[0], v_score[1] + torch.stack([rating_score[v_score[0][a]] for a, rating_score in enumerate(ratings_scores)]).sum()),\n",
        "                    max_alphas\n",
        "                )\n",
        "\n",
        "                v, score = max(scores, key=lambda x: x[1])\n",
        "                revs_sents_rating_preds.append(v + 1)\n",
        "\n",
        "        for i in range(len(reviews)):\n",
        "            print(f'Review {i}: ')\n",
        "            print(f'Tokenized review text (stop words removed):')\n",
        "            print(*(tokenized_reviews[i]))\n",
        "            print(f'Review text mapped to word ID-s:', *(ided_reviews[i]))\n",
        "            print(f'Predicted aspects ratings: ', *[f'{aspect}: {(revs_sents_rating_preds[i][aspect_idx])}' for aspect_idx, aspect in enumerate(self.ds.aspects)])\n",
        "            print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQBbWzo3ZqkM"
      },
      "outputs": [],
      "source": [
        "rating_rb_model = RatingModel(rb, rb_model)\n",
        "# rating_rb_model.valid_nll()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "pgruDQtfJNJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training artifacts\n",
        "It came out the dataset converges really fast"
      ],
      "metadata": {
        "id": "n6zM4glmMPdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word clouds\n",
        "\n",
        "Ratebeer dataset:\n",
        "\n",
        "\n",
        "Ocenpiwo dataset:\n",
        "\n"
      ],
      "metadata": {
        "id": "i8rV_QWJKXHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "ZrJHov6VLzPL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M-dJ1YWuCG8",
        "outputId": "4c385754-9b47-47e8-c6bd-6b8b22c52f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 0: \n",
            "Tokenized review text (stop words removed):\n",
            "['good', 'looking', 'bootle'] ['aroma', 'pretty', 'astonishig'] ['sour', 'sweet', 'palate', 'profile'] ['like', 'taste'] ['overall', 'recommend']\n",
            "Review text mapped to word ID-s: [14, 300, 17235] [2, 68, 0] [75, 3, 50, 318] [21, 13] [86, 1052]\n",
            "Predicted sentences aspects:  appearance aroma palate taste overall\n",
            "\n",
            "Review 1: \n",
            "Tokenized review text (stop words removed):\n",
            "['tastes', 'best', 'bottle'] ['heap', 'think'] ['nice', 'hoppy', 'smell'] ['supposed', 'sour'] ['beautiful', 'smooth', 'head']\n",
            "Review text mapped to word ID-s: [117, 149, 8] [7873, 175] [11, 49, 143] [1326, 75] [308, 51, 1]\n",
            "Predicted sentences aspects:  palate overall aroma taste appearance\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rb_model.show_inference([\n",
        "    'Good-looking bootle. Aroma is pretty astonishig. Sour and sweet palate profile. I like the taste very much. Overall I can recommend it to everyone.',\n",
        "\n",
        "    'Tastes best from bottle. Not so heap as one could think. Nice hoppy smell. I had not supposed it will be sour though. Beautiful smooth head.',\n",
        "], \n",
        "[   [5, 10, 5, 10, 20], \n",
        "    [4, 8, 3, 7, 15]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rV3jWvFHxU7",
        "outputId": "ce944dd9-5c3c-4b22-a92f-3609100d6af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 0: \n",
            "Tokenized review text (stop words removed):\n",
            "['good', 'looking', 'bootle'] ['aroma', 'pretty', 'astonishig'] ['sour', 'sweet', 'palate', 'profile'] ['like', 'taste'] ['overall', 'recommend']\n",
            "Review text mapped to word ID-s: [14, 300, 17235] [2, 68, 0] [75, 3, 50, 318] [21, 13] [86, 1052]\n",
            "Predicted aspects ratings:  appearance: 5 aroma: 9 palate: 4 taste: 9 overall: 17\n",
            "\n",
            "Review 1: \n",
            "Tokenized review text (stop words removed):\n",
            "['tastes', 'best', 'bottle'] ['heap', 'think'] ['nice', 'hoppy', 'smell'] ['supposed', 'sour'] ['beautiful', 'smooth', 'head']\n",
            "Review text mapped to word ID-s: [117, 149, 8] [7873, 175] [11, 49, 143] [1326, 75] [308, 51, 1]\n",
            "Predicted aspects ratings:  appearance: 5 aroma: 6 palate: 3 taste: 6 overall: 13\n",
            "\n",
            "Review 2: \n",
            "Tokenized review text (stop words removed):\n",
            "['barely', 'drink'] ['smell', 'disgusting'] ['bottle', 'design', 'ugly', 'gets'] ['recommend']\n",
            "Review text mapped to word ID-s: [644, 110] [143, 2300] [8, 4204, 2112, 443] [1052]\n",
            "Predicted aspects ratings:  appearance: 1 aroma: 1 palate: 3 taste: 1 overall: 1\n",
            "\n",
            "Review 3: \n",
            "Tokenized review text (stop words removed):\n",
            "['pours', 'deep', 'copper', 'rusty', 'orange', 'initial', 'thick', 'white', 'creamy', 'head', 'pales', 'course', 'minutes'] ['incredible', 'lacing', 'creamy', 'quarter', 'inch', 'thick', 'head', 'slides', 'glass'] ['smell', 'quite', 'powerful', 'especially', 'bottle'] ['fresh', 'juicy', 'fruit', 'like', 'apples', 'mild', 'banana', 'general', 'spices', 'nose'] ['wheaty', 'grassy', 'well'] ['taste', 'great', 'subtle', 'flavours'] ['banana', 'peel', 'bubblegum', 'meted', 'balanced', 'general', 'bitterness', 'fades', 'wheaty', 'malt', 'flavour', 'like', 'banana', 'bread'] ['sip', 'fast', 'taste', 'bit', 'lemon', 'bitter', 'tang'] ['great', 'feel', 'carbonated', 'definitely', 'typical', 'beers', 'average', 'hefeweizen'] ['smooth', 'medium', 'body', 'heavy', 'light'] ['excellent', 'beer', 'definitely', 'bit', 'darker', 'ones', 'categories', 'unfiltered', 'wheat', 'beers', 'definitely', 'unique', 'taste', 'making', 'worthy', 'try']\n",
            "Review text mapped to word ID-s: [17, 84, 106, 1412, 36, 371, 76, 9, 54, 1, 3531, 762, 1064] [1076, 55, 54, 2490, 685, 76, 1, 3773, 94] [143, 53, 800, 445, 8] [128, 528, 46, 21, 298, 66, 121, 1227, 144, 31] [418, 130, 30] [13, 59, 219, 269] [121, 367, 557, 0, 89, 1227, 41, 390, 418, 7, 62, 21, 121, 139] [382, 594, 13, 24, 123, 22, 947] [59, 158, 215, 210, 293, 131, 112, 926] [51, 16, 20, 142, 4] [206, 5, 210, 24, 546, 1006, 6911, 1154, 92, 131, 210, 429, 13, 680, 1616, 220]\n",
            "Predicted aspects ratings:  appearance: 5 aroma: 10 palate: 5 taste: 9 overall: 17\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rating_rb_model.show_inference([\n",
        "    'Good-looking bootle. Aroma is pretty astonishig. Sour and sweet palate profile. I like the taste very much. Overall I can recommend it to everyone.',\n",
        "\n",
        "    'Tastes best from bottle. Not so heap as one could think. Nice hoppy smell. I had not supposed it will be sour though. Beautiful smooth head.',\n",
        "\n",
        "    'Could barely even drink it. The smell disgusting. Besides the bottle design is as ugly as it gets. Would not recommend.',\n",
        "\n",
        "    'Pours a deep copper, rusty orange, with an initial thick white creamy head that pales down over the course of 5 minutes. Incredible lacing as the creamy quarter inch thick head slides down the glass. \\\n",
        "        Smell is quite powerful, especially out of the bottle. Fresh juicy fruit like apples and mild banana, general spices on the nose. Wheaty and grassy as well. \\\n",
        "        Taste is great with some subtle flavours. Banana peel and bubblegum and meted out and balanced by some general bitterness that fades into a wheaty malt flavour, again, like banana bread. If you sip this fast you can taste a bit of lemon bitter tang in there too. \\\n",
        "        Great feel and not over carbonated, but definitely more than typical beers, but average for a Hefeweizen. Very smooth, medium body that isn’t too heavy or too light. Excellent beer, definitely a bit darker than other ones in the top categories for unfiltered wheat beers, and definitely has its own unique taste, making it a very worthy try for anyone!'\n",
        "    ],\n",
        "    [17, \n",
        "     13, \n",
        "     1, \n",
        "     17]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rating distributions"
      ],
      "metadata": {
        "id": "SSbXf5Z2L-UD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmaps"
      ],
      "metadata": {
        "id": "-U5FWSV2L8mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn\n",
        "\n",
        "def gen_heatmap(aspect1, aspect2, model):\n",
        "    s = seaborn.heatmap(model.alphas[(aspect1, aspect2)], \n",
        "                        xticklabels=range(1, model.ds.aspect_max[aspect1] + 1),\n",
        "                        yticklabels=range(1, model.ds.aspect_max[aspect2] + 1),\n",
        "                        cmap='bone')\n",
        "    s.set(xlabel=model.ds.aspects[aspect1], ylabel=model.ds.aspects[aspect2])"
      ],
      "metadata": {
        "id": "avIHvsE9JO4r"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_heatmap(4, 0, rating_rb_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "7FhugwEPMAWS",
        "outputId": "fee7bf2c-4312-44f6-d97a-15762475e3e0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9tKbic1oNnRM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "1e100ibu.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "environment": {
      "kernel": "conda-env-teststackoverflow-py",
      "name": "pytorch-gpu.1-10.m89",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}