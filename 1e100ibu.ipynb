{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madziejm/1e100-ibu/blob/master/1e100ibu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMhH5l9MeZho"
      },
      "source": [
        "## Preliminary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT48xPyXJHy9"
      },
      "source": [
        "#### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl9RJDTxJOTY",
        "outputId": "41d85b63-99a0-4489-aada-d6999f3d9a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev = cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'dev = {dev}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Yet9JpBt8kcx"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet icecream\n",
        "from icecream import ic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at8Z-ETyKngp"
      },
      "outputs": [],
      "source": [
        "# !pip install --quiet -Iv torch==1.10.1\n",
        "# !pip install --quiet -Iv torchtext==0.11.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcpSzAJmFV56"
      },
      "source": [
        "## Dataset representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ8cumFHEEQd",
        "outputId": "bfb7e570-29d2-4236-dad8-ae25575e7a05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 21.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install 'spacy<3.3.0,>=3.2.0' --quiet\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvQgSM-fB7YP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0866bc0-7515-427e-d17d-ad0e17f291cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 3.2.1\n",
            "Version: 1.10.0+cu111\n",
            "Version: 0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip show spacy | egrep Version\n",
        "# we want SpaCy 3\n",
        "!pip show torch | egrep Version\n",
        "!pip show torchtext | egrep Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ztJX3eRuqI"
      },
      "source": [
        "#### review example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rYfSug_5Vlf"
      },
      "outputs": [],
      "source": [
        "#     \"\"\"\n",
        "#     beer/name: John Harvards Simcoe IPA\n",
        "#     beer/beerId: 63836\n",
        "#     beer/brewerId: 8481\n",
        "#     beer/ABV: 5.4\n",
        "#     beer/style: India Pale Ale &#40;IPA&#41;\n",
        "#     review/appearance: 4/5\n",
        "#     review/aroma: 6/10\n",
        "#     review/palate: 3/5\n",
        "#     review/taste: 6/10\n",
        "#     review/overall: 13/20\n",
        "#     review/time: 1157587200\n",
        "#     review/profileName: hopdog\n",
        "#     review/text: On tap at the Springfield, PA location. Poured a deep and cloudy orange (almost a copper) color with a small sized off white head. Aromas or oranges and all around citric. Tastes of oranges, light caramel and a very light grapefruit finish. I too would not believe the 80+ IBUs - I found this one to have a very light bitterness with a medium sweetness to it. Light lacing left on the glass.\n",
        "#     \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yZeL2NQcSjP"
      },
      "source": [
        "#### dataset representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C1MCM0dWB93e"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from torchtext._torchtext import (Vocab as VocabPybind) # make use of some hidden interface\n",
        "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import gc # garbage collector interface\n",
        "import io\n",
        "import re\n",
        "import spacy # nlp toolkit\n",
        "import torch\n",
        "\n",
        "class RateBeerReviews(torch.utils.data.Dataset):\n",
        "    def __init__(self, filepath='/content/SNAP-Ratebeer.txt', reviews_max=float('inf')):\n",
        "        self.aspects = ['appearance', 'aroma', 'palate', 'taste', 'overall']\n",
        "        self.aspect_count = len(self.aspects)\n",
        "        self.aspect_max = [5, 10, 5, 10, 20]\n",
        "        self._aspect_ratings = [ [] for _ in self.aspects ]\n",
        "        self._texts = []\n",
        "        self.unkn_tok = \"<unk>\" # unknown/out of vocabulary token\n",
        "        self._len = 0\n",
        "        self._fetch_data(filepath, reviews_max)\n",
        "        self._post_process(max_word_count=20000) # 20K words should be okay\n",
        "\n",
        "    def _fetch_data(self, filepath, reviews_max):\n",
        "        with io.open(filepath, encoding='utf-8') as f:\n",
        "            for line in tqdm(f, total=(40938282 if reviews_max == float('inf') else reviews_max * 14), desc='Reading data'):\n",
        "                if line == '\\n': # separator\n",
        "                    self._len += 1\n",
        "                    if reviews_max <= self._len:\n",
        "                        break\n",
        "                elif line.startswith('review/appearance: '):\n",
        "                    line = line[len('review/appearance: '):]\n",
        "                    self._aspect_ratings[0].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/aroma: '):\n",
        "                    line = line[len('review/aroma: '):]\n",
        "                    self._aspect_ratings[1].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/palate: '):\n",
        "                    line = line[len('review/palate: '):]\n",
        "                    self._aspect_ratings[2].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/taste: '):\n",
        "                    line = line[len('review/taste: '):]\n",
        "                    self._aspect_ratings[3].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/overall: '):\n",
        "                    line = line[len('review/overall: '):]\n",
        "                    self._aspect_ratings[4].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/text: '):\n",
        "                    line = line[len('review/text: '):]\n",
        "                    if line.startswith('UPDATED:'):\n",
        "                        line = line[len(\"UPDATED: APR 29, 2008\"):] # drop prefix\n",
        "                    line = re.sub('~', ' ', line.strip()) # remove whitespace incl. trailing newline and tildes that can be found in data for some reason\n",
        "                    if line:\n",
        "                        self._texts.append(line)\n",
        "                    else: # some reviews do not have associated text; unwind (remove) their ratings for each aspect\n",
        "                        for aspect_ratings in self._aspect_ratings:\n",
        "                            aspect_ratings.pop()\n",
        "                        self._len -= 1\n",
        "\n",
        "    def _post_process(self, min_word_freq=None, max_word_count=None):\n",
        "        assert (min_word_freq is not None) ^ bool(max_word_count is not None), \"provide one of min_word_freq and max_word_count\"\n",
        "        nlp = spacy.util.get_lang_class('en')()\n",
        "        nlp.add_pipe(\"sentencizer\", config={\"punct_chars\": ['.', '?', '!']})\n",
        "        nlp.Defaults.stop_words |= { '-', '+'}\n",
        "        nlp.Defaults.stop_words -= {'mostly', 'whole', 'indeed', 'quite', 'ever', 'nothing', 'perhaps', 'not', 'no', 'only', 'well', 'really', 'except'}\n",
        "        print(\"Spacy pipe (tokenization&sentence split)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = [tuple(list(tok.lower_ for tok in sent if not tok.is_stop and not tok.is_punct and not tok.is_space and len(tok) > 2) for sent in doc.sents if 0 != len(sent)) for doc in nlp.pipe(self._texts)]\n",
        "        for i, text in enumerate(self._texts):\n",
        "            assert 0 != len(text) # make sure no empty reviews again (new could be introduced by removing stop words unfortunately)\n",
        "        print(\"Building vocab (word-id mapping)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        sent_gen = (sent for text in self._texts for sent in text)\n",
        "        if min_word_freq:\n",
        "            self.vocab = build_vocab_from_iterator(sent_gen, specials=[self.unkn_tok], min_word_freq=5)\n",
        "        else:\n",
        "            words = Counter()\n",
        "            for tokens in sent_gen:\n",
        "                words.update(tokens)\n",
        "            words = [word for word, freq in words.most_common(max_word_count)] # list sorted by frequency yikees\n",
        "            self.vocab = Vocab(VocabPybind(words, None))\n",
        "        self.vocab.insert_token(self.unkn_tok, 0)\n",
        "        self.vocab.set_default_index(self.vocab[self.unkn_tok]) # set index for out-of-vocabulary words\n",
        "        print(\"Mapping words to ids..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = [tuple(self.vocab.lookup_indices(sent) for sent in text) for text in self._texts]\n",
        "        gc.collect() # force garbage collection\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # # 1 # python\n",
        "        # sentences = tuple(sent for sent in self._texts[i])\n",
        "        # ratings = tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count))\n",
        "        # 2 # tensor\n",
        "        sentences = tuple(torch.LongTensor(sent) for sent in self._texts[i])\n",
        "        ratings = torch.LongTensor(tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count)))\n",
        "        # # 3 # dev\n",
        "        # sentences = tuple(torch.tensor(sent) for sent in self._texts[i])\n",
        "        # ratings = torch.tensor(tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count)))\n",
        "        return (sentences, ratings)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy1qrIH1CAlx"
      },
      "source": [
        "If you want to read dataset from dataset file, set FETCH_RATEBEER to true in the cell below and RECREATE_PICKLE to True. If you left them untouched, it'lle be read from serialized `RateBeerReviews` class object instead of parsing text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B_QTo-xBCelp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31595ef-9eee-47df-e1b5-ebf359a77cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VBDjyR4jpzAgzcDUGNQFguOfLC3rtOV_\n",
            "To: /content/ratebeer-20K-vocab.pickle\n",
            "\r  0%|          | 0.00/393M [00:00<?, ?B/s]\r  2%|2         | 8.91M/393M [00:00<00:04, 86.0MB/s]\r  4%|4         | 17.3M/393M [00:00<00:06, 55.4MB/s]\r 12%|#1        | 45.6M/393M [00:00<00:04, 72.9MB/s]\r 14%|#4        | 56.1M/393M [00:00<00:04, 73.4MB/s]\r 17%|#7        | 67.6M/393M [00:00<00:03, 82.3MB/s]\r 24%|##3       | 92.8M/393M [00:00<00:03, 94.7MB/s]\r 28%|##7       | 110M/393M [00:01<00:02, 102MB/s]  \r 33%|###3      | 131M/393M [00:01<00:02, 120MB/s]\r 37%|###7      | 146M/393M [00:01<00:01, 129MB/s]\r 42%|####1     | 165M/393M [00:01<00:01, 141MB/s]\r 47%|####7     | 185M/393M [00:01<00:01, 143MB/s]\r 53%|#####3    | 209M/393M [00:01<00:01, 163MB/s]\r 58%|#####7    | 228M/393M [00:01<00:01, 153MB/s]\r 64%|######4   | 252M/393M [00:01<00:01, 141MB/s]\r 71%|#######   | 278M/393M [00:02<00:00, 162MB/s]\r 75%|#######5  | 296M/393M [00:02<00:00, 145MB/s]\r 81%|########  | 317M/393M [00:02<00:00, 157MB/s]\r 85%|########4 | 334M/393M [00:02<00:00, 109MB/s]\r 90%|######### | 355M/393M [00:02<00:00, 128MB/s]\r 95%|#########4| 372M/393M [00:02<00:00, 133MB/s]\r100%|##########| 393M/393M [00:02<00:00, 136MB/s]\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "export FETCH_RATEBEER=false\n",
        "if [ \"$FETCH_RATEBEER\" = true ] && [ -e $RATEBEER_FILE ]\n",
        "then # original dataset\n",
        "    export RATEBEER_FILE='/content/SNAP-Ratebeer.txt'\n",
        "    gdown --id '12tEEYQcHZtg5aWyfIiWWVIDAJNT-5d_T' # https://drive.google.com/file/d/12tEEYQcHZtg5aWyfIiWWVIDAJNT-5d_T/view?usp=sharing\n",
        "    echo \"Dataset head (trailing newline makes entry end): \"\n",
        "    head -n 16 $RATEBEER_FILE\n",
        "    iconv -f ISO-8859-1 -t UTF-8 $RATEBEER_FILE -o {RATEBEER_FILE}.new && mv {RATEBEER_FILE}.new $RATEBEER_FILE\n",
        "else # pickle\n",
        "    gdown --id '1VBDjyR4jpzAgzcDUGNQFguOfLC3rtOV_' # https://drive.google.com/file/d/1VBDjyR4jpzAgzcDUGNQFguOfLC3rtOV_/view?usp=sharing  # 20K words dataset\n",
        "    # gdown --id '1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j' # https://drive.google.com/file/d/1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j/view?usp=sharing # min frequency 5 words dataset\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N5zmSYDBCg4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "255da3aa-811e-4925-e340-7ee0f8b17543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ],
      "source": [
        "import pickle # serialize lib\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/drive')\n",
        "\n",
        "# DATASET_PICKLE='/content/ratebeer.pickle'\n",
        "DATASET_PICKLE='/content/ratebeer-20K-vocab.pickle'\n",
        "RECREATE_PICKLE = False\n",
        "\n",
        "if RECREATE_PICKLE:\n",
        "    rb = RateBeerReviews()\n",
        "    with open(DATASET_PICKLE, 'wb') as f:\n",
        "        print('Dumping..')\n",
        "        torch.save(rb, f)\n",
        "else:\n",
        "    with open(DATASET_PICKLE, 'rb') as f:\n",
        "        rb = torch.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FYkddHhTN8vW"
      },
      "outputs": [],
      "source": [
        "rb.aspect_max = [5, 10, 5, 10, 20] # hotfix\n",
        "rb.anchor_words = { 'appearance' : ('appearance', 'color'), \n",
        "                    'aroma'      : ('aroma'),\n",
        "                    'palate'     : ('palate', 'mouthfeel'),\n",
        "                    'taste'      : ('taste'),\n",
        "                    'overall'    : ('overall') }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ0p4rxp2UJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f0e880-7b6f-4a6b-e245-6fefb53c18f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20001\n",
            "['tractor', 'syd', 'chiefly', 'defiant', 'stripper', 'val', 'bierhaus', 'eyeball', 'sporatic', 'deviate', 'grapenuts', 'beervana', 'tarness', 'lenny', 'phenomenon', 'reek', 'usualy', 'bernard', 'barbecued', 'fruitbeer', 'enjoyit', 'rive', 'sas', 'flavorable', 'ecstatic', 'leopoldstoch', 'soulless', 'allagashs', 'mikrobryggeri', 'disgusted', 'heine', 'covey', 'respectively', 'scout', 'emitting', 'flemdawg', 'atom', 'brightens', 'pch', 'www.bierzwerg.de', 'oakwood', 'flagged', 'collor', 'rbnag-11', 'belter', 'ecstasy', 'deem', 'gust', 'creamyish', 'opq', 'fruti', 'accompagne', 'zunge', 'sinful', 'prunish', 'ingrediants', 'refill', 'rug', 'alc%', 'blase', 'paramount', 'oldrtybastrd', 'fininsh', 'sofort', 'sazz', 'echoing', 'prospect', 'contributor', 'crazily', 'rogueone', 'beer_hawk', 'presenza', 'overlain', 'hinteren', 'spur', 'suspicions', 'glendale', 'renowned', 'hover', '09/08/2008', 'flashback', 'degraded', 'vegetabley', 'sunburst', 'snug', 'beneficial', 'linkery', 'mtn', 'doublebock', 'bevy', 'itch', 'instances', 'filmed', 'pliney', 'equinox', 'habe', 'frying', 'froot', 'admired', 'torbida', 'lassan', 'blatantly', 'poolside', 'finishable', 'rename', 'understandably', 'caramel/', 'luggage', 'clusters', 'thingy', 'enemy', 'tilting', 'feelin', 'halogen', 'whichever', 'soulard', 'rlb', 'slot', 'byproducts', 'humboldt', 'scoff', 'f&amp;t', 'jaghana', 'companions', 'deeep', 'inaugural', 'unadventurous', 'cutler', 'five+', 'resemblence', 'arising', 'wimbledon', 'index.html', 'flemmish', 'talc', 'liefmans', 'coniston', 'absorb', 'denisons', '340ml', 'stroh', 'appease', 'paradox', 'fella', 'greedy', 'ome', 'ead', 'wre', 'nastiest', '20110129', 'laff', 'chocoffee', 'insists', 'bratislava', 'lungo', 'induce', 'quebecois', 'quadrupels', 'precede', 'crackly', '20/20', 'licious', 'strangly', 'conscious', 'centers', 'turva', 'tycker', 'smoooooth', 'buildup', 'mouthfeeel', 'lastin', 'declare', '1.89', 'delhaize', 'gyldenbrun', 'isnt', 'rdbrun', 'sommer', 'units', 'outs', 'ale--2007', 'galena', 'locked', 'historically', 'transform', 'bleed', 'burpy', 'quckly', 'hungover', 'cappuchino', 'facts', 'individuality', 'stability', 'everclear', 'witnessed', 'dpt', 'deau', 'fittingly', 'houblonnage', 'fortement', 'pcc', 'cui', 'legally', 'bde', 'massage', 'strident', 'bf08', 'treacley', 'uric', 'spewing', 'stripes', 'spa', '5,5', 'isles', 'google', 'graphite', 'contemplating', 'wesley', 'petersburg', 'humulus', 'recovered', 'erased', 'deuce', 'crawfish', 'creamily', 'boozed', 'blueberrys', 'provoking', 'turnover', 'propose', 'warranted', 'attain', '100ml', 'saf', 'mspindler', 'effusive', 'epics', 'spontaneously', 'toch', 'tuoksussa', '270511', 'frankenmuth', 'rg1', 'wendy', 'rbsg2003', 'frome', 'heiniken', 'potosi', 'erasmus', 'bedondaine', 'coffee-', 'toom', 'weakens', '3.5/5.0score', 'flyer', 'drinakble', 'bleah', 'charamel', 'surviving', 'challenges', 'alkoholos', 'caramalts', 'deffinately', 'tooo', 'differant', 'eloquent', 'asset', 'ohhh', 'retros', 'ambree', 'fantasic', 'altho', 'adjustment', 'umbrella', 'petty', 'unwashed', 'mouldering', 'arousing', 'czechs', 'backwash', 'goldy', 'betting', 'lloyds', 'fulham', 'plymouth', 'focussed', 'input', 'beard', 'thanks!pours', 'brles', 'underlined', 'chracter', 'scone', 'shores', 'patty', 'links', \"'em\", '20100803', 'vortex', 'robbed', 'bombardment', 'largo', '7/4/8/4/16', 'lagerbier', 'dripped', 'narines', 'woolly', 'grief', 'contend', 'solar', 'compaired', 'misguided', 'yardwork', 'cloak', 'flavoe', 'spices-', 'grapefuity', 'southport', 'styling', 'szpen', 'nowt', 'aesthetic', 'zombie', 'flanked', 'hater', 'deciso', 'badged', 'lowlander', 'beiaard', 'dishwatery', 'humide', 'guineas', 'zeitgeist', 'tug', 'dewy', 'amp', 'gotta', 'anvil', 'commemorate', 'devotion', 'application', 'anyhoo', 'grav', 'merky', 'fatheads', 'pepperish', 'sunes', 'feminine', 'pivovar', 'nth', 'sorbon', 'tappours', 'spitty', 'macdoogals', '2.29', 'spitfire', 'templar', 'tapny', 'tying', 'geur', 'b&amp;t', 'mcfarland', 'tregs', 'rodger', 'heidelberg', 'shiver', 'aussies', 'orangina', 'coexist', 'recurring', 'unangenehm', 'elmers', 'conserve', 'lager-', 'bietet', '20110416', 'vry', 'gratifying', 'unvaried', 'complexes', 'definently', 'prevalant', 'ceder', 'disclosure', 'wiffs', 'foaminess', 'yowza', 'segment', 'playdough', 'steriods', 'vitt', 'thirsting', 'obliged', 'glgg', 'persuasion', 'wagner', 'bristling', 'sixpence', 'elgin', 'briank.', 'repour', 'tweety', 'chocolate/', 'bulbous', 'all-', 'coworker', 'partying', 'mund', 'citirc', 'strasse', 'chatting', 'barclays', 'ownership', 'vera', 'pourri', 'mediumbroad', 'masculine', 'bonnes', 'aftetaste', 'ellum', 'killian', 'seperates', 'restrain', 'dogfishhead', 'kclingers', 'rangers', 'everthing', 'black-', 'rideau', 'gauze', 'winding', 'chiffon', 'sone', 'frumento', 'sera', 'magt', 'citrics', 'craziness', 'decient', 'dextrin', 'veel', 'fazit', 'loaves', 'stubbornly', 'outlandish', 'pondering', 'copperfields', 'pil', 'afore', 'measuring', 'trained', 'terry', 'midlothian', 'dissect', 'semi-', 'scones', 'kennedy', '1/5', '1pint', 'melody', 'boutin', 'perfuminess', '12z', 'narke', '1.8/5.0drinkability', 'ferocious', 'hammersmith', 'earhy', 'dorm', 'barrow', 'hazing', 'peaceful', 'periphery', 'babe', 'oettinger', 'widmers', 'mibromeo', 'sowie', 'cbf07', 'chien', 'cigarcitybrew', 'porn', 'gratitude', 'wlv', 'parallels', 'mly', 'nitroed', 'geringer', 'harmonize', 'erhalten', 'weicher', 'berzeugen', 'surfaced', 'overactive', 'fruitful', 'reccommend', 'amt', 'afther', 'beermerchants', 'treating', 'critic', 'durao', 'grapeskins', 'entrancing', 'brains', 'uncharacteristically', 'supporter', 'mealiness', 'boggling', 'questioning', 'lawnmowing', 'classica', 'turbulent', 'immortal', 'dai', 'glasset', 'rips', 'restaraunt', 'metall', 'scope', 'sulfites', 'moneys', 'unruly', 'scotish', 'phils', '25/09/2009', 'mattyb83', 'attend', 'unroasted', 'lgith', 'disolves', 'residence', 'sessioned', 'kingpinipa', 'goof', 'boards', 'disconnected', 'fulfilled', 'critter', 'soviet', 'aleish', 'decidely', 'chocolte', 'realization', 'adheres', 'bruins', 'drinakable', 'exprience', 'exhausted', 'harmonized', 'detached', 'orangs', 'froide', 'regualr', '06/08/2010', 'getrunken', 'underlaying', 'importing', 'beermongers', 'schultheiss', 'irrelevant', 'knny', 'maltbase', 'germ', 'teve', 'ehhhh', 'snort', 'dimished', 'bottled@one', 'benefitted', 'brewery-', 'southside', 'walmart', 'outshines', 'top.3', \"ol'\", '17/09/2010', 'crystallised', 'texans', '08/08/2008', 'crumbled', 'hausbrauerei', 'grimstad', 'showy', 'fing', 'resplendent', 'simcoes', 'doe', 'suburban', 'portery', 'waterbury', 'bodacious', 'illegible', 'vielleicht', 'centuries', 'puzzle', 'republics', 'sauv', 'roads', 'potsdam', 'brennans', '12pack', 'cly', 'herew', 'merrifield', 'tuning', 'chapter', 'drmatt', 'subsumed', 'smoothens', 'lag', 'pinesap', 'breeds', 'langsam', 'scharf', '291108', 'vollmundiger', 'coulour', 'lembra', 'aftershave', 'occassions', 'cope', 'assist', 'mdio', 'pennington', 'jamais', 'sdme', 'frugter', '-sweet', 'logged', 'intricately', 'slurp', 'rohrbachs', 'pitfruit', 'wigston', 'specialist', 'encrusted', 'durability', 'incident', 'employees', 'dart', 'head.nice', 'homey', 'intimidated', 'alcohol-', 'ambition', 'boutique', 'maid', 'cratered', '2200', 'discrte', 'leau', 'acidule', 'coriande', 'avocado', 'poudre', 'chaude', 'concasse', 'voir', 'chimney', 'minmal', 'beeriness', 'praises', 'papier', 'targeted', 'onthe', 'bustling', 'hosp', 'pomelo', 'steeping', '3/5.0score', 'eagles', 'tonge', 'organge', 'reel', 'grazie', 'deficient', 'maltines', 'baden', 'pedals', 'manassas', 'engulfs', 'michelobs', 'beersofeurope.co.uk', 'suerliche', 'legion', 'teeming', 'bourbonny', 'nicks', 'syndrome', 'sournes', 'lemonheads', 'bza', 'straighforward', 'mothballs', 'rambling', 'ward', 'eddie', 'itap', 'tynd', '35,5cl', 'fusely', 'cdumler', 'sessionnable', 'yell', 'stables', 'requirements', 'promotes', 'detest', 'golds', 'venant', 'spendy', 'excitable', 'freshman', 'novices', 'obtuse', 'tigers', 'milkchocolate', 'alls', 'egyptian', 'carafa', 'hvede', 'abrasively', '1pt9.4floz', 'answered', 'bitteren', 'vanillia', 'hophead75', 'warhead', 'puking', 'interbrew', 'homies', 'mixpack', 'aruba', 'viscousness', 'excursion', 'subtlely', 'imprint', 'osaka', 'strt', '20110802', 'paddle', 'ronnie', 'folly', 'celcius', 'speaker', '0,75l', 'tests', 'rend', 'fruitiest', 'null', 'aces', 'rosewood', 'inexplicably', 'oteyj', 'completion', 'trending', 'resonance', 'seemd', 'unlucky', 'keizer', 'orane', 'keyboard', 'jester', 'resonating', 'witty', 'aching', 'peely', 'ummmm', 'huntersville', 'virtues', 'exaggeration', 'bodes', 'slickly', 'palme', 'dry-', 'reedy', 'voto', 'primed', 'stressed', 'slicker', 'cornboy', 'poorest', 'performed', 'ticklish', 'thebeersommelier', 'poure', 'e.smag', 'cannons', 'rosty', 'bitternote', 'ahtanum', 'crispies', 'docked', 'chills', 'extrmement', 'aroma--', 'steigt', 'semblent', 'domin', 'tobaccoish', 'mastery', 'ontop', 'xtra', 'vieillie', 'coincide', 'hanssens', 'unavoidable', 'gah', 'fishermans', 'raisinny', 'petaluma', 'mich.', 'evan', 'concentrates', 'apr.', 'underfermented', 'bpa', '6/3/7/3/14', 'sweltering', 'fodder', 'sedate', 'barking', 'mde', 'genova', 'arkadia', 'liquoricey', 'penderels', 'wussy', 'megmarad', 'pilz', 'sudwerk', '7,5', 'channel', 'swath', 'tastybeer', 'erica', 'laser', 'reigned', 'mainlly', 'requiring', 'bottled@fonefan', 'retension', 'ebbs', 'meadows', 'impatient', 'worms', 'pranqster', 'caleb', 'claus', 'churchill', 'coins', 'beaches', 'unclemike', 'pannuhuone', 'mystic', 'aromai', 'raining', 'compacte', 'versione', 'invasion', 'spilt', 'ambered', 'chaps', 'hanover', 'tissue', 'caffreys', 'befejezse', 'cocos', 'evidente', 'cludy', 'iconic', 'steelers', 'mariage', 'serpents', 'rapsberry', 'dulles', 'galss', 'cameo', 'withstand', 'requested', 'gesture', 'scintillating', 'badgers', '20081004', 'pumps', 'milden', 'frische', 'disapoint', 'archetype', 'presenta', 'internal', 'ooooh', 'dattes', 'alcoho', 'kasteel', 'beauties', 'profusion', 'adventures', 'finsish', 'nestles', 'capitals', 'beglian', 'slo', 'alcoholly', 'sliky', 'bearer', 'acidula', 'startlingly', 'sligt', 'pieno', 'pods', 'unintended', 'ettersmaken', 'darkgolden', 'bsp', 'stlig', 'indefinable', 'tangier', 'talcum', 'gears', 'clarks', 'dre', 'cheltenham', 'xiii', 'bolster', 'dmac621', 'beastly', 'dimly', 'brotig', 'gbf', 'mouthfeal', 'smeel', 'shivers', 'expose', 'zitronig', 'celui', 'cependant', 'coppers', 'enticed', 'precedence', 'merritt', 'unfairly', 'nothings', 'directed', 'earthed', 'bragging', 'wouldnt', 'snca', 'scraped', 'escorted', 'majors', 'stance', 'forge', 'ncbc', 'gravesend', 'oranged', 'utzben', '040409', '10/2/10', 'proceedings', 'transferred', 'marys', 'berriness', 'nto', 'lance', 'madrid', 'yeasted', 'consistenza', 'lwenbru', 'mondo', 'prosit', 'oakish', 'annoys', 'simmer', 'torrent', 'geranium', 'vicinity', 'rfds', 'dukes', 'ecrvich', 'slime', 'gewisse', 'drunks', 'destin', 'dixen', 'dadurch', 'rom', 'employed', 'sittin', 'stuffs', 'dpinette', 'papa', '-at', 'hsc', 'wickedpete', 'awaisanen', '7.5%abv', 'neil', 'exponentially']\n"
          ]
        }
      ],
      "source": [
        "# word ID-s count and 1K of least common words\n",
        "print(len(rb.vocab.get_itos()))\n",
        "print(rb.vocab.get_itos()[-1000:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Hsa1YLKT3J"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOc_2b-E4o0c"
      },
      "source": [
        "### Training (implementation of $(1)$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "05JPg4ATQTOW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "import datetime\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from more_itertools import grouper\n",
        "\n",
        "# for wordcloudsdest_path=dest_path\n",
        "import functools\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from PIL import Image\n",
        "from os import path\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import functools\n",
        "from operator import iadd\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, dataset):\n",
        "        self.ds = dataset\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        word_count = len(self.ds.vocab.get_itos())\n",
        "        self.theta = torch.rand((word_count, self.ds.aspect_count), device=dev)\n",
        "        # scale to [-0.1, 0.0], as we enforce this weight to 1.0 for some words later on\n",
        "        self.theta = self.theta * -0.1\n",
        "        # scale to [0.0, 0.9], as we enforce this weight to 1.0 for some words later on\n",
        "        # self.theta = self.theta * 0.9\n",
        "        # enforce 1 initialization on aspect name (page 4)\n",
        "        # aspect_ids = self.ds.vocab.lookup_indices(self.ds.aspects)\n",
        "        for aspect_idx, aspect in enumerate(self.ds.aspects):\n",
        "            words = list(self.ds.anchor_words[aspect])\n",
        "\n",
        "            words_ids = self.ds.vocab.lookup_indices(words)\n",
        "            # ic(aspect_id)\n",
        "            # ic(aspect_idx)\n",
        "            # ic(self.ds.vocab.get_itos()[aspect_id])\n",
        "            for word_id in words_ids:\n",
        "                self.theta[word_id, aspect_idx] = 1.0\n",
        "        self.theta.requires_grad_()\n",
        "\n",
        "        # introduce separate phi for each aspect\n",
        "        # self.phis = [torch.rand((word_count, self.ds.aspect_max[i])).to(dev) for i in range(self.ds.aspect_count)]\n",
        "        self.phis = [torch.zeros((word_count, self.ds.aspect_max[i]), device=dev, dtype=self.theta.dtype) for i in range(self.ds.aspect_count)]\n",
        "        # # normalize that sum across all words is 1 for a given aspect (eq. 7) # do not normalize as for now\n",
        "        # self.phis = [phi / phi.sum(dim=0) for phi in self.phis]\n",
        "        for phi in self.phis: phi.requires_grad_()\n",
        "    \n",
        "    def word_clouds(self, dest_path='/drive/MyDrive/Colab Notebooks/1e100ibu/saves/', filename='words.png', show=True):\n",
        "        words = self.ds.vocab.get_itos()\n",
        "        fig = plt.figure(figsize=(21, 5))\n",
        "        plt.subplots_adjust(wspace=0.2)\n",
        "        # plt.subplots_adjust(wspace=0.01, hspace=0.000000001)\n",
        "        # plt.tight_layout()\n",
        "        i = 0\n",
        "\n",
        "        for aspect in range(self.ds.aspect_count):\n",
        "            # print()\n",
        "            aspect_name = self.ds.aspects[aspect]\n",
        "            # print(aspect_name)\n",
        "            atheta = self.theta[:, aspect].tolist()\n",
        "            \n",
        "            zipped = list(zip(words, atheta))\n",
        "            # ic(list(filter(lambda x: x[0] == 'antrunk', zipped)))\n",
        "            # sorted_zip = sorted(zipped, reverse=True, key=lambda x: x[1])[:50]\n",
        "            # print(*[(word, '%s' % float('%.3g' % weight)) for (word, weight) in sorted_zip])\n",
        "\n",
        "            wc = WordCloud(background_color=\"white\", color_func=lambda *args, **kwargs: 'black', width=1000, height=1000)\n",
        "            wc.generate_from_frequencies(dict(zipped))\n",
        "\n",
        "            fig.add_subplot(self.ds.aspect_count, max(self.ds.aspect_max) + 1, i * (max(self.ds.aspect_max) + 1) + 1)\n",
        "            plt.imshow(wc)\n",
        "            plt.title(f'{aspect_name} Theta', fontsize=2)\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            for rating in range(self.ds.aspect_max[aspect]):\n",
        "                aphi = self.phis[aspect][:, rating].tolist()\n",
        "\n",
        "                zipped = list(zip(words, aphi))\n",
        "                sorted_zip = sorted(zipped, reverse=True, key=lambda x: x[1])[:200]\n",
        "\n",
        "                if any(a == 0 for _, a in sorted_zip):\n",
        "                    print(f'Omitting {aspect_name} Phi {str(rating + 1)}')\n",
        "                    continue\n",
        "\n",
        "                wc = WordCloud(background_color=\"white\", color_func=lambda *args, **kwargs: 'black', width=1000, height=1000)\n",
        "                wc.generate_from_frequencies(dict(zipped))\n",
        "                fig.add_subplot(self.ds.aspect_count, max(self.ds.aspect_max) + 1, i * (max(self.ds.aspect_max) + 1) + rating + 2)\n",
        "\n",
        "                plt.imshow(wc)\n",
        "                plt.title(f'{aspect_name} Phi {str(rating + 1)}', fontsize=2)\n",
        "                plt.axis(\"off\")\n",
        "\n",
        "            i += 1\n",
        "        plt.savefig(f'{dest_path}/{filename}', dpi=600)\n",
        "        plt.show(block=show)\n",
        "    \n",
        "    def rev_words_thetas(self, rev_sens_ids):\n",
        "        \"\"\"\n",
        "        TODO filename sentence_aspects_likelihood_theta\n",
        "        \"\"\"\n",
        "        return [self.theta[sen_ids] for sen_ids in rev_sens_ids]\n",
        "\n",
        "    def rev_words_phis(self, rev_sens_ids):\n",
        "        \"\"\"\n",
        "        TODO filename sentence_aspects_likelihood_phi\n",
        "        \"\"\"\n",
        "        return [[self.phis[aspect_idx][sen_ids, :] for aspect_idx in range(self.ds.aspect_count)] for sen_ids in rev_sens_ids]\n",
        "    \n",
        "    def dump_weights(self, dest_path='/drive/MyDrive/Colab Notebooks/1e100ibu/saves/', filename=''):\n",
        "        weights = {'phis': self.phis, 'theta': self.theta}\n",
        "        torch.save(weights,  f'{dest_path}/{filename}')\n",
        "\n",
        "    def load_weights(self, src_path):\n",
        "        weights = torch.load(src_path, map_location=torch.device(dev))\n",
        "        self.theta = weights['theta']\n",
        "        self.phis  = weights['phis']\n",
        "    \n",
        "    def _linear_assignement(self, costs):\n",
        "        # for nll we want to minimize\n",
        "        return linear_sum_assignment(costs, maximize=False)\n",
        "    \n",
        "    def plot_nll_history(self, dir, filename):\n",
        "        plt.figure().set_facecolor('white') # no alpha please\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('NLL')\n",
        "        plt.xlim(left=0.0, right=self.train_epoch_history[-1])\n",
        "        plt.ylim(bottom=min(0.0, min(self.train_nll_history)), top=max(max(self.train_nll_history), max(self.test_nll_history)))\n",
        "        plt.plot(self.train_epoch_history, self.train_nll_history, '-o')\n",
        "        plt.plot(self.test_epoch_history, self.test_nll_history, '-o')\n",
        "        plt.legend(['train batch NLL', 'test mean batch NLL'], loc='upper right')\n",
        "        plt.title('Train and test dataset NLL')\n",
        "        plt.savefig(os.path.join(dir, filename))\n",
        "        plt.show()\n",
        "\n",
        "    def experiments(self, root_dir='/drive/MyDrive/Colab Notebooks/1e100ibu/saves/'):\n",
        "        num_of_runs = 3\n",
        "\n",
        "        for idx in range(num_of_runs):\n",
        "            directory = str(datetime.datetime.now())\n",
        "            path = os.path.join(root_dir, directory)\n",
        "            os.mkdir(path)\n",
        "\n",
        "            self.train(dest_path=path)\n",
        "\n",
        "        for idx in range(num_of_runs):\n",
        "            directory = str(datetime.datetime.now()) + '-adam'\n",
        "            path = os.path.join(root_dir, directory)\n",
        "            os.mkdir(path)\n",
        "\n",
        "            self.train(dest_path=path, optim='adam')\n",
        "\n",
        "    def test_nll(self, debug=False):\n",
        "        nlls = []\n",
        "        with torch.no_grad():\n",
        "            test_loader = DataLoader(self.test_ds, batch_size=100, collate_fn=lambda x: x) # do not use default collate function as it requires fixed-length input and raises this exception otherwise https://github.com/pytorch/pytorch/issues/42654\n",
        "            for i, batch in enumerate(tqdm(test_loader, desc='test validation')):\n",
        "                batch_nlls = []\n",
        "                for (rev_sents_ids, review_aspects_scores) in batch:\n",
        "                    rev_thetas = self.rev_words_thetas(rev_sents_ids)\n",
        "                    rev_phis   = self.rev_words_phis(rev_sents_ids)\n",
        "                    res_sents_scores = torch.stack(\n",
        "                        [\n",
        "                        rev_thetas[j].sum(dim=0) + torch.stack(tuple(rev_phis[j][a][:, review_aspects_scores[a] - 1].sum() for a in range(self.ds.aspect_count))) # 1 x aspect count\n",
        "                        for j in range(len(rev_sents_ids))\n",
        "                        ],\n",
        "                    ) # sent count x aspect count\n",
        "\n",
        "                    denoms = torch.logsumexp(res_sents_scores, dim=1)[:, None]\n",
        "                    assert(denoms.shape == (len(rev_sents_ids), 1))\n",
        "\n",
        "                    res_sents_scores = -res_sents_scores + denoms\n",
        "\n",
        "                    sents_aspect_preds_max = torch.argmin(res_sents_scores, dim=1)\n",
        "                    row_ind, col_ind = self._linear_assignement(costs=res_sents_scores.detach().cpu().numpy())\n",
        "                    sents_aspect_preds_linear = sents_aspect_preds_max\n",
        "\n",
        "                    sents_aspect_preds_linear[row_ind] = torch.from_numpy(col_ind).to(dev)\n",
        "                    \n",
        "                    batch_nlls.append(res_sents_scores.take_along_dim(sents_aspect_preds_linear[:, None], dim=1).sum().cpu().item())\n",
        "                nlls.append(functools.reduce(lambda a, b: a + b, batch_nlls))\n",
        "        nll_sum = functools.reduce(lambda a, b: a + b, nlls)\n",
        "        if debug: ic(nll_sum)\n",
        "        if debug: ic(torch.exp(-torch.tensor(nll_sum)))\n",
        "        mean_batch_nll = nll_sum / len(nlls)\n",
        "        return mean_batch_nll\n",
        "\n",
        "\n",
        "    def train(self, dest_path='/drive/MyDrive/Colab Notebooks/1e100ibu/saves/', epoch_count=1, optim='sgd'):\n",
        "        train_size = int(0.8 * len(self.ds))\n",
        "        test_size = len(self.ds) - train_size\n",
        "\n",
        "        params = (\n",
        "            self.theta,\n",
        "            *self.phis\n",
        "        )\n",
        "        lr = 0.0005\n",
        "        # lr = sum(self.ds.aspect_max) * 0.01 / train_size # for whole batch iteration\n",
        "\n",
        "        weight_decay = 0.00005\n",
        "        momentum = 0.1\n",
        "        if optim == 'sgd':\n",
        "            self._optim = torch.optim.SGD(\n",
        "                params=params,\n",
        "                lr=lr,\n",
        "                weight_decay=weight_decay,\n",
        "                momentum=momentum\n",
        "            )\n",
        "        elif optim == 'adam': # do not use Adam, weights go to NaN with it for some reason\n",
        "            self._optim = torch.optim.Adam(\n",
        "                params=params,\n",
        "                lr=lr,\n",
        "                weight_decay=weight_decay,\n",
        "                betas=(momentum, 0.999) # the first is params momentum, second RMSProp momentum (for now fix to 0.999 which is default Pytorch value)\n",
        "            )\n",
        "        else:\n",
        "            assert False\n",
        "        self._sched = torch.optim.lr_scheduler.StepLR(self._optim, step_size=1, gamma=0.95)\n",
        "\n",
        "\n",
        "        self.train_ds, self.test_ds = random_split(self.ds, [train_size, test_size])\n",
        "\n",
        "        train_loader = DataLoader(self.train_ds, batch_size=100, shuffle=True, collate_fn=lambda x: x) # do not use default collate function as it requires fixed-length input and raises this exception otherwise https://github.com/pytorch/pytorch/issues/42654\n",
        "        self.train_epoch_history = []\n",
        "        self.test_epoch_history = []\n",
        "        self.train_nll_history = []\n",
        "        self.test_nll_history = []\n",
        "        try:\n",
        "            for epoch in range(epoch_count):\n",
        "                batch_count = len(train_loader)\n",
        "                for i, batch in enumerate(tqdm(train_loader, desc=f'train epoch {epoch}/{epoch_count}')):\n",
        "                    batch_nlls = []\n",
        "                    for (rev_sents_ids, review_aspects_scores) in batch:\n",
        "                        # for sent_ids in rev_sents_ids: sent_ids.to(dev)\n",
        "                        rev_thetas = self.rev_words_thetas(rev_sents_ids)\n",
        "                        rev_phis   = self.rev_words_phis(rev_sents_ids)\n",
        "\n",
        "                        res_sents_scores = torch.stack(\n",
        "                            [\n",
        "                            rev_thetas[j].sum(dim=0) + torch.stack(tuple(rev_phis[j][a][:, review_aspects_scores[a] - 1].sum() for a in range(self.ds.aspect_count))) # 1 x aspect count\n",
        "                            for j in range(len(rev_sents_ids))\n",
        "                            ],\n",
        "                        ) # sent count x aspect count\n",
        "\n",
        "                        denoms = torch.logsumexp(res_sents_scores, dim=1)[:, None]\n",
        "                        assert(denoms.shape == (len(rev_sents_ids), 1))\n",
        "\n",
        "                        res_sents_scores = -res_sents_scores + denoms\n",
        "\n",
        "                        sents_aspect_preds_max = torch.argmin(res_sents_scores, dim=1)\n",
        "                        row_ind, col_ind = self._linear_assignement(costs=res_sents_scores.detach().cpu().numpy())\n",
        "                        sents_aspect_preds_linear = sents_aspect_preds_max\n",
        "\n",
        "                        # (most likely) aspect assignments (5)\n",
        "                        sents_aspect_preds_linear[row_ind] = torch.from_numpy(col_ind).to(dev)\n",
        "                        \n",
        "                        # sentence likelihood (6)\n",
        "                        batch_nlls.append(res_sents_scores.take_along_dim(sents_aspect_preds_linear[:, None], dim=1).sum())\n",
        "\n",
        "                    batch_nll = torch.stack(batch_nlls).sum()\n",
        "                    self._optim.zero_grad(set_to_none=True)\n",
        "                    if 0 == i % 100:\n",
        "                        self.train_nll_history.append(batch_nll.cpu().detach().item())\n",
        "                        self.train_epoch_history.append(epoch + i / batch_count)\n",
        "                    if 0 == i % 5000:\n",
        "                        if i != 0:\n",
        "                            self.dump_weights(dest_path=dest_path, filename=(f'-epoch-{epoch}-{epoch_count}-{int(i)}'))\n",
        "                            self.word_clouds(dest_path=dest_path, filename=(f'cloud-epoch-{epoch}-{epoch_count}-{int(i)}.png'))\n",
        "                            ic(self.rev_words_thetas([self.ds.vocab.lookup_indices(['taste', 'aroma', 'palete', 'antrunk'])]))\n",
        "                        # self.test_nll_history.append(self.test_nll())\n",
        "                        # self.test_epoch_history.append(epoch + i / batch_count)\n",
        "                    batch_nll.backward()\n",
        "\n",
        "                    self._optim.step()\n",
        "                self._sched.step()\n",
        "\n",
        "            self.dump_weights(dest_path=dest_path, filename=(f'weights-end'))\n",
        "            self.word_clouds(dest_path=dest_path, filename=(f'cloud-end.png'))\n",
        "            self.plot_nll_history(dest_path, (f'plot-end.png'))\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print('Interrupted.')\n",
        "\n",
        "            self.dump_weights(dest_path=dest_path, filename=(f'weights-interrupt'))\n",
        "            self.word_clouds(dest_path=dest_path, filename=(f'cloud-interrupt.png'))\n",
        "            self.plot_nll_history(dest_path, (f'plot-interrupt.png'))\n",
        "\n",
        "        except Exception as e:\n",
        "            ic(len(batch_nlls))\n",
        "            ic(denoms)\n",
        "            ic(rev_sents_ids)\n",
        "\n",
        "            self.dump_weights(dest_path=dest_path, filename=(f'weights-exception'))\n",
        "            self.word_clouds(dest_path=dest_path, filename=(f'cloud-exception.png'))\n",
        "            self.plot_nll_history(dest_path, (f'plot-exception.png'))\n",
        "\n",
        "            ic(batch_nll)\n",
        "            ic(denoms.size())\n",
        "            ic((len(rev_sents_ids),))\n",
        "            ic(denoms.size() == (len(rev_sents_ids),))\n",
        "            raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-BbREMkrBEph"
      },
      "outputs": [],
      "source": [
        "model = Model(rb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyP8T2ASG4ZW"
      },
      "outputs": [],
      "source": [
        "%%capture output\n",
        "# %%script python --no-raise-error\n",
        "# model.train()\n",
        "model.experiments(root_dir='./saves/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.show()"
      ],
      "metadata": {
        "id": "-ByiBGGuLQF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %store output > output_log\n",
        "import pickle\n",
        "with open('output', 'wb') as f:\n",
        "    pickle.dump(output, f)"
      ],
      "metadata": {
        "id": "SYr-3QAhLRga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUPz4AgEIewN"
      },
      "outputs": [],
      "source": [
        "model.word_clouds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhS8AvGN_KIU"
      },
      "outputs": [],
      "source": [
        "indices = model.ds.vocab.lookup_indices(['soul', 'tyskie', '1/3', 'antrunk', 'nett', '<unk>'])\n",
        "# indices = model.ds.vocab.lookup_indices(['antrunk'])Napi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGr0C7WLqsu0",
        "outputId": "13fbdafa-af3f-4d07-f004-6605e672f4f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[ 6.2895e-02,  7.7352e-02,  3.8608e-01,  7.7248e-01,  4.9354e-01],\n",
              "         [ 6.1379e-03,  1.1089e-01,  1.4684e-01,  1.5803e+00, -2.0832e-01],\n",
              "         [ 3.9038e-01,  1.8007e-01,  3.8165e-01, -8.4364e-02,  5.1858e-01],\n",
              "         [ 6.5582e+00, -1.8309e+00, -2.0364e-01, -1.4185e+00, -1.1770e+00],\n",
              "         [-1.2600e-01,  1.3653e-01,  1.1414e+00,  3.4624e-01,  2.3008e-01],\n",
              "         [ 6.1379e-03,  1.1089e-01,  1.4684e-01,  1.5803e+00, -2.0832e-01]],\n",
              "        grad_fn=<IndexBackward0>)]"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model.train()\n",
        "ic(self.rev_words_thetas([self.ds.vocab.lookup_indices(['taste', 'aroma', '1/3', 'antrunk', 'nett', '<unk>'])]))\n",
        "\n",
        "# weights = torch.tensor((2.0, 1.0, 3.0, 7.0))\n",
        "# # sorted_zip = sorted(zipped, reverse=True, key=lambda x: x[1])[:50]\n",
        "# sorted(list(zip('j p g m'.split(), weights)), reverse=True, key=lambda x: x[1])[:2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sms-vI1OTGdC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIU4FlkGZExD"
      },
      "outputs": [],
      "source": [
        "itos = model.ds.vocab.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p51JARNsVe7k",
        "outputId": "92e77fef-a80b-4625-a8df-dabd0c9c9162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('appearance', 4), ('aroma', 7), ('palate', 3), ('taste', 8), ('overall', 15)]\n",
            "['<unk>', 'world', 'live', '<unk>', '2008']\n",
            "['amber', 'beer', 'good', 'head']\n",
            "['aroma', 'malty', 'rich', 'flavour', 'rich', 'vinous', 'malty']\n",
            "['overall', 'nice', 'vinous', 'rich', 'beer']\n",
            "(tensor([   0,  602, 1256,    0,  490]), tensor([27,  5, 14,  1]), tensor([  2,  28, 102,  62, 102, 474,  28]), tensor([ 86,  11, 474, 102,   5])) tensor([ 4,  7,  3,  8, 15])\n",
            "['overall', 'aroma', 'palate', 'overall']\n",
            "\n",
            "[('appearance', 3), ('aroma', 8), ('palate', 4), ('taste', 8), ('overall', 16)]\n",
            "['pours', 'light', 'amber']\n",
            "['great', 'american', 'hop', 'aroma', 'tones', 'pine', 'grapefruit', 'tangerine', 'flowers', 'apricot']\n",
            "['sweet', 'malt', 'background']\n",
            "['subtle', 'tropical', 'tones', 'present']\n",
            "['spicy', 'hop', 'flavour', 'tones', 'grapefruit', 'pine', 'apricot', 'tangerine']\n",
            "['fruity']\n",
            "['discrete', 'malty', 'background']\n",
            "['light', 'sweet', 'medium', 'bitter']\n",
            "['medium', 'dry']\n",
            "['medium', 'bodied']\n",
            "['favourite', 'ipa']\n",
            "(tensor([17,  4, 27]), tensor([ 59, 262,  33,   2, 436, 101, 115, 786, 489, 352]), tensor([  3,   7, 289]), tensor([219, 438, 436, 272]), tensor([ 79,  33,  62, 436, 115, 101, 352, 786]), tensor([42]), tensor([6819,   28,  289]), tensor([ 4,  3, 16, 22]), tensor([16, 32]), tensor([16, 61]), tensor([1559,  116])) tensor([ 3,  8,  4,  8, 16])\n",
            "['aroma', 'palate', 'taste', 'taste', 'taste', 'appearance', 'appearance', 'overall', 'overall', 'overall', 'overall']\n",
            "\n",
            "[('appearance', 3), ('aroma', 4), ('palate', 3), ('taste', 6), ('overall', 13)]\n",
            "['tap', 'flying', 'pie', 'boise']\n",
            "['yellow', 'white', 'head']\n",
            "['bit', 'carbonation', 'well']\n",
            "(tensor([  71, 1635,  704, 6189]), tensor([65,  9,  1]), tensor([24, 34, 30])) tensor([ 3,  4,  3,  6, 13])\n",
            "['appearance', 'aroma', 'overall']\n",
            "\n",
            "[('appearance', 4), ('aroma', 6), ('palate', 3), ('taste', 7), ('overall', 13)]\n",
            "['botle']\n",
            "['black', 'pour']\n",
            "['hell', '<unk>', '<unk>']\n",
            "['aorma', 'toasty', 'charred']\n",
            "['chocolate', 'notes', 'smooth', 'aroma', 'oatmeal']\n",
            "['flavor', 'fairly', 'smooth', 'chocoalte', 'malty', 'taste', 'wtih', 'tiny', 'hint', 'smoke']\n",
            "['interesting', 'spectacular']\n",
            "(tensor([5721]), tensor([56, 82]), tensor([802,   0,   0]), tensor([8217,  285, 1128]), tensor([ 26,  25,  51,   2, 630]), tensor([   6,  140,   51, 4674,   28,   13, 3235,  433,   85,  198]), tensor([ 167, 1291])) tensor([ 4,  6,  3,  7, 13])\n",
            "['appearance', 'aroma', 'taste', 'appearance', 'palate', 'appearance', 'appearance']\n",
            "\n",
            "[('appearance', 2), ('aroma', 6), ('palate', 2), ('taste', 5), ('overall', 11)]\n",
            "['bottle', 'rhythm', 'booze', 'barton']\n",
            "['jet', 'black', 'thin', 'lasting', 'beige', 'head', 'gentle', 'condition', '<unk>', 'bitter', 'coffee', 'chocolate', 'nose', 'soft', 'sweet', 'mouth', 'hints', 'chocolate', 'roasted', 'coffee', 'low', 'level', 'fruitiness', 'finish', 'mostly', 'roasted', 'fruity', 'blackberry', 'notes', 'heavy', 'coffee', 'suggestion', 'burnt', 'malt']\n",
            "(tensor([    8, 12694,   581, 18093]), tensor([1161,   56,   43,  134,  132,    1,  634, 1330,    0,   22,   45,   26,\n",
            "          31,  113,    3,  133,   99,   26,   48,   45,  191,  519,  283,   10,\n",
            "         147,   48,   42, 1657,   25,  142,   45, 2568,  194,    7])) tensor([ 2,  6,  2,  5, 11])\n",
            "['palate', 'aroma']\n",
            "\n",
            "[('appearance', 4), ('aroma', 6), ('palate', 4), ('taste', 6), ('overall', 14)]\n",
            "['bottle', 'courtesy', '<unk>']\n",
            "['aroma', 'alcohol', 'malt', 'fruitiness', 'biscuits', 'yeast']\n",
            "['flavor', 'sweet', 'somewhat', 'boozy', 'fruitiness', 'malt']\n",
            "['medium', 'body', 'average', 'carbonation']\n",
            "['nce', 'beer']\n",
            "(tensor([  8, 253,   0]), tensor([   2,   38,    7,  283, 1163,   64]), tensor([  6,   3, 156, 520, 283,   7]), tensor([ 16,  20, 112,  34]), tensor([13749,     5])) tensor([ 4,  6,  4,  6, 14])\n",
            "['palate', 'palate', 'appearance', 'overall', 'overall']\n",
            "\n",
            "[('appearance', 3), ('aroma', 4), ('palate', 3), ('taste', 4), ('overall', 8)]\n",
            "['low', 'carb', 'beer']\n",
            "['<unk>', 'nothing', 'special']\n",
            "(tensor([191, 959,   5]), tensor([  0, 146, 333])) tensor([3, 4, 3, 4, 8])\n",
            "['appearance', 'appearance']\n",
            "\n",
            "[('appearance', 2), ('aroma', 4), ('palate', 2), ('taste', 4), ('overall', 7)]\n",
            "['tasting']\n",
            "['clear', 'light', 'yellow', 'small', 'average', 'white', 'head']\n",
            "['aroma', 'light', 'malty', 'hoppy']\n",
            "['flavor', 'light', 'moderate', 'sweet', 'light', 'bitter']\n",
            "['body', 'light', 'medium', 'texture', 'watery', 'carbonation', 'soft']\n",
            "(tensor([183]), tensor([ 37,   4,  65,  47, 112,   9,   1]), tensor([ 2,  4, 28, 49]), tensor([ 6,  4, 97,  3,  4, 22]), tensor([ 20,   4,  16, 148, 119,  34, 113])) tensor([2, 4, 2, 4, 7])\n",
            "['taste', 'aroma', 'overall', 'overall', 'overall']\n",
            "\n",
            "[('appearance', 2), ('aroma', 5), ('palate', 2), ('taste', 4), ('overall', 7)]\n",
            "['looks', 'like', 'hazy', 'apple', 'cider', 'small', 'frothy', 'head']\n",
            "['funny', 'fermented', 'apple', 'aroma']\n",
            "['palate', 'heavy', 'meaty']\n",
            "['alcohol', 'notes', 'sharp']\n",
            "['tastes', 'like', 'vodka']\n",
            "['hops', 'virtually', 'undetectable', 'citric', 'like', 'bitterness', 'finish']\n",
            "['malt', 'flavors', 'difficult', 'read', 'big', 'alcohol', 'educated', 'guess', 'complex', 'sweet', 'somewhat', 'green']\n",
            "['overall', 'strong', 'beer', 'lacking', '<unk>', 'malt', 'hop', 'flavors', 'counteract', 'sharpness']\n",
            "['bought', 'bottles', 'going', 'lay', 'improves']\n",
            "(tensor([281,  21,  57, 154, 501,  47, 155,   1]), tensor([1871, 1549,  154,    2]), tensor([  50,  142, 1413]), tensor([ 38,  25, 279]), tensor([ 117,   21, 3109]), tensor([  12, 1115, 4852,  321,   21,   41,   10]), tensor([    7,    67,  1402,  1448,    72,    38, 16184,   471,   157,     3,\n",
            "          156,   301]), tensor([  86,   70,    5,  537,    0,    7,   33,   67, 9032, 1861]), tensor([ 525,  555,  240, 4209, 3072])) tensor([2, 5, 2, 4, 7])\n",
            "['aroma', 'taste', 'overall', 'overall', 'overall', 'overall', 'overall', 'overall', 'taste']\n",
            "\n",
            "[('appearance', 2), ('aroma', 4), ('palate', 3), ('taste', 5), ('overall', 10)]\n",
            "['<unk>', '<unk>', 'cask']\n",
            "['fishy', 'grains', 'aroma']\n",
            "['clear', 'medium', 'orange', 'body', 'quick', 'white', 'bubbles']\n",
            "['grainy', 'alcohol', 'flavour', 'light', 'straw', 'grass', 'notes']\n",
            "['medium', 'body']\n",
            "['meh', 'interesting']\n",
            "['olde', 'mitre', 'london', 'england']\n",
            "(tensor([  0,   0, 138]), tensor([5458,  297,    2]), tensor([ 37,  16,  36,  20, 550,   9, 254]), tensor([168,  38,  62,   4, 225, 196,  25]), tensor([16, 20]), tensor([1198,  167]), tensor([3786, 7258,  719, 1433])) tensor([ 2,  4,  3,  5, 10])\n",
            "['overall', 'palate', 'overall', 'palate', 'overall', 'palate', 'palate']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (rev_sents_ids, review_aspects_scores) in enumerate(model.test_ds):\n",
        "    if i > 9:\n",
        "        break\n",
        "    print(list(zip(model.ds.aspects, review_aspects_scores.tolist())))\n",
        "    for sen_ids in rev_sents_ids:\n",
        "        print([itos[idx] for idx in sen_ids])\n",
        "        # print(model.ds.vocab.lookup_indices(.tolist()))\n",
        "    print(rev_sents_ids, review_aspects_scores)\n",
        "    aspects = model.ided_review_forward(rev_sents_ids, review_aspects_scores)\n",
        "    print([model.ds.aspects[aspect] for aspect in aspects])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddNbPIhSXUYv"
      },
      "outputs": [],
      "source": [
        "# danger\n",
        "# model.ds.vocab.lookup_indices([0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v9gC5-yfnhE"
      },
      "source": [
        "## Word clouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0WN-C-ofrlQ",
        "outputId": "b4953b26-c616-47b7-f584-27d4e03b9c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVSk4pncPJqO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "1e100ibu.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}