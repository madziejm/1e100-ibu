{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madziejm/1e100-ibu/blob/master/1e100ibu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMhH5l9MeZho"
      },
      "source": [
        "## Preliminary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT48xPyXJHy9"
      },
      "source": [
        "#### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl9RJDTxJOTY",
        "outputId": "41d85b63-99a0-4489-aada-d6999f3d9a53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev = cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'dev = {dev}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06lrti-Nt8g-"
      },
      "outputs": [],
      "source": [
        "try: # mount user's Google Drive if on Colab to save training artifacts\n",
        "    from google.colab import drive\n",
        "    drive.mount('/drive')\n",
        "    ROOT_DIR = './'\n",
        "    MODEL_ROOT_DIR = '/drive/MyDrive/Colab Notebooks/1e100ibu/saves/'\n",
        "except ImportError:\n",
        "    ROOT_DIR = '/content/'\n",
        "    MODEL_ROOT_DIR = './saves/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yet9JpBt8kcx"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet icecream\n",
        "from icecream import ic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at8Z-ETyKngp"
      },
      "outputs": [],
      "source": [
        "# !pip install --quiet -Iv torch==1.10.1\n",
        "# !pip install --quiet -Iv torchtext==0.11.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcpSzAJmFV56"
      },
      "source": [
        "## Dataset representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ8cumFHEEQd",
        "outputId": "4a429d02-ac44-423d-ecf1-723d31990873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting pl-core-news-md==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_md-3.2.0/pl_core_news_md-3.2.0-py3-none-any.whl (87.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 87.9 MB 21 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pl-core-news-md==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->pl-core-news-md==3.2.0) (2.0.1)\n",
            "Installing collected packages: pl-core-news-md\n",
            "Successfully installed pl-core-news-md-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pl_core_news_md')\n"
          ]
        }
      ],
      "source": [
        "!pip install 'spacy<3.3.0,>=3.2.0' --quiet\n",
        "!python -m spacy download en_core_web_sm --quiet\n",
        "!python -m spacy download pl_core_news_md --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvQgSM-fB7YP",
        "outputId": "4f453179-8364-4e55-a02b-515f1b271e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Version: 3.2.1\n",
            "Version: 1.10.0+cu111\n",
            "Version: 0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip show spacy | egrep Version\n",
        "# we want SpaCy 3\n",
        "!pip show torch | egrep Version\n",
        "!pip show torchtext | egrep Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yZeL2NQcSjP"
      },
      "source": [
        "#### dataset representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1MCM0dWB93e"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from torchtext._torchtext import (Vocab as VocabPybind) # make use of some hidden interface\n",
        "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import gc # garbage collector interface\n",
        "import io\n",
        "import re\n",
        "import spacy # nlp toolkit\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "\n",
        "class BaseReviews(torch.utils.data.Dataset):\n",
        "    def __init__(self, aspects, aspect_max, aspect_ratings, texts, unkn_tok, _len, anchor_words):\n",
        "        self.aspects = aspects\n",
        "        self.aspect_count = len(aspects)\n",
        "        self.aspect_max = aspect_max\n",
        "        self._aspect_ratings = aspect_ratings\n",
        "        self._texts = texts\n",
        "        self.unkn_tok = unkn_tok\n",
        "        self._len = _len\n",
        "        self.anchor_words = anchor_words\n",
        "        self.vocab = None\n",
        "\n",
        "    def dump(self, dest_path, filename):\n",
        "        contents = {\n",
        "            'aspects'        : self.aspects,\n",
        "            'aspect_max'     : self.aspect_max,\n",
        "            '_aspect_ratings': self._aspect_ratings,\n",
        "            '_texts'         : self._texts,\n",
        "            'unkn_tok'       : self.unkn_tok,\n",
        "            '_len'           : self._len,\n",
        "            'anchor_words'   : self.anchor_words,\n",
        "            'vocab'          : self.vocab,\n",
        "        }\n",
        "        with open(f'{dest_path}/{filename}', 'wb') as f:\n",
        "            pickle.dump(contents, f)\n",
        "    \n",
        "    def load(self, dest_path, filename):\n",
        "        with open(f'{dest_path}/{filename}', 'rb') as f:\n",
        "            contents = pickle.load(f)\n",
        "            self.aspects        = contents['aspects']\n",
        "            self.aspect_max     = contents['aspect_max']\n",
        "            self._aspect_ratings = contents['_aspect_ratings']\n",
        "            self._texts          = contents['_texts']\n",
        "            self.unkn_tok       = contents['unkn_tok']\n",
        "            self._len           = contents['_len']\n",
        "            self.anchor_words   = contents['anchor_words']\n",
        "            self.vocab          = contents['vocab']\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # # 1 # python\n",
        "        # sentences = tuple(sent for sent in self._texts[i])\n",
        "        # ratings = tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count))\n",
        "        # 2 # tensor\n",
        "        sentences = tuple(torch.LongTensor(sent) for sent in self._texts[i])\n",
        "        ratings = torch.LongTensor(tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count)))\n",
        "        # # 3 # dev\n",
        "        # sentences = tuple(torch.tensor(sent) for sent in self._texts[i])\n",
        "        # ratings = torch.tensor(tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count)))\n",
        "        return (sentences, ratings)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjfMvDnUt8hF"
      },
      "outputs": [],
      "source": [
        "class RateBeerReviews(BaseReviews):\n",
        "    \"\"\"\n",
        "    beer/name: John Harvards Simcoe IPA\n",
        "    beer/beerId: 63836\n",
        "    beer/brewerId: 8481\n",
        "    beer/ABV: 5.4\n",
        "    beer/style: India Pale Ale &#40;IPA&#41;\n",
        "    review/appearance: 4/5\n",
        "    review/aroma: 6/10\n",
        "    review/palate: 3/5\n",
        "    review/taste: 6/10\n",
        "    review/overall: 13/20\n",
        "    review/time: 1157587200\n",
        "    review/profileName: hopdog\n",
        "    review/text: On tap at the Springfield, PA location. Poured a deep and cloudy orange (almost a copper) color with a small sized off white head. Aromas or oranges and all around citric. Tastes of oranges, light caramel and a very light grapefruit finish. I too would not believe the 80+ IBUs - I found this one to have a very light bitterness with a medium sweetness to it. Light lacing left on the glass.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        aspects = ['appearance', 'aroma', 'palate', 'taste', 'overall']\n",
        "        super().__init__(\n",
        "            aspects        = aspects,\n",
        "            aspect_max     = [5, 10, 5, 10, 20],\n",
        "            aspect_ratings = [ [] for _ in aspects ],\n",
        "            texts          = [],\n",
        "            unkn_tok       = '<unk>', # unknown/out of vocabulary token\n",
        "            _len            = 0,\n",
        "            anchor_words = {\n",
        "                'appearance' : ('appearance', 'color'),\n",
        "                'aroma'      : ('aroma'),\n",
        "                'palate'     : ('palate', 'mouthfeel'),\n",
        "                'taste'      : ('taste'),\n",
        "                'overall'    : ('overall'),\n",
        "            },\n",
        "        )\n",
        "        self.pipe = None\n",
        "\n",
        "    def build(self, filepath=f'{ROOT_DIR}/SNAP-Ratebeer.txt', max_reviews=float('inf'), min_word_freq=None, max_word_count=None):\n",
        "        with io.open(filepath, encoding='utf-8') as f:\n",
        "            for line in tqdm(f, total=(40938282 if max_reviews == float('inf') else max_reviews * 14), desc='Reading data'):\n",
        "                if line == '\\n': # separator\n",
        "                    self._len += 1\n",
        "                    if max_reviews <= self._len:\n",
        "                        break\n",
        "                elif line.startswith('review/appearance: '):\n",
        "                    line = line[len('review/appearance: '):]\n",
        "                    self._aspect_ratings[0].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/aroma: '):\n",
        "                    line = line[len('review/aroma: '):]\n",
        "                    self._aspect_ratings[1].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/palate: '):\n",
        "                    line = line[len('review/palate: '):]\n",
        "                    self._aspect_ratings[2].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/taste: '):\n",
        "                    line = line[len('review/taste: '):]\n",
        "                    self._aspect_ratings[3].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/overall: '):\n",
        "                    line = line[len('review/overall: '):]\n",
        "                    self._aspect_ratings[4].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/text: '):\n",
        "                    line = line[len('review/text: '):]\n",
        "                    if line.startswith('UPDATED:'):\n",
        "                        line = line[len(\"UPDATED: APR 29, 2008\"):] # drop prefix\n",
        "                    line = re.sub('~', ' ', line.strip()) # remove whitespace incl. trailing newline and tildes that can be found in data for some reason\n",
        "                    if line:\n",
        "                        self._texts.append(line)\n",
        "                    else: # some reviews do not have associated text; unwind (remove) their ratings for each aspect\n",
        "                        for aspect_ratings in self._aspect_ratings:\n",
        "                            aspect_ratings.pop()\n",
        "                        self._len -= 1\n",
        "        self._post_process(min_word_freq, max_word_count) # 20K words should be okay\n",
        "    \n",
        "    def _fetch_nlp_pipeline(self):\n",
        "        if not self.pipe:\n",
        "            nlp = spacy.util.get_lang_class('en')()\n",
        "            nlp.add_pipe(\"sentencizer\", config={\"punct_chars\": ['.', '?', '!']})\n",
        "            nlp.Defaults.stop_words |= { '-', '+'}\n",
        "            nlp.Defaults.stop_words -= {'mostly', 'whole', 'indeed', 'quite', 'ever', 'nothing', 'perhaps', 'not', 'no', 'only', 'well', 'really', 'except'}\n",
        "            self.pipe = lambda reviews: nlp.pipe(reviews)\n",
        "    \n",
        "    def _free_nlp_pipeline(self):\n",
        "        self.nlp = None\n",
        "\n",
        "    def tokenize_reviews(self, reviews_texts: str):\n",
        "        return [tuple(list(tok.lower_ for tok in sent if not tok.is_stop and not tok.is_punct and not tok.is_space and len(tok) > 2) for sent in doc.sents if 0 != len(sent)) for doc in self.pipe(reviews_texts)]\n",
        "    \n",
        "    def id_map_reviews(self, texts):\n",
        "        return [tuple(self.vocab.lookup_indices(sent) for sent in text) for text in texts]\n",
        "    \n",
        "    def _post_process(self, min_word_freq=None, max_word_count=None):\n",
        "        assert (min_word_freq is not None) ^ bool(max_word_count is not None), \"provide one of min_word_freq and max_word_count\"\n",
        "        self._fetch_nlp_pipeline()\n",
        "        print(\"Spacy pipe (tokenization&sentence split)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = self.tokenize_reviews(self._texts)\n",
        "        for i, text in enumerate(self._texts):\n",
        "            assert 0 != len(text) # make sure no empty reviews again (new could be introduced by removing stop words unfortunately)\n",
        "        print(\"Building vocab (word-id mapping)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        sent_gen = (sent for text in self._texts for sent in text)\n",
        "        if min_word_freq:\n",
        "            self.vocab = build_vocab_from_iterator(sent_gen, specials=[self.unkn_tok], min_word_freq=5)\n",
        "        else:\n",
        "            words = Counter()\n",
        "            for tokens in sent_gen:\n",
        "                words.update(tokens)\n",
        "            words = [word for word, freq in words.most_common(max_word_count)] # list sorted by frequency yikees\n",
        "            self.vocab = Vocab(VocabPybind(words, None))\n",
        "        self.vocab.insert_token(self.unkn_tok, 0)\n",
        "        self.vocab.set_default_index(self.vocab[self.unkn_tok]) # set index for out-of-vocabulary words\n",
        "        print(\"Mapping words to ids..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = self.id_map_reviews(self._texts)\n",
        "        gc.collect() # force garbage collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy1qrIH1CAlx"
      },
      "source": [
        "If you want to read dataset from dataset file, set USE_RATEBEER_PICKLE to true in the cell below and RECREATE_PICKLE to True. If you left them untouched, it'lle be read from serialized `RateBeerReviews` class object instead of parsing text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WjAdZVWt8hH",
        "outputId": "61079331-4a47-4420-80e2-2d94091adfef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: USE_RATEBEER_PICKLE=false\n"
          ]
        }
      ],
      "source": [
        "%env USE_RATEBEER_PICKLE=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_QTo-xBCelp"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "if [ \"$USE_RATEBEER_PICKLE\" = true ]\n",
        "then # download pickle\n",
        "    if [ ! -f './ratebeer-20K-vocab.pickle' ]\n",
        "    then\n",
        "        gdown --id '1VBDjyR4jpzAgzcDUGNQFguOfLC3rtOV_' # https://drive.google.com/file/d/1VBDjyR4jpzAgzcDUGNQFguOfLC3rtOV_/view?usp=sharing  # 20K words dataset\n",
        "        # gdown --id '1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j' # https://drive.google.com/file/d/1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j/view?usp=sharing # min frequency 5 words dataset\n",
        "    fi\n",
        "else # download original dataset\n",
        "    if [ ! -f './SNAP-Ratebeer.txt' ]\n",
        "    then\n",
        "        gdown --id '12tEEYQcHZtg5aWyfIiWWVIDAJNT-5d_T' # https://drive.google.com/file/d/12tEEYQcHZtg5aWyfIiWWVIDAJNT-5d_T/view?usp=sharing\n",
        "        echo \"Dataset head (trailing newline makes entry end): \"\n",
        "        head -n 16 $RATEBEER_FILE\n",
        "        iconv -f ISO-8859-1 -t UTF-8 $RATEBEER_FILE -o {RATEBEER_FILE}.new && mv {RATEBEER_FILE}.new $RATEBEER_FILE\n",
        "    fi\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "ddf7c63fc6d74718bbffef0133de1393"
          ]
        },
        "id": "N5zmSYDBCg4B",
        "outputId": "255da3aa-811e-4925-e340-7ee0f8b17543"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddf7c63fc6d74718bbffef0133de1393",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Reading data:   0%|          | 0/40938282 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spacy pipe (tokenization&sentence split)..\n",
            "Building vocab (word-id mapping)..\n",
            "Mapping words to ids..\n",
            "Dumping..\n"
          ]
        }
      ],
      "source": [
        "rb = RateBeerReviews()\n",
        "\n",
        "if os.environ.get('USE_RATEBEER_PICKLE') == 'true':\n",
        "    rb.load('./', 'ratebeer-20K-vocab.pickle')\n",
        "else: # build pickle\n",
        "    rb.build('./SNAP-Ratebeer.txt', max_word_count=20000)\n",
        "    print('Dumping..')\n",
        "    rb.dump('./', 'ratebeer-20K-vocab.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCLRTvJYV2So"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from torchtext._torchtext import (Vocab as VocabPybind) # make use of some hidden interface\n",
        "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import gc # garbage collector interface\n",
        "import io\n",
        "import re\n",
        "import spacy # nlp toolkit\n",
        "import torch\n",
        "import json\n",
        "\n",
        "class OcenPiwoReviews(torch.utils.data.Dataset):\n",
        "    def __init__(self, filepath='/content/ocen-piwo-utf8.json', reviews_max=float('inf')):\n",
        "        self.aspects = ['ogólny', 'smak', 'zapach', 'wygląd',]\n",
        "        self.aspect_count = len(self.aspects)\n",
        "        self.aspect_max = [10, 10, 10, 10]\n",
        "        self._aspect_ratings = [ [] for _ in self.aspects ]\n",
        "        self._texts = []\n",
        "        self.unkn_tok = \"<unk>\" # unknown/out of vocabulary token\n",
        "        self._len = 0\n",
        "        self._fetch_data(filepath, reviews_max)\n",
        "        self._post_process(max_word_count=20000) # 20K words should be okay\n",
        "\n",
        "    def _fetch_data(self, filepath, reviews_max):\n",
        "        with io.open(filepath, encoding='utf-8') as f:\n",
        "            json_dict = json.loads(f.read())\n",
        "\n",
        "            for i, reviews in enumerate(json_dict.values()):\n",
        "                for sentences, ratings in reviews:\n",
        "                    self._len += 1\n",
        "\n",
        "                    for aspect in range(self.aspect_count):\n",
        "                        self._aspect_ratings[aspect].append(ratings[aspect])\n",
        "\n",
        "                    self._texts.append(sentences)\n",
        "\n",
        "    def _post_process(self, min_word_freq=None, max_word_count=None):\n",
        "        nlp = spacy.load('pl_core_news_md')\n",
        "        nlp.add_pipe(\"sentencizer\", config={\"punct_chars\": ['.', '?', '!']})\n",
        "        nlp.Defaults.stop_words |= { '-', '+', }\n",
        "        print(\"Spacy pipe (tokenization&sentence split)..\")\n",
        "\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = [tuple(list(tok.lower_ for tok in sent if not tok.is_stop and not tok.is_punct and not tok.is_space and len(tok) > 2) \n",
        "            for sent in doc.sents if 0 != len(sent)) for doc in nlp.pipe(self._texts)]\n",
        "\n",
        "        for i, text in enumerate(self._texts):\n",
        "            assert 0 != len(text) # make sure no empty reviews again (new could be introduced by removing stop words unfortunately)\n",
        "\n",
        "        print(\"Building vocab (word-id mapping)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        sent_gen = (sent for text in self._texts for sent in text)\n",
        "\n",
        "        if min_word_freq:\n",
        "            self.vocab = build_vocab_from_iterator(sent_gen, specials=[self.unkn_tok], min_word_freq=5)\n",
        "        else:\n",
        "            words = Counter()\n",
        "            for tokens in sent_gen:\n",
        "                words.update(tokens)\n",
        "            words = [word for word, freq in words.most_common(max_word_count)] # list sorted by frequency yikees\n",
        "            self.vocab = Vocab(VocabPybind(words, None))\n",
        "        self.vocab.insert_token(self.unkn_tok, 0)\n",
        "        self.vocab.set_default_index(self.vocab[self.unkn_tok]) # set index for out-of-vocabulary words\n",
        "        print(\"Mapping words to ids..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = [tuple(self.vocab.lookup_indices(sent) for sent in text) for text in self._texts]\n",
        "        gc.collect() # force garbage collection\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # # 1 # python\n",
        "        # sentences = tuple(sent for sent in self._texts[i])\n",
        "        # ratings = tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count))\n",
        "        # 2 # tensor\n",
        "        sentences = tuple(torch.LongTensor(sent) for sent in self._texts[i])\n",
        "        ratings = torch.LongTensor(tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count)))\n",
        "        # # 3 # dev\n",
        "        # sentences = tuple(torch.tensor(sent) for sent in self._texts[i])\n",
        "        # ratings = torch.tensor(tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count)))\n",
        "        return (sentences, ratings)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzSnHS8OWhD7",
        "outputId": "a5822079-fc1d-4e69-935c-22acbb7907b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RM_Sk8QeOQnjnje0gwxQfJIOIK0KLLWV\n",
            "To: /content/ocen-piwo-utf8.json\n",
            "100% 29.5M/29.5M [00:00<00:00, 71.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id '1RM_Sk8QeOQnjnje0gwxQfJIOIK0KLLWV'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSGWhTLvWXwq",
        "outputId": "d42c8d60-bab3-4597-d04b-7cd55bd701a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spacy pipe (tokenization&sentence split)..\n",
            "Building vocab (word-id mapping)..\n",
            "Mapping words to ids..\n"
          ]
        }
      ],
      "source": [
        "op = OcenPiwoReviews()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mRsWfMxrst_",
        "outputId": "02dfc800-c89d-48b8-b356-f83fd79fda70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dumping..\n"
          ]
        }
      ],
      "source": [
        "DATASET_PICKLE='/content/ocenpiwo-20K-vocab.pickle'\n",
        "\n",
        "with open(DATASET_PICKLE, 'wb') as f:\n",
        "    print('Dumping..')\n",
        "    torch.save(op, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOLgrQqMt8hL"
      },
      "source": [
        "If you want to read dataset from dataset file, set FETCH_RATEBEER to true in the cell below and RECREATE_PICKLE to True. If you left them untouched, it'lle be read from serialized `RateBeerReviews` class object instead of parsing text file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOc_2b-E4o0c"
      },
      "source": [
        "### Training (implementation of $(1)$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05JPg4ATQTOW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "import datetime\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from more_itertools import grouper\n",
        "\n",
        "# for wordcloudsdest_path=dest_path\n",
        "import functools\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from PIL import Image\n",
        "from os import path\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import functools\n",
        "from operator import iadd\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, dataset):\n",
        "        self.ds = dataset\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        word_count = len(self.ds.vocab.get_itos())\n",
        "        self.theta = torch.rand((word_count, self.ds.aspect_count), device=dev)\n",
        "        # scale to [-0.1, 0.0], as we enforce this weight to 1.0 for some words later on\n",
        "        self.theta = self.theta * -0.1\n",
        "        # scale to [0.0, 0.9], as we enforce this weight to 1.0 for some words later on\n",
        "        # self.theta = self.theta * 0.9\n",
        "        # enforce 1 initialization on aspect name (page 4)\n",
        "        # aspect_ids = self.ds.vocab.lookup_indices(self.ds.aspects)\n",
        "        for aspect_idx, aspect in enumerate(self.ds.aspects):\n",
        "            words = self.ds.anchor_words[aspect]\n",
        "\n",
        "            if isinstance(words, str):\n",
        "                words = [words]\n",
        "            else:\n",
        "                words = list(words)\n",
        "\n",
        "            words_ids = self.ds.vocab.lookup_indices(words)\n",
        "            for word_id in words_ids:\n",
        "                self.theta[word_id, aspect_idx] = 1.0\n",
        "        self.theta.requires_grad_()\n",
        "\n",
        "        # introduce separate phi for each aspect\n",
        "        # self.phis = [torch.rand((word_count, self.ds.aspect_max[i])).to(dev) for i in range(self.ds.aspect_count)]\n",
        "        self.phis = [torch.zeros((word_count, self.ds.aspect_max[i]), device=dev, dtype=self.theta.dtype) for i in range(self.ds.aspect_count)]\n",
        "        # # normalize that sum across all words is 1 for a given aspect (eq. 7) # do not normalize as for now\n",
        "        # self.phis = [phi / phi.sum(dim=0) for phi in self.phis]\n",
        "        for phi in self.phis: phi.requires_grad_()\n",
        "    \n",
        "    def word_clouds(self, dest_path='/drive/MyDrive/Colab Notebooks/1e100ibu/saves/', filename='words.png', show=True):\n",
        "        words = self.ds.vocab.get_itos()\n",
        "        fig = plt.figure(figsize=(21, 5))\n",
        "        plt.subplots_adjust(wspace=0.2)\n",
        "        # plt.subplots_adjust(wspace=0.01, hspace=0.000000001)\n",
        "        # plt.tight_layout()\n",
        "        i = 0\n",
        "\n",
        "        for aspect in range(self.ds.aspect_count):\n",
        "            # print()\n",
        "            aspect_name = self.ds.aspects[aspect]\n",
        "            # print(aspect_name)\n",
        "            atheta = self.theta[:, aspect].tolist()\n",
        "            \n",
        "            zipped = list(zip(words, atheta))\n",
        "            # ic(list(filter(lambda x: x[0] == 'antrunk', zipped)))\n",
        "            # sorted_zip = sorted(zipped, reverse=True, key=lambda x: x[1])[:50]\n",
        "            # print(*[(word, '%s' % float('%.3g' % weight)) for (word, weight) in sorted_zip])\n",
        "\n",
        "            wc = WordCloud(background_color=\"white\", color_func=lambda *args, **kwargs: 'black', width=1000, height=1000)\n",
        "            wc.generate_from_frequencies(dict(zipped))\n",
        "\n",
        "            fig.add_subplot(self.ds.aspect_count, max(self.ds.aspect_max) + 1, i * (max(self.ds.aspect_max) + 1) + 1)\n",
        "            plt.imshow(wc)\n",
        "            plt.title(f'{aspect_name} Theta', fontsize=2)\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            for rating in range(self.ds.aspect_max[aspect]):\n",
        "                aphi = self.phis[aspect][:, rating].tolist()\n",
        "\n",
        "                zipped = list(zip(words, aphi))\n",
        "                sorted_zip = sorted(zipped, reverse=True, key=lambda x: x[1])[:200]\n",
        "\n",
        "                if any(a == 0 for _, a in sorted_zip):\n",
        "                    print(f'Omitting {aspect_name} Phi {str(rating + 1)}')\n",
        "                    continue\n",
        "\n",
        "                wc = WordCloud(background_color=\"white\", color_func=lambda *args, **kwargs: 'black', width=1000, height=1000)\n",
        "                wc.generate_from_frequencies(dict(zipped))\n",
        "                fig.add_subplot(self.ds.aspect_count, max(self.ds.aspect_max) + 1, i * (max(self.ds.aspect_max) + 1) + rating + 2)\n",
        "\n",
        "                plt.imshow(wc)\n",
        "                plt.title(f'{aspect_name} Phi {str(rating + 1)}', fontsize=2)\n",
        "                plt.axis(\"off\")\n",
        "\n",
        "            i += 1\n",
        "        plt.savefig(f'{dest_path}/{filename}', dpi=600)\n",
        "        plt.show(block=show)\n",
        "    \n",
        "    def rev_words_thetas(self, rev_sens_ids):\n",
        "        \"\"\"\n",
        "        TODO filename sentence_aspects_likelihood_theta\n",
        "        \"\"\"\n",
        "        return [self.theta[sen_ids] for sen_ids in rev_sens_ids]\n",
        "\n",
        "    def rev_words_phis(self, rev_sens_ids):\n",
        "        \"\"\"\n",
        "        TODO filename sentence_aspects_likelihood_phi\n",
        "        \"\"\"\n",
        "        return [[self.phis[aspect_idx][sen_ids, :] for aspect_idx in range(self.ds.aspect_count)] for sen_ids in rev_sens_ids]\n",
        "    \n",
        "    def dump_weights(self, dest_path='/drive/MyDrive/Colab Notebooks/1e100ibu/saves/', filename=''):\n",
        "        weights = {'phis': self.phis, 'theta': self.theta}\n",
        "        torch.save(weights,  f'{dest_path}/{filename}')\n",
        "\n",
        "    def load_weights(self, src_path):\n",
        "        weights = torch.load(src_path, map_location=torch.device(dev))\n",
        "        self.theta = weights['theta']\n",
        "        self.phis  = weights['phis']\n",
        "    \n",
        "    def _linear_assignement(self, costs):\n",
        "        # for nll we want to minimize\n",
        "        return linear_sum_assignment(costs, maximize=False)\n",
        "    \n",
        "    def plot_nll_history(self, dir, filename):\n",
        "        plt.figure().set_facecolor('white') # no alpha please\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('NLL')\n",
        "        plt.xlim(left=0.0, right=self.train_epoch_history[-1])\n",
        "        plt.ylim(bottom=min(0.0, min(self.train_nll_history)), top=max(max(self.train_nll_history), max(self.test_nll_history)))\n",
        "        plt.plot(self.train_epoch_history, self.train_nll_history, '-o')\n",
        "        plt.plot(self.test_epoch_history, self.test_nll_history, '-o')\n",
        "        plt.legend(['train batch NLL', 'test mean batch NLL'], loc='upper right')\n",
        "        plt.title('Train and test dataset NLL')\n",
        "        plt.savefig(os.path.join(dir, filename))\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def experiments(root_dir='/drive/MyDrive/Colab Notebooks/1e100ibu/saves/'):\n",
        "        for _ in range(1):\n",
        "            directory = str(datetime.datetime.now()) + '-adam'\n",
        "            path = os.path.join(root_dir, directory)\n",
        "            os.mkdir(path)\n",
        "\n",
        "            model = Model(rb)\n",
        "            model.train(dest_path=path, optim='adam')\n",
        "\n",
        "        for _ in range(2):\n",
        "            directory = str(datetime.datetime.now())\n",
        "            path = os.path.join(root_dir, directory)\n",
        "            os.mkdir(path)\n",
        "            \n",
        "            model = Model(rb)\n",
        "            model.train(dest_path=path)\n",
        "\n",
        "    def test_nll(self, debug=False):\n",
        "        nlls = []\n",
        "        with torch.no_grad():\n",
        "            test_loader = DataLoader(self.test_ds, batch_size=100, collate_fn=lambda x: x) # do not use default collate function as it requires fixed-length input and raises this exception otherwise https://github.com/pytorch/pytorch/issues/42654\n",
        "            for i, batch in enumerate(tqdm(test_loader, desc='test validation')):\n",
        "                batch_nlls = []\n",
        "                for (rev_sents_ids, review_aspects_scores) in batch:\n",
        "                    rev_thetas = self.rev_words_thetas(rev_sents_ids)\n",
        "                    rev_phis   = self.rev_words_phis(rev_sents_ids)\n",
        "                    res_sents_scores = torch.stack(\n",
        "                        [\n",
        "                        rev_thetas[j].sum(dim=0) + torch.stack(tuple(rev_phis[j][a][:, review_aspects_scores[a] - 1].sum() for a in range(self.ds.aspect_count))) # 1 x aspect count\n",
        "                        for j in range(len(rev_sents_ids))\n",
        "                        ],\n",
        "                    ) # sent count x aspect count\n",
        "\n",
        "                    denoms = torch.logsumexp(res_sents_scores, dim=1)[:, None]\n",
        "                    assert(denoms.shape == (len(rev_sents_ids), 1))\n",
        "\n",
        "                    res_sents_scores = -res_sents_scores + denoms\n",
        "\n",
        "                    sents_aspect_preds_max = torch.argmin(res_sents_scores, dim=1)\n",
        "                    row_ind, col_ind = self._linear_assignement(costs=res_sents_scores.detach().cpu().numpy())\n",
        "                    sents_aspect_preds_linear = sents_aspect_preds_max\n",
        "\n",
        "                    sents_aspect_preds_linear[row_ind] = torch.from_numpy(col_ind).to(dev)\n",
        "                    \n",
        "                    batch_nlls.append(res_sents_scores.take_along_dim(sents_aspect_preds_linear[:, None], dim=1).sum().cpu().item())\n",
        "                nlls.append(functools.reduce(lambda a, b: a + b, batch_nlls))\n",
        "        nll_sum = functools.reduce(lambda a, b: a + b, nlls)\n",
        "        if debug: ic(nll_sum)\n",
        "        if debug: ic(torch.exp(-torch.tensor(nll_sum)))\n",
        "        mean_batch_nll = nll_sum / len(nlls)\n",
        "        return mean_batch_nll\n",
        "\n",
        "\n",
        "    def train(self, dest_path='/drive/MyDrive/Colab Notebooks/1e100ibu/saves/', epoch_count=1, optim='sgd'):\n",
        "        train_size = int(0.8 * len(self.ds))\n",
        "        test_size = len(self.ds) - train_size\n",
        "\n",
        "        params = (\n",
        "            self.theta,\n",
        "            *self.phis\n",
        "        )\n",
        "        lr = 0.0005\n",
        "        # lr = sum(self.ds.aspect_max) * 0.01 / train_size # for whole batch iteration\n",
        "\n",
        "        weight_decay = 0.0001\n",
        "        momentum = 0.1\n",
        "        if optim == 'sgd':\n",
        "            self._optim = torch.optim.SGD(\n",
        "                params=params,\n",
        "                lr=lr,\n",
        "                weight_decay=weight_decay,\n",
        "                momentum=momentum\n",
        "            )\n",
        "        elif optim == 'adam': # do not use Adam, weights go to NaN with it for some reason\n",
        "            self._optim = torch.optim.Adam(\n",
        "                params=params,\n",
        "                lr=lr,\n",
        "                weight_decay=weight_decay,\n",
        "                betas=(momentum, 0.999) # the first is params momentum, second RMSProp momentum (for now fix to 0.999 which is default Pytorch value)\n",
        "            )\n",
        "        else:\n",
        "            assert False\n",
        "        self._sched = torch.optim.lr_scheduler.StepLR(self._optim, step_size=1, gamma=0.95)\n",
        "\n",
        "\n",
        "        self.train_ds, self.test_ds = random_split(self.ds, [train_size, test_size])\n",
        "\n",
        "        train_loader = DataLoader(self.train_ds, batch_size=100, shuffle=True, collate_fn=lambda x: x) # do not use default collate function as it requires fixed-length input and raises this exception otherwise https://github.com/pytorch/pytorch/issues/42654\n",
        "        self.train_epoch_history = []\n",
        "        self.test_epoch_history = []\n",
        "        self.train_nll_history = []\n",
        "        self.test_nll_history = []\n",
        "        try:\n",
        "            for epoch in range(epoch_count):\n",
        "                batch_count = len(train_loader)\n",
        "                for i, batch in enumerate(tqdm(train_loader, desc=f'train epoch {epoch}/{epoch_count}')):\n",
        "                    batch_nlls = []\n",
        "                    for (rev_sents_ids, review_aspects_scores) in batch:\n",
        "                        # for sent_ids in rev_sents_ids: sent_ids.to(dev)\n",
        "                        rev_thetas = self.rev_words_thetas(rev_sents_ids)\n",
        "                        rev_phis   = self.rev_words_phis(rev_sents_ids)\n",
        "\n",
        "                        res_sents_scores = torch.stack(\n",
        "                            [\n",
        "                            rev_thetas[j].sum(dim=0) + torch.stack(tuple(rev_phis[j][a][:, review_aspects_scores[a] - 1].sum() for a in range(self.ds.aspect_count))) # 1 x aspect count\n",
        "                            for j in range(len(rev_sents_ids))\n",
        "                            ],\n",
        "                        ) # sent count x aspect count\n",
        "\n",
        "                        denoms = torch.logsumexp(res_sents_scores, dim=1)[:, None]\n",
        "                        assert(denoms.shape == (len(rev_sents_ids), 1))\n",
        "\n",
        "                        res_sents_scores = -res_sents_scores + denoms\n",
        "\n",
        "                        sents_aspect_preds_max = torch.argmin(res_sents_scores, dim=1)\n",
        "                        row_ind, col_ind = self._linear_assignement(costs=res_sents_scores.detach().cpu().numpy())\n",
        "                        sents_aspect_preds_linear = sents_aspect_preds_max\n",
        "\n",
        "                        # (most likely) aspect assignments (5)\n",
        "                        sents_aspect_preds_linear[row_ind] = torch.from_numpy(col_ind).to(dev)\n",
        "                        \n",
        "                        # sentence likelihood (6)\n",
        "                        batch_nlls.append(res_sents_scores.take_along_dim(sents_aspect_preds_linear[:, None], dim=1).sum())\n",
        "\n",
        "                    batch_nll = torch.stack(batch_nlls).sum()\n",
        "                    self._optim.zero_grad(set_to_none=True)\n",
        "                    if 0 == i % 100:\n",
        "                        self.train_nll_history.append(batch_nll.cpu().detach().item())\n",
        "                        self.train_epoch_history.append(epoch + i / batch_count)\n",
        "                    if 0 == i % 5000:\n",
        "                        if i != 0:\n",
        "                            self.dump_weights(dest_path=dest_path, filename=(f'-epoch-{epoch}-{epoch_count}-{int(i)}'))\n",
        "                            self.word_clouds(dest_path=dest_path, filename=(f'cloud-epoch-{epoch}-{epoch_count}-{int(i)}.png'))\n",
        "                            ic(self.rev_words_thetas([self.ds.vocab.lookup_indices(['taste', 'aroma', 'palete', 'antrunk'])]))\n",
        "                        self.test_nll_history.append(self.test_nll())\n",
        "                        self.test_epoch_history.append(epoch + i / batch_count)\n",
        "                    batch_nll.backward()\n",
        "\n",
        "                    self._optim.step()\n",
        "                self._sched.step()\n",
        "\n",
        "            self.dump_weights(dest_path=dest_path, filename=(f'weights-end'))\n",
        "            self.word_clouds(dest_path=dest_path, filename=(f'cloud-end.png'))\n",
        "            self.plot_nll_history(dest_path, (f'plot-end.png'))\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print('Interrupted.')\n",
        "\n",
        "            self.dump_weights(dest_path=dest_path, filename=(f'weights-interrupt'))\n",
        "            self.word_clouds(dest_path=dest_path, filename=(f'cloud-interrupt.png'))\n",
        "            self.plot_nll_history(dest_path, (f'plot-interrupt.png'))\n",
        "\n",
        "        except Exception as e:\n",
        "            ic(len(batch_nlls))\n",
        "            ic(denoms)\n",
        "            ic(rev_sents_ids)\n",
        "\n",
        "            self.dump_weights(dest_path=dest_path, filename=(f'weights-exception'))\n",
        "            self.word_clouds(dest_path=dest_path, filename=(f'cloud-exception.png'))\n",
        "            self.plot_nll_history(dest_path, (f'plot-exception.png'))\n",
        "\n",
        "            ic(batch_nll)\n",
        "            ic(denoms.size())\n",
        "            ic((len(rev_sents_ids),))\n",
        "            ic(denoms.size() == (len(rev_sents_ids),))\n",
        "            raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyP8T2ASG4ZW"
      },
      "outputs": [],
      "source": [
        "%%capture output\n",
        "# %%script python --no-raise-error\n",
        "# model = Model(rb)\n",
        "# model.train()\n",
        "Model.experiments(root_dir='./saves/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ByiBGGuLQF_"
      },
      "outputs": [],
      "source": [
        "output.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYr-3QAhLRga"
      },
      "outputs": [],
      "source": [
        "# %store output > output_log\n",
        "import pickle\n",
        "with open('output', 'wb') as f:\n",
        "    pickle.dump(output, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddNbPIhSXUYv"
      },
      "outputs": [],
      "source": [
        "# danger\n",
        "# model.ds.vocab.lookup_indices([0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVSk4pncPJqO"
      },
      "outputs": [],
      "source": [
        "model = Model(rb) # single run\n",
        "# model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNwOBHLQt8hN"
      },
      "outputs": [],
      "source": [
        "model.show_inference([\n",
        "    'Good-looking bootle. Aroma is pretty astonishig. Sour and sweet palate profile. I like the taste very much. Overall I can recommend it to everyone.',\n",
        "    'Tastes best from bottle. Not so heap as one could think. Nice hoppy smell. I had not supposed it will be sour though. Beautiful smooth head.',\n",
        "])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "1e100ibu.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}