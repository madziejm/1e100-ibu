{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madziejm/1e100-ibu/blob/master/1e100ibu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMhH5l9MeZho"
      },
      "source": [
        "## Preliminary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT48xPyXJHy9"
      },
      "source": [
        "#### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl9RJDTxJOTY",
        "outputId": "5b5d6462-7466-4b2c-acf2-5bb9c3a401e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev = cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'dev = {dev}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMaGx0YYuCGu",
        "outputId": "bbd124c5-19a0-4410-907f-ce52273cc217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ],
      "source": [
        "try: # mount user's Google Drive if on Colab to save training artifacts\n",
        "    from google.colab import drive\n",
        "    drive.mount('/drive')\n",
        "    ROOT_DIR = './'\n",
        "    MODEL_ROOT_DIR = '/drive/MyDrive/Colab Notebooks/1e100ibu/saves/'\n",
        "except ImportError:\n",
        "    ROOT_DIR = '/content/'\n",
        "    MODEL_ROOT_DIR = './saves/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Yet9JpBt8kcx"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet icecream\n",
        "from icecream import ic\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "at8Z-ETyKngp"
      },
      "outputs": [],
      "source": [
        "# !pip install --quiet -Iv torch==1.10.1\n",
        "# !pip install --quiet -Iv torchtext==0.11.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcpSzAJmFV56"
      },
      "source": [
        "## Dataset representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ8cumFHEEQd",
        "outputId": "2f137458-fd2b-4b7d-a27f-63c90f91f82f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 6.0 MB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 35.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 628 kB 25.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 451 kB 43.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 31.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 5.1 MB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[K     |████████████████████████████████| 87.9 MB 26 kB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pl_core_news_md')\n"
          ]
        }
      ],
      "source": [
        "!pip install 'spacy<3.3.0,>=3.2.0' --quiet\n",
        "!python -m spacy download en_core_web_sm --quiet\n",
        "!python -m spacy download pl_core_news_md --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvQgSM-fB7YP",
        "outputId": "bdf938d5-b15e-472b-be3d-d49cfe1c9fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 3.2.1\n",
            "Version: 1.10.0+cu111\n",
            "Version: 0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip show spacy | egrep Version\n",
        "# we want SpaCy 3\n",
        "!pip show torch | egrep Version\n",
        "!pip show torchtext | egrep Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yZeL2NQcSjP"
      },
      "source": [
        "#### dataset representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C1MCM0dWB93e"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from torchtext._torchtext import (Vocab as VocabPybind) # make use of some hidden interface\n",
        "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import gc # garbage collector interface\n",
        "import io\n",
        "import re\n",
        "import spacy # nlp toolkit\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "\n",
        "class BaseReviews(torch.utils.data.Dataset):\n",
        "    def __init__(self, aspects, aspect_max, aspect_ratings, texts, unkn_tok, _len, anchor_words):\n",
        "        self.aspects = aspects\n",
        "        self.aspect_count = len(aspects)\n",
        "        self.aspect_max = aspect_max\n",
        "        self._aspect_ratings = aspect_ratings\n",
        "        self._texts = texts\n",
        "        self.unkn_tok = unkn_tok\n",
        "        self._len = _len\n",
        "        self.anchor_words = anchor_words\n",
        "        self.vocab = None\n",
        "\n",
        "    def dump(self, dest_path, filename):\n",
        "        contents = {\n",
        "            'aspects'        : self.aspects,\n",
        "            'aspect_max'     : self.aspect_max,\n",
        "            '_aspect_ratings': self._aspect_ratings,\n",
        "            '_texts'         : self._texts,\n",
        "            'unkn_tok'       : self.unkn_tok,\n",
        "            '_len'           : self._len,\n",
        "            'anchor_words'   : self.anchor_words,\n",
        "            'vocab'          : self.vocab,\n",
        "        }\n",
        "        with open(f'{dest_path}/{filename}', 'wb') as f:\n",
        "            pickle.dump(contents, f)\n",
        "    \n",
        "    def load(self, dest_path, filename):\n",
        "        with open(f'{dest_path}/{filename}', 'rb') as f:\n",
        "            contents = pickle.load(f)\n",
        "            self.aspects        = contents['aspects']\n",
        "            self.aspect_max     = contents['aspect_max']\n",
        "            self._aspect_ratings = contents['_aspect_ratings']\n",
        "            self._texts          = contents['_texts']\n",
        "            self.unkn_tok       = contents['unkn_tok']\n",
        "            self._len           = contents['_len']\n",
        "            self.anchor_words   = contents['anchor_words']\n",
        "            self.vocab          = contents['vocab']\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # # 1 # python\n",
        "        # sentences = tuple(sent for sent in self._texts[i])\n",
        "        # ratings = tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count))\n",
        "        # 2 # tensor\n",
        "        sentences = tuple(torch.LongTensor(sent) for sent in self._texts[i])\n",
        "        ratings = torch.LongTensor(tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count)))\n",
        "        # # 3 # dev\n",
        "        # sentences = tuple(torch.tensor(sent) for sent in self._texts[i])\n",
        "        # ratings = torch.tensor(tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count)))\n",
        "        return (sentences, ratings)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EaRC_7rAuCG0"
      },
      "outputs": [],
      "source": [
        "class RateBeerReviews(BaseReviews):\n",
        "    \"\"\"\n",
        "    beer/name: John Harvards Simcoe IPA\n",
        "    beer/beerId: 63836\n",
        "    beer/brewerId: 8481\n",
        "    beer/ABV: 5.4\n",
        "    beer/style: India Pale Ale &#40;IPA&#41;\n",
        "    review/appearance: 4/5\n",
        "    review/aroma: 6/10\n",
        "    review/palate: 3/5\n",
        "    review/taste: 6/10\n",
        "    review/overall: 13/20\n",
        "    review/time: 1157587200\n",
        "    review/profileName: hopdog\n",
        "    review/text: On tap at the Springfield, PA location. Poured a deep and cloudy orange (almost a copper) color with a small sized off white head. Aromas or oranges and all around citric. Tastes of oranges, light caramel and a very light grapefruit finish. I too would not believe the 80+ IBUs - I found this one to have a very light bitterness with a medium sweetness to it. Light lacing left on the glass.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        aspects = ['appearance', 'aroma', 'palate', 'taste', 'overall']\n",
        "        super().__init__(\n",
        "            aspects        = aspects,\n",
        "            aspect_max     = [5, 10, 5, 10, 20],\n",
        "            aspect_ratings = [ [] for _ in aspects ],\n",
        "            texts          = [],\n",
        "            unkn_tok       = '<unk>', # unknown/out of vocabulary token\n",
        "            _len            = 0,\n",
        "            anchor_words = {\n",
        "                'appearance' : ('appearance', 'color'),\n",
        "                'aroma'      : ('aroma'),\n",
        "                'palate'     : ('palate', 'mouthfeel'),\n",
        "                'taste'      : ('taste'),\n",
        "                'overall'    : ('overall'),\n",
        "            },\n",
        "        )\n",
        "        self.pipe = None\n",
        "\n",
        "    def build(self, filepath=f'{ROOT_DIR}/SNAP-Ratebeer.txt', max_reviews=float('inf'), min_word_freq=None, max_word_count=None):\n",
        "        with io.open(filepath, encoding='utf-8') as f:\n",
        "            for line in tqdm(f, total=(40938282 if max_reviews == float('inf') else max_reviews * 14), desc='Reading data'):\n",
        "                if line == '\\n': # separator\n",
        "                    self._len += 1\n",
        "                    if max_reviews <= self._len:\n",
        "                        break\n",
        "                elif line.startswith('review/appearance: '):\n",
        "                    line = line[len('review/appearance: '):]\n",
        "                    self._aspect_ratings[0].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/aroma: '):\n",
        "                    line = line[len('review/aroma: '):]\n",
        "                    self._aspect_ratings[1].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/palate: '):\n",
        "                    line = line[len('review/palate: '):]\n",
        "                    self._aspect_ratings[2].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/taste: '):\n",
        "                    line = line[len('review/taste: '):]\n",
        "                    self._aspect_ratings[3].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/overall: '):\n",
        "                    line = line[len('review/overall: '):]\n",
        "                    self._aspect_ratings[4].append(int(line.split('/')[0])) # lhs of split by '/' is rating, rhs is max possible rating\n",
        "                elif line.startswith('review/text: '):\n",
        "                    line = line[len('review/text: '):]\n",
        "                    if line.startswith('UPDATED:'):\n",
        "                        line = line[len(\"UPDATED: APR 29, 2008\"):] # drop prefix\n",
        "                    line = re.sub('~', ' ', line.strip()) # remove whitespace incl. trailing newline and tildes that can be found in data for some reason\n",
        "                    if line:\n",
        "                        self._texts.append(line)\n",
        "                    else: # some reviews do not have associated text; unwind (remove) their ratings for each aspect\n",
        "                        for aspect_ratings in self._aspect_ratings:\n",
        "                            aspect_ratings.pop()\n",
        "                        self._len -= 1\n",
        "        self._post_process(min_word_freq, max_word_count) # 20K words should be okay\n",
        "    \n",
        "    def _fetch_nlp_pipeline(self):\n",
        "        if not self.pipe:\n",
        "            nlp = spacy.util.get_lang_class('en')()\n",
        "            nlp.add_pipe(\"sentencizer\", config={\"punct_chars\": ['.', '?', '!']})\n",
        "            nlp.Defaults.stop_words |= { '-', '+'}\n",
        "            nlp.Defaults.stop_words -= {'mostly', 'whole', 'indeed', 'quite', 'ever', 'nothing', 'perhaps', 'not', 'no', 'only', 'well', 'really', 'except'}\n",
        "            self.pipe = lambda reviews: nlp.pipe(reviews)\n",
        "    \n",
        "    def _free_nlp_pipeline(self):\n",
        "        self.nlp = None\n",
        "\n",
        "    def tokenize_reviews(self, reviews_texts: str):\n",
        "        return [tuple(list(tok.lower_ for tok in sent if not tok.is_stop and not tok.is_punct and not tok.is_space and len(tok) > 2) for sent in doc.sents if 0 != len(sent)) for doc in self.pipe(reviews_texts)]\n",
        "    \n",
        "    def id_map_reviews(self, texts):\n",
        "        return [tuple(self.vocab.lookup_indices(sent) for sent in text) for text in texts]\n",
        "    \n",
        "    def _post_process(self, min_word_freq=None, max_word_count=None):\n",
        "        assert (min_word_freq is not None) ^ bool(max_word_count is not None), \"provide one of min_word_freq and max_word_count\"\n",
        "        self._fetch_nlp_pipeline()\n",
        "        print(\"Spacy pipe (tokenization&sentence split)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = self.tokenize_reviews(self._texts)\n",
        "        for i, text in enumerate(self._texts):\n",
        "            assert 0 != len(text) # make sure no empty reviews again (new could be introduced by removing stop words unfortunately)\n",
        "        print(\"Building vocab (word-id mapping)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        sent_gen = (sent for text in self._texts for sent in text)\n",
        "        if min_word_freq:\n",
        "            self.vocab = build_vocab_from_iterator(sent_gen, specials=[self.unkn_tok], min_word_freq=5)\n",
        "        else:\n",
        "            words = Counter()\n",
        "            for tokens in sent_gen:\n",
        "                words.update(tokens)\n",
        "            words = [word for word, freq in words.most_common(max_word_count)] # list sorted by frequency yikees\n",
        "            self.vocab = Vocab(VocabPybind(words, None))\n",
        "        self.vocab.insert_token(self.unkn_tok, 0)\n",
        "        self.vocab.set_default_index(self.vocab[self.unkn_tok]) # set index for out-of-vocabulary words\n",
        "        print(\"Mapping words to ids..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = self.id_map_reviews(self._texts)\n",
        "        gc.collect() # force garbage collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy1qrIH1CAlx"
      },
      "source": [
        "If you want to read dataset from dataset file, set USE_RATEBEER_PICKLE to true in the cell below and RECREATE_PICKLE to True. If you left them untouched, it'lle be read from serialized `RateBeerReviews` class object instead of parsing text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO_xtf2XuCG3",
        "outputId": "49ebca8a-d239-4ce2-c513-68f6325ca4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: USE_RATEBEER_PICKLE=false\n"
          ]
        }
      ],
      "source": [
        "%env USE_RATEBEER_PICKLE=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_QTo-xBCelp",
        "outputId": "3b95cb07-5a2c-46e2-98ec-44a95e86d93f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is interrupted.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "if [ \"$USE_RATEBEER_PICKLE\" = true ]\n",
        "then # download pickle\n",
        "    if [ ! -f './ratebeer-20K-vocab.pickle' ]\n",
        "    then\n",
        "        gdown --id '1VBDjyR4jpzAgzcDUGNQFguOfLC3rtOV_' # https://drive.google.com/file/d/1VBDjyR4jpzAgzcDUGNQFguOfLC3rtOV_/view?usp=sharing  # 20K words dataset\n",
        "        # gdown --id '1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j' # https://drive.google.com/file/d/1ebDMDlOxtFh8B5i8lajR7q3kq-0hM02j/view?usp=sharing # min frequency 5 words dataset\n",
        "    fi\n",
        "else # download original dataset\n",
        "    if [ ! -f './SNAP-Ratebeer.txt' ]\n",
        "    then\n",
        "        gdown --id '12tEEYQcHZtg5aWyfIiWWVIDAJNT-5d_T' # https://drive.google.com/file/d/12tEEYQcHZtg5aWyfIiWWVIDAJNT-5d_T/view?usp=sharing\n",
        "        echo \"Dataset head (trailing newline makes entry end): \"\n",
        "        export $RATEBEER_FILE='./SNAP-Ratebeer.txt'\n",
        "        head -n 16 $RATEBEER_FILE\n",
        "        iconv -f ISO-8859-1 -t UTF-8 $RATEBEER_FILE -o {RATEBEER_FILE}.new && mv {RATEBEER_FILE}.new $RATEBEER_FILE\n",
        "    fi\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "N5zmSYDBCg4B",
        "outputId": "627bb5fa-b878-4bbd-aad9-260632487d76"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-dc6635aadeda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRateBeerReviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'USE_RATEBEER_PICKLE'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ratebeer-20K-vocab.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# build pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "rb = RateBeerReviews()\n",
        "\n",
        "if os.environ.get('USE_RATEBEER_PICKLE') == 'true':\n",
        "    rb.load('./', 'ratebeer-20K-vocab.pickle')\n",
        "else: # build pickle\n",
        "    rb.build('./SNAP-Ratebeer.txt', max_word_count=20000)\n",
        "    print('Dumping..')\n",
        "    rb.dump('./', 'ratebeer-20K-vocab.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PCLRTvJYV2So"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from torchtext._torchtext import (Vocab as VocabPybind) # make use of some hidden interface\n",
        "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import gc # garbage collector interface\n",
        "import io\n",
        "import re\n",
        "import spacy # nlp toolkit\n",
        "import torch\n",
        "import json\n",
        "\n",
        "class OcenPiwoReviews(BaseReviews):\n",
        "    def __init__(self):\n",
        "        aspects = ['ogólny', 'smak', 'zapach', 'wygląd',]\n",
        "        super().__init__(\n",
        "            aspects        = aspects,\n",
        "            aspect_max     = [10, 10, 10, 10],\n",
        "            aspect_ratings = [ [] for _ in aspects ],\n",
        "            texts          = [],\n",
        "            unkn_tok       = '<unk>', # unknown/out of vocabulary token\n",
        "            _len            = 0,\n",
        "            anchor_words = {\n",
        "                'ogólny'     : ('ogólnie'),\n",
        "                'smak'       : ('smak'),\n",
        "                'zapach'     : ('zapach'),\n",
        "                'wygląd'     : ('wygląd', 'wygląda')\n",
        "            },\n",
        "        )\n",
        "        self.pipe = None\n",
        "\n",
        "    def build(self, filepath=f'{ROOT_DIR}/ocen-piwo-utf8.json', min_word_freq=None, max_word_count=None):\n",
        "        with io.open(filepath, encoding='utf-8') as f:\n",
        "            json_dict = json.loads(f.read())\n",
        "\n",
        "            for i, reviews in enumerate(json_dict.values()):\n",
        "                for sentences, ratings in reviews:\n",
        "                    self._len += 1\n",
        "\n",
        "                    for aspect in range(self.aspect_count):\n",
        "                        self._aspect_ratings[aspect].append(ratings[aspect])\n",
        "\n",
        "                    self._texts.append(sentences)\n",
        "        self._post_process(min_word_freq, max_word_count)\n",
        "\n",
        "    def _fetch_nlp_pipeline(self):\n",
        "        if not self.pipe:\n",
        "            nlp = spacy.load('pl_core_news_md')\n",
        "            nlp.add_pipe(\"sentencizer\", config={\"punct_chars\": ['.', '?', '!']})\n",
        "            nlp.Defaults.stop_words |= { '-', '+'}\n",
        "            self.pipe = lambda reviews: nlp.pipe(reviews)\n",
        "    \n",
        "    def _free_nlp_pipeline(self):\n",
        "        self.nlp = None\n",
        "\n",
        "    def tokenize_reviews(self, reviews_texts: str):\n",
        "        return [tuple(list(tok.lower_ for tok in sent if not tok.is_stop and not tok.is_punct and not tok.is_space and len(tok) > 2) \\\n",
        "            for sent in doc.sents if 0 != len(sent)) for doc in self.pipe(reviews_texts)]\n",
        "    \n",
        "    def id_map_reviews(self, texts):\n",
        "        return [tuple(self.vocab.lookup_indices(sent) for sent in text) for text in texts]\n",
        "    \n",
        "    def _post_process(self, min_word_freq=None, max_word_count=None):\n",
        "        assert (min_word_freq is not None) ^ bool(max_word_count is not None), \"provide one of min_word_freq and max_word_count\"\n",
        "        self._fetch_nlp_pipeline()\n",
        "        print(\"Spacy pipe (tokenization&sentence split)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = self.tokenize_reviews(self._texts)\n",
        "        for i, text in enumerate(self._texts):\n",
        "            assert 0 != len(text) # make sure no empty reviews again (new could be introduced by removing stop words unfortunately)\n",
        "        print(\"Building vocab (word-id mapping)..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        sent_gen = (sent for text in self._texts for sent in text)\n",
        "        if min_word_freq:\n",
        "            self.vocab = build_vocab_from_iterator(sent_gen, specials=[self.unkn_tok], min_word_freq=5)\n",
        "        else:\n",
        "            words = Counter()\n",
        "            for tokens in sent_gen:\n",
        "                words.update(tokens)\n",
        "            words = [word for word, freq in words.most_common(max_word_count)] # list sorted by frequency yikees\n",
        "            self.vocab = Vocab(VocabPybind(words, None))\n",
        "        self.vocab.insert_token(self.unkn_tok, 0)\n",
        "        self.vocab.set_default_index(self.vocab[self.unkn_tok]) # set index for out-of-vocabulary words\n",
        "        print(\"Mapping words to ids..\")\n",
        "        gc.collect() # force garbage collection\n",
        "        self._texts = [tuple(self.vocab.lookup_indices(sent) for sent in text) for text in self._texts]\n",
        "        gc.collect() # force garbage collection\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # # 1 # python\n",
        "        # sentences = tuple(sent for sent in self._texts[i])\n",
        "        # ratings = tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count))\n",
        "        # 2 # tensor\n",
        "        sentences = tuple(torch.LongTensor(sent) for sent in self._texts[i])\n",
        "        ratings = torch.LongTensor(tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count)))\n",
        "        # # 3 # dev\n",
        "        # sentences = tuple(torch.tensor(sent) for sent in self._texts[i])\n",
        "        # ratings = torch.tensor(tuple(self._aspect_ratings[a][i] for a in range(self.aspect_count)))\n",
        "        return (sentences, ratings)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env USE_OCENPIWO_PICKLE=false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0YlFh34widy",
        "outputId": "a2052523-d9c8-451e-cd6f-67f3d6d0d8ef"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: USE_OCENPIWO_PICKLE=false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "if [ \"$USE_OCENPIWO_PICKLE\" = true ]\n",
        "then # download pickle\n",
        "    if [ ! -f './ocenpiwo-20K-vocab.pickle' ]\n",
        "    then\n",
        "        gdown --id '1zUXnY0UqvevNnePThTfquF6hUxxPOls9' # https://drive.google.com/file/d/1zUXnY0UqvevNnePThTfquF6hUxxPOls9/view  # 20K words dataset\n",
        "    fi\n",
        "else # download original dataset\n",
        "    if [ ! -f './ocen-piwo-utf8.json' ]\n",
        "    then\n",
        "        gdown --id '1RM_Sk8QeOQnjnje0gwxQfJIOIK0KLLWV' # https://drive.google.com/file/d/1RM_Sk8QeOQnjnje0gwxQfJIOIK0KLLWV\n",
        "    fi\n",
        "fi"
      ],
      "metadata": {
        "id": "HhGr49H6wmVp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mRsWfMxrst_",
        "outputId": "441d9b01-00de-4db6-8562-54daaea0ac1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spacy pipe (tokenization&sentence split)..\n"
          ]
        }
      ],
      "source": [
        "op = OcenPiwoReviews()\n",
        "\n",
        "if os.environ.get('USE_OCENPIWO_PICKLE') == 'true':\n",
        "    op.load('./', 'ocenpiwo-20K-vocab.pickle')\n",
        "else: # build pickle\n",
        "    op.build('./ocen-piwo-utf8.json', max_word_count=20000)\n",
        "    print('Dumping..')\n",
        "    op.dump('./', 'ocenpiwo-20K-vocab.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maiw0KGOuCG6"
      },
      "source": [
        "If you want to read dataset from dataset file, set FETCH_RATEBEER to true in the cell below and RECREATE_PICKLE to True. If you left them untouched, it'lle be read from serialized `RateBeerReviews` class object instead of parsing text file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOc_2b-E4o0c"
      },
      "source": [
        "### Training (implementation of $(1)$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05JPg4ATQTOW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "import datetime\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from more_itertools import grouper\n",
        "\n",
        "# for wordcloudsdest_path=dest_path\n",
        "import functools\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from PIL import Image\n",
        "from os import path\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import functools\n",
        "from operator import iadd\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, dataset):\n",
        "        self.ds = dataset\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        word_count = len(self.ds.vocab.get_itos())\n",
        "        self.theta = torch.rand((word_count, self.ds.aspect_count), device=dev)\n",
        "        # scale to [-0.1, 0.0], as we enforce this weight to 1.0 for some words later on\n",
        "        self.theta = self.theta * -0.1\n",
        "        # scale to [0.0, 0.9], as we enforce this weight to 1.0 for some words later on\n",
        "        # self.theta = self.theta * 0.9\n",
        "        # enforce 1 initialization on aspect name (page 4)\n",
        "        # aspect_ids = self.ds.vocab.lookup_indices(self.ds.aspects)\n",
        "        for aspect_idx, aspect in enumerate(self.ds.aspects):\n",
        "            words = self.ds.anchor_words[aspect]\n",
        "\n",
        "            if isinstance(words, str):\n",
        "                words = [words]\n",
        "            else:\n",
        "                words = list(words)\n",
        "\n",
        "            words_ids = self.ds.vocab.lookup_indices(words)\n",
        "            for word_id in words_ids:\n",
        "                self.theta[word_id, aspect_idx] = 1.0\n",
        "        self.theta.requires_grad_()\n",
        "\n",
        "        # introduce separate phi for each aspect\n",
        "        # self.phis = [torch.rand((word_count, self.ds.aspect_max[i])).to(dev) for i in range(self.ds.aspect_count)]\n",
        "        self.phis = [torch.zeros((word_count, self.ds.aspect_max[i]), device=dev, dtype=self.theta.dtype) for i in range(self.ds.aspect_count)]\n",
        "        # # normalize that sum across all words is 1 for a given aspect (eq. 7) # do not normalize as for now\n",
        "        # self.phis = [phi / phi.sum(dim=0) for phi in self.phis]\n",
        "        for phi in self.phis: phi.requires_grad_()\n",
        "    \n",
        "    def word_clouds(self, dest_path=MODEL_ROOT_DIR, filename='words.png', show=True):\n",
        "        words = self.ds.vocab.get_itos()\n",
        "        fig = plt.figure(figsize=(21, 5))\n",
        "        plt.subplots_adjust(wspace=0.2)\n",
        "        # plt.subplots_adjust(wspace=0.01, hspace=0.000000001)\n",
        "        # plt.tight_layout()\n",
        "        i = 0\n",
        "\n",
        "        for aspect in range(self.ds.aspect_count):\n",
        "            aspect_name = self.ds.aspects[aspect]\n",
        "            atheta = self.theta[:, aspect].tolist()\n",
        "            \n",
        "            zipped = list(zip(words, atheta))\n",
        "\n",
        "            wc = WordCloud(background_color=\"white\", color_func=lambda *args, **kwargs: 'black', width=1000, height=1000)\n",
        "            wc.generate_from_frequencies(dict(zipped))\n",
        "\n",
        "            fig.add_subplot(self.ds.aspect_count, max(self.ds.aspect_max) + 1, i * (max(self.ds.aspect_max) + 1) + 1)\n",
        "            plt.imshow(wc)\n",
        "            plt.title(f'{aspect_name} Theta', fontsize=2)\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            for rating in range(self.ds.aspect_max[aspect]):\n",
        "                aphi = self.phis[aspect][:, rating].tolist()\n",
        "\n",
        "                zipped = list(zip(words, aphi))\n",
        "                sorted_zip = sorted(zipped, reverse=True, key=lambda x: x[1])[:200]\n",
        "\n",
        "                if any(a == 0 for _, a in sorted_zip):\n",
        "                    print(f'Omitting {aspect_name} Phi {str(rating + 1)}')\n",
        "                    continue\n",
        "\n",
        "                wc = WordCloud(background_color=\"white\", color_func=lambda *args, **kwargs: 'black', width=1000, height=1000)\n",
        "                wc.generate_from_frequencies(dict(zipped))\n",
        "                fig.add_subplot(self.ds.aspect_count, max(self.ds.aspect_max) + 1, i * (max(self.ds.aspect_max) + 1) + rating + 2)\n",
        "\n",
        "                plt.imshow(wc)\n",
        "                plt.title(f'{aspect_name} Phi {str(rating + 1)}', fontsize=2)\n",
        "                plt.axis(\"off\")\n",
        "\n",
        "            i += 1\n",
        "        plt.savefig(f'{dest_path}/{filename}', dpi=600)\n",
        "        plt.show(block=show)\n",
        "    \n",
        "    def rev_words_thetas(self, rev_sens_ids):\n",
        "        \"\"\"\n",
        "        TODO filename sentence_aspects_likelihood_theta\n",
        "        \"\"\"\n",
        "        return [self.theta[sen_ids] for sen_ids in rev_sens_ids]\n",
        "\n",
        "    def rev_words_phis(self, rev_sens_ids):\n",
        "        \"\"\"\n",
        "        TODO filename sentence_aspects_likelihood_phi\n",
        "        \"\"\"\n",
        "        return [[self.phis[aspect_idx][sen_ids, :] for aspect_idx in range(self.ds.aspect_count)] for sen_ids in rev_sens_ids]\n",
        "    \n",
        "    def dump_weights(self, dest_path=MODEL_ROOT_DIR, filename=''):\n",
        "        weights = {'phis': self.phis, 'theta': self.theta}\n",
        "        torch.save(weights,  f'{dest_path}/{filename}')\n",
        "\n",
        "    def load_weights(self, src_path):\n",
        "        weights = torch.load(src_path, map_location=torch.device(dev))\n",
        "        self.theta = weights['theta']\n",
        "        self.phis  = weights['phis']\n",
        "    \n",
        "    def _linear_assignement(self, costs):\n",
        "        # for nll we want to minimize\n",
        "        return linear_sum_assignment(costs, maximize=False)\n",
        "    \n",
        "    def plot_nll_history(self, dir, filename):\n",
        "        plt.figure().set_facecolor('white') # no alpha please\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('NLL')\n",
        "        plt.xlim(left=0.0, right=self.train_epoch_history[-1])\n",
        "        plt.ylim(bottom=min(0.0, min(self.train_nll_history)), top=max(max(self.train_nll_history), max(self.test_nll_history)))\n",
        "        plt.plot(self.train_epoch_history, self.train_nll_history, '-o')\n",
        "        plt.plot(self.test_epoch_history, self.test_nll_history, '-o')\n",
        "        plt.legend(['train batch NLL', 'test mean batch NLL'], loc='upper right')\n",
        "        plt.title('Train and test dataset NLL')\n",
        "        plt.savefig(os.path.join(dir, filename))\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def experiments(root_dir=MODEL_ROOT_DIR):\n",
        "        for _ in range(1):\n",
        "            directory = str(datetime.datetime.now()) + '-adam'\n",
        "            path = os.path.join(root_dir, directory)\n",
        "            os.mkdir(path)\n",
        "\n",
        "            model = Model(rb)\n",
        "            model.train(dest_path=path, optim='adam')\n",
        "\n",
        "        for _ in range(2):\n",
        "            directory = str(datetime.datetime.now())\n",
        "            path = os.path.join(root_dir, directory)\n",
        "            os.mkdir(path)\n",
        "            \n",
        "            model = Model(rb)\n",
        "            model.train(dest_path=path)\n",
        "\n",
        "    def test_nll(self, debug=False):\n",
        "        nlls = []\n",
        "        with torch.no_grad():\n",
        "            test_loader = DataLoader(self.test_ds, batch_size=100, collate_fn=lambda x: x) # do not use default collate function as it requires fixed-length input and raises this exception otherwise https://github.com/pytorch/pytorch/issues/42654\n",
        "            for i, batch in enumerate(tqdm(test_loader, desc='test validation')):\n",
        "                batch_nlls = []\n",
        "                for (rev_sents_ids, review_aspects_scores) in batch:\n",
        "                    rev_thetas = self.rev_words_thetas(rev_sents_ids)\n",
        "                    rev_phis   = self.rev_words_phis(rev_sents_ids)\n",
        "                    asp_sents_scores = torch.stack(\n",
        "                        [\n",
        "                        rev_thetas[j].sum(dim=0) + torch.stack(tuple(rev_phis[j][a][:, review_aspects_scores[a] - 1].sum() for a in range(self.ds.aspect_count))) # 1 x aspect count\n",
        "                        for j in range(len(rev_sents_ids))\n",
        "                        ],\n",
        "                    ) # sent count x aspect count\n",
        "\n",
        "                    denoms = torch.logsumexp(asp_sents_scores, dim=1)[:, None]\n",
        "                    assert(denoms.shape == (len(rev_sents_ids), 1))\n",
        "\n",
        "                    asp_sents_scores = -asp_sents_scores + denoms\n",
        "\n",
        "                    sents_aspect_preds_max = torch.argmin(asp_sents_scores, dim=1)\n",
        "                    row_ind, col_ind = self._linear_assignement(costs=asp_sents_scores.detach().cpu().numpy())\n",
        "                    sents_aspect_preds_linear = sents_aspect_preds_max\n",
        "\n",
        "                    sents_aspect_preds_linear[row_ind] = torch.from_numpy(col_ind).to(dev)\n",
        "                    \n",
        "                    batch_nlls.append(asp_sents_scores.take_along_dim(sents_aspect_preds_linear[:, None], dim=1).sum().cpu().item())\n",
        "                nlls.append(functools.reduce(lambda a, b: a + b, batch_nlls))\n",
        "        nll_sum = functools.reduce(lambda a, b: a + b, nlls)\n",
        "        if debug: ic(nll_sum)\n",
        "        if debug: ic(torch.exp(-torch.tensor(nll_sum)))\n",
        "        mean_batch_nll = nll_sum / len(nlls)\n",
        "        return mean_batch_nll\n",
        "\n",
        "    def show_inference(self, reviews: list[str]):\n",
        "        self.ds._fetch_nlp_pipeline()\n",
        "        tokenized_reviews = self.ds.tokenize_reviews(reviews)\n",
        "\n",
        "        # revs_nlls = []\n",
        "        \n",
        "        ided_reviews = self.ds.id_map_reviews(tokenized_reviews)\n",
        "\n",
        "\n",
        "        revs_sents_preds = []\n",
        "        # aspect inference\n",
        "        for rev_sents_ids in ided_reviews:\n",
        "            rev_thetas = self.rev_words_thetas(rev_sents_ids)\n",
        "            rev_phis   = self.rev_words_phis(rev_sents_ids)\n",
        "            # why don't we use torch.nn.LogSoftmax after all?\n",
        "            asp_sents_scores = torch.stack(\n",
        "                [\n",
        "                        rev_thetas[j].sum(dim=0) + torch.stack(tuple(rev_phis[j][a][:].sum() for a in range(self.ds.aspect_count))) # 1 x aspect count\n",
        "                for j in range(len(rev_sents_ids)) # sentence\n",
        "                ],\n",
        "            ) # sent count x aspect count\n",
        "\n",
        "            denoms = torch.logsumexp(asp_sents_scores, dim=1)[:, None]\n",
        "            assert(denoms.shape == (len(rev_sents_ids), 1))\n",
        "\n",
        "            asp_sents_scores = -asp_sents_scores + denoms\n",
        "\n",
        "            sents_aspect_preds_max = torch.argmin(asp_sents_scores, dim=1) # min of nll === max likelihood\n",
        "            row_ind, col_ind = self._linear_assignement(costs=asp_sents_scores.detach().cpu().numpy())\n",
        "            sents_aspect_preds_linear = sents_aspect_preds_max\n",
        "\n",
        "            sents_aspect_preds_linear[row_ind] = torch.from_numpy(col_ind).to(dev)\n",
        "\n",
        "            revs_sents_preds.append(sents_aspect_preds_linear)\n",
        "            \n",
        "            # revs_nlls.append(asp_sents_scores.take_along_dim(sents_aspect_preds_linear[:, None], dim=1).sum().cpu().item())\n",
        "            rating_sents_scores = rev_phis\n",
        "            # torch.stack(\n",
        "            #     [\n",
        "            #             rev_thetas[j].sum(dim=0) + torch.stack(tuple(rev_phis[j][a][:].sum() for a in range(self.ds.aspect_count))) # 1 x aspect count\n",
        "            #     for j in range(len(rev_sents_ids)) # sentence\n",
        "            #     ],\n",
        "            # ) # sent count x aspect count\n",
        "\n",
        "            ic(rating_sents_scores)\n",
        "            raise BaseException\n",
        "        \n",
        "\n",
        "        for i in range(len(reviews)):\n",
        "            print(f'Review {i}: ')\n",
        "            print(f'Tokenized review text (stop words removed):')\n",
        "            print(*(tokenized_reviews[i]))\n",
        "            print(f'Review text mapped to word ID-s:', *(ided_reviews[i]))\n",
        "            # print()\n",
        "            print(f'Predicted sentences aspects: ', *[self.ds.aspects[target_idx] for target_idx in (revs_sents_preds[i])])\n",
        "            # print()\n",
        "            # print()\n",
        "            print()\n",
        "\n",
        "        # revs_nlls = []\n",
        "        # ic(revs_sents_preds)\n",
        "        # ic(revs_nlls)\n",
        "\n",
        "    def train(self, dest_path=MODEL_ROOT_DIR, epoch_count=1, optim='sgd'):\n",
        "        train_size = int(0.8 * len(self.ds))\n",
        "        test_size = len(self.ds) - train_size\n",
        "\n",
        "        params = (\n",
        "            self.theta,\n",
        "            *self.phis\n",
        "        )\n",
        "        # lr = 0.0005 # for ratebeer; remove\n",
        "        batch_size = 100\n",
        "        lr = sum(self.ds.aspect_max) * 1e-7 / batch_size\n",
        "\n",
        "        weight_decay = 0.0001\n",
        "        momentum = 0.1\n",
        "        if optim == 'sgd':\n",
        "            self._optim = torch.optim.SGD(\n",
        "                params=params,\n",
        "                lr=lr,\n",
        "                weight_decay=weight_decay,\n",
        "                momentum=momentum\n",
        "            )\n",
        "        elif optim == 'adam': # do not use Adam, weights go to NaN with it for some reason\n",
        "            self._optim = torch.optim.Adam(\n",
        "                params=params,\n",
        "                lr=lr,\n",
        "                weight_decay=weight_decay,\n",
        "                betas=(momentum, 0.999) # the first is params momentum, second RMSProp momentum (for now fix to 0.999 which is default Pytorch value)\n",
        "            )\n",
        "        else:\n",
        "            assert False\n",
        "        self._sched = torch.optim.lr_scheduler.StepLR(self._optim, step_size=1, gamma=0.95)\n",
        "\n",
        "\n",
        "        self.train_ds, self.test_ds = random_split(self.ds, [train_size, test_size])\n",
        "\n",
        "        train_loader = DataLoader(self.train_ds, batch_size=100, shuffle=True, collate_fn=lambda x: x) # do not use default collate function as it requires fixed-length input and raises this exception otherwise https://github.com/pytorch/pytorch/issues/42654\n",
        "        self.train_epoch_history = []\n",
        "        self.test_epoch_history = []\n",
        "        self.train_nll_history = []\n",
        "        self.test_nll_history = []\n",
        "        try:\n",
        "            for epoch in range(epoch_count):\n",
        "                batch_count = len(train_loader)\n",
        "                for i, batch in enumerate(tqdm(train_loader, desc=f'train epoch {epoch}/{epoch_count}')):\n",
        "                    batch_nlls = []\n",
        "                    for (rev_sents_ids, review_aspects_scores) in batch:\n",
        "                        # for sent_ids in rev_sents_ids: sent_ids.to(dev)\n",
        "                        rev_thetas = self.rev_words_thetas(rev_sents_ids)\n",
        "                        rev_phis   = self.rev_words_phis(rev_sents_ids)\n",
        "\n",
        "\n",
        "                        # why don't we use torch.nn.LogSoftmax after all?\n",
        "\n",
        "                        asp_sents_scores = torch.stack(\n",
        "                            [\n",
        "                            rev_thetas[j].sum(dim=0) + torch.stack(tuple(rev_phis[j][a][:, review_aspects_scores[a] - 1].sum() for a in range(self.ds.aspect_count))) # 1 x aspect count\n",
        "                            for j in range(len(rev_sents_ids))\n",
        "                            ],\n",
        "                        ) # sent count x aspect count\n",
        "\n",
        "                        denoms = torch.logsumexp(asp_sents_scores, dim=1)[:, None]\n",
        "                        assert(denoms.shape == (len(rev_sents_ids), 1))\n",
        "\n",
        "                        asp_sents_scores = -asp_sents_scores + denoms\n",
        "\n",
        "                        sents_aspect_preds_max = torch.argmin(asp_sents_scores, dim=1)\n",
        "                        row_ind, col_ind = self._linear_assignement(costs=asp_sents_scores.detach().cpu().numpy())\n",
        "                        sents_aspect_preds_linear = sents_aspect_preds_max\n",
        "\n",
        "                        # (most likely) aspect assignments (5)\n",
        "                        sents_aspect_preds_linear[row_ind] = torch.from_numpy(col_ind).to(dev)\n",
        "                        \n",
        "                        # sentence likelihood (6)\n",
        "                        batch_nlls.append(asp_sents_scores.take_along_dim(sents_aspect_preds_linear[:, None], dim=1).sum())\n",
        "\n",
        "                    batch_nll = torch.stack(batch_nlls).sum()\n",
        "                    self._optim.zero_grad(set_to_none=True)\n",
        "                    if 0 == i % 100:\n",
        "                        self.train_nll_history.append(batch_nll.cpu().detach().item())\n",
        "                        self.train_epoch_history.append(epoch + i / batch_count)\n",
        "                    if 0 == i % 5000:\n",
        "                        if i != 0:\n",
        "                            self.dump_weights(dest_path=dest_path, filename=(f'-epoch-{epoch}-{epoch_count}-{int(i)}'))\n",
        "                            self.word_clouds(dest_path=dest_path, filename=(f'cloud-epoch-{epoch}-{epoch_count}-{int(i)}.png'))\n",
        "                            # ic(self.rev_words_thetas([self.ds.vocab.lookup_indices(['taste', 'aroma', 'palete', 'antrunk'])]))\n",
        "                        self.test_nll_history.append(self.test_nll())\n",
        "                        self.test_epoch_history.append(epoch + i / batch_count)\n",
        "                    batch_nll.backward()\n",
        "\n",
        "                    self._optim.step()\n",
        "                self._sched.step()\n",
        "\n",
        "            self.dump_weights(dest_path=dest_path, filename=(f'weights-end'))\n",
        "            self.word_clouds(dest_path=dest_path, filename=(f'cloud-end.png'))\n",
        "            self.plot_nll_history(dest_path, (f'plot-end.png'))\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print('Interrupted.')\n",
        "\n",
        "            self.dump_weights(dest_path=dest_path, filename=(f'weights-interrupt'))\n",
        "            self.word_clouds(dest_path=dest_path, filename=(f'cloud-interrupt.png'))\n",
        "            self.plot_nll_history(dest_path, (f'plot-interrupt.png'))\n",
        "\n",
        "        except Exception as e:\n",
        "            ic(len(batch_nlls))\n",
        "            ic(denoms)\n",
        "            ic(rev_sents_ids)\n",
        "\n",
        "            self.dump_weights(dest_path=dest_path, filename=(f'weights-exception'))\n",
        "            self.word_clouds(dest_path=dest_path, filename=(f'cloud-exception.png'))\n",
        "            self.plot_nll_history(dest_path, (f'plot-exception.png'))\n",
        "\n",
        "            ic(batch_nll)\n",
        "            ic(denoms.size())\n",
        "            ic((len(rev_sents_ids),))\n",
        "            ic(denoms.size() == (len(rev_sents_ids),))\n",
        "            raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyP8T2ASG4ZW"
      },
      "outputs": [],
      "source": [
        "%%capture output\n",
        "# %%script python --no-raise-error\n",
        "# model = Model(rb)\n",
        "# model.train()\n",
        "Model.experiments(root_dir='./saves/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ByiBGGuLQF_"
      },
      "outputs": [],
      "source": [
        "output.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYr-3QAhLRga"
      },
      "outputs": [],
      "source": [
        "# %store output > output_log\n",
        "import pickle\n",
        "with open('output', 'wb') as f:\n",
        "    pickle.dump(output, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddNbPIhSXUYv"
      },
      "outputs": [],
      "source": [
        "# danger\n",
        "# model.ds.vocab.lookup_indices([0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVSk4pncPJqO"
      },
      "outputs": [],
      "source": [
        "model = Model(rb) # single run\n",
        "# model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M-dJ1YWuCG8"
      },
      "outputs": [],
      "source": [
        "model.show_inference([\n",
        "    'Good-looking bootle. Aroma is pretty astonishig. Sour and sweet palate profile. I like the taste very much. Overall I can recommend it to everyone.',\n",
        "    'Tastes best from bottle. Not so heap as one could think. Nice hoppy smell. I had not supposed it will be sour though. Beautiful smooth head.',\n",
        "])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "1e100ibu.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}